 
>>> PBS_NODEFILE content:
sophia-gpu-12.lab.alcf.anl.gov
1n*1t
Tue Jul 22 16:41:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:87:00.0 Off |                    0 |
| N/A   28C    P0             51W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          Off |   00000000:90:00.0 Off |                    0 |
| N/A   27C    P0             54W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
Start time: 2025-07-22 16:41:31
Python 3.9.18
Python path: /lus/eagle/projects/fthmc/software/ml/bin/python
PYTHONPATH: /eagle/fthmc/run
You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

============================================================
>>> CUDA device count: 2
PyTorch version: 2.5.0+cu124
CUDA available: True
>>> Arguments:
Lattice size: 64x64
Minimum beta: 2.0
Maximum beta: 2.0
Beta gap: 0.5
Number of epochs: 32
Batch size: 32
Number of subsets: 8
Number of workers: 0
Model tag: stable
Save tag: stable
Random seed: 2008
Identity initialization: True
Check Jacobian: False
Continue training: False
============================================================
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
Trying to use torch.compile for optimized computation...
Successfully initialized torch.compile
>>> Training from scratch
Loaded data shape: torch.Size([4096, 2, 64, 64])
Training data shape: torch.Size([3276, 2, 64, 64])
Testing data shape: torch.Size([820, 2, 64, 64])

>>> Training the model at beta =  2.0

>>> Training the model at beta = 2.0

Training epochs:   0%|          | 0/32 [00:00<?, ?it/s]Training epochs:   0%|          | 0/32 [00:00<?, ?it/s]
Epoch 1/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 1/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 1/32:   2%|▏         | 1/52 [00:17<14:27, 17.02s/it][A
Epoch 1/32:   2%|▏         | 1/52 [00:17<14:28, 17.02s/it][A
Epoch 1/32:   4%|▍         | 2/52 [00:18<06:24,  7.70s/it][A
Epoch 1/32:   4%|▍         | 2/52 [00:18<06:24,  7.70s/it][A
Epoch 1/32:   6%|▌         | 3/52 [00:19<03:52,  4.75s/it][A
Epoch 1/32:   6%|▌         | 3/52 [00:19<03:52,  4.75s/it][A
Epoch 1/32:   8%|▊         | 4/52 [00:20<02:41,  3.36s/it][A
Epoch 1/32:   8%|▊         | 4/52 [00:20<02:41,  3.36s/it][A
Epoch 1/32:  10%|▉         | 5/52 [00:21<02:01,  2.59s/it][A
Epoch 1/32:  10%|▉         | 5/52 [00:21<02:01,  2.59s/it][A
Epoch 1/32:  12%|█▏        | 6/52 [00:23<01:38,  2.14s/it][A
Epoch 1/32:  12%|█▏        | 6/52 [00:23<01:38,  2.14s/it][A
Epoch 1/32:  13%|█▎        | 7/52 [00:24<01:23,  1.84s/it][A
Epoch 1/32:  13%|█▎        | 7/52 [00:24<01:23,  1.85s/it][A
Epoch 1/32:  15%|█▌        | 8/52 [00:25<01:12,  1.66s/it][A
Epoch 1/32:  15%|█▌        | 8/52 [00:25<01:12,  1.66s/it][A
Epoch 1/32:  17%|█▋        | 9/52 [00:26<01:05,  1.53s/it][A
Epoch 1/32:  17%|█▋        | 9/52 [00:26<01:05,  1.53s/it][A
Epoch 1/32:  19%|█▉        | 10/52 [00:28<01:00,  1.45s/it][A
Epoch 1/32:  19%|█▉        | 10/52 [00:28<01:00,  1.45s/it][A

Epoch 1/32:  21%|██        | 11/52 [00:29<00:56,  1.39s/it]Epoch 1/32:  21%|██        | 11/52 [00:29<00:56,  1.39s/it][A[A
Epoch 1/32:  23%|██▎       | 12/52 [00:30<00:53,  1.34s/it][A
Epoch 1/32:  23%|██▎       | 12/52 [00:30<00:53,  1.34s/it][A
Epoch 1/32:  25%|██▌       | 13/52 [00:31<00:51,  1.31s/it][A
Epoch 1/32:  25%|██▌       | 13/52 [00:31<00:51,  1.31s/it][A
Epoch 1/32:  27%|██▋       | 14/52 [00:33<00:48,  1.28s/it][A
Epoch 1/32:  27%|██▋       | 14/52 [00:33<00:48,  1.29s/it][A
Epoch 1/32:  29%|██▉       | 15/52 [00:34<00:46,  1.27s/it][A
Epoch 1/32:  29%|██▉       | 15/52 [00:34<00:46,  1.27s/it][A
Epoch 1/32:  31%|███       | 16/52 [00:35<00:45,  1.26s/it][A
Epoch 1/32:  31%|███       | 16/52 [00:35<00:45,  1.26s/it][A

Epoch 1/32:  33%|███▎      | 17/52 [00:36<00:43,  1.25s/it][AEpoch 1/32:  33%|███▎      | 17/52 [00:36<00:43,  1.25s/it][A
Epoch 1/32:  35%|███▍      | 18/52 [00:38<00:42,  1.25s/it][A
Epoch 1/32:  35%|███▍      | 18/52 [00:38<00:42,  1.25s/it][A
Epoch 1/32:  37%|███▋      | 19/52 [00:39<00:41,  1.25s/it][A
Epoch 1/32:  37%|███▋      | 19/52 [00:39<00:41,  1.25s/it][A
Epoch 1/32:  38%|███▊      | 20/52 [00:40<00:39,  1.24s/it][A
Epoch 1/32:  38%|███▊      | 20/52 [00:40<00:39,  1.24s/it][A
Epoch 1/32:  40%|████      | 21/52 [00:41<00:38,  1.25s/it][A
Epoch 1/32:  40%|████      | 21/52 [00:41<00:38,  1.25s/it][A
Epoch 1/32:  42%|████▏     | 22/52 [00:43<00:37,  1.25s/it][A
Epoch 1/32:  42%|████▏     | 22/52 [00:43<00:37,  1.25s/it][A
Epoch 1/32:  44%|████▍     | 23/52 [00:44<00:36,  1.25s/it][A
Epoch 1/32:  44%|████▍     | 23/52 [00:44<00:36,  1.25s/it][A
Epoch 1/32:  46%|████▌     | 24/52 [00:45<00:35,  1.25s/it]
[AEpoch 1/32:  46%|████▌     | 24/52 [00:45<00:35,  1.25s/it][A
Epoch 1/32:  48%|████▊     | 25/52 [00:46<00:34,  1.27s/it][A
Epoch 1/32:  48%|████▊     | 25/52 [00:46<00:34,  1.27s/it][A
Epoch 1/32:  50%|█████     | 26/52 [00:48<00:33,  1.29s/it][A
Epoch 1/32:  50%|█████     | 26/52 [00:48<00:33,  1.29s/it][A
Epoch 1/32:  52%|█████▏    | 27/52 [00:49<00:32,  1.29s/it][A
Epoch 1/32:  52%|█████▏    | 27/52 [00:49<00:32,  1.29s/it][A
Epoch 1/32:  54%|█████▍    | 28/52 [00:50<00:31,  1.30s/it][A
Epoch 1/32:  54%|█████▍    | 28/52 [00:50<00:31,  1.30s/it][A
Epoch 1/32:  56%|█████▌    | 29/52 [00:52<00:29,  1.30s/it][A
Epoch 1/32:  56%|█████▌    | 29/52 [00:52<00:29,  1.30s/it][A
Epoch 1/32:  58%|█████▊    | 30/52 [00:53<00:28,  1.31s/it][A
Epoch 1/32:  58%|█████▊    | 30/52 [00:53<00:28,  1.31s/it][A
Epoch 1/32:  60%|█████▉    | 31/52 [00:54<00:27,  1.31s/it][A
Epoch 1/32:  60%|█████▉    | 31/52 [00:54<00:27,  1.31s/it][A
Epoch 1/32:  62%|██████▏   | 32/52 [00:56<00:26,  1.31s/it][A
Epoch 1/32:  62%|██████▏   | 32/52 [00:56<00:26,  1.31s/it][A

Epoch 1/32:  63%|██████▎   | 33/52 [00:57<00:24,  1.31s/it][AEpoch 1/32:  63%|██████▎   | 33/52 [00:57<00:24,  1.31s/it][A
Epoch 1/32:  65%|██████▌   | 34/52 [00:58<00:23,  1.32s/it][A
Epoch 1/32:  65%|██████▌   | 34/52 [00:58<00:23,  1.32s/it][A
Epoch 1/32:  67%|██████▋   | 35/52 [01:00<00:22,  1.32s/it][A
Epoch 1/32:  67%|██████▋   | 35/52 [01:00<00:22,  1.32s/it][A
Epoch 1/32:  69%|██████▉   | 36/52 [01:01<00:21,  1.32s/it][A
Epoch 1/32:  69%|██████▉   | 36/52 [01:01<00:21,  1.32s/it][A
Epoch 1/32:  71%|███████   | 37/52 [01:02<00:19,  1.32s/it][A
Epoch 1/32:  71%|███████   | 37/52 [01:02<00:19,  1.32s/it][A
Epoch 1/32:  73%|███████▎  | 38/52 [01:04<00:18,  1.33s/it][A
Epoch 1/32:  73%|███████▎  | 38/52 [01:04<00:18,  1.33s/it][A
Epoch 1/32:  75%|███████▌  | 39/52 [01:05<00:17,  1.35s/it][A
Epoch 1/32:  75%|███████▌  | 39/52 [01:05<00:17,  1.35s/it][A

Epoch 1/32:  77%|███████▋  | 40/52 [01:06<00:16,  1.37s/it][AEpoch 1/32:  77%|███████▋  | 40/52 [01:06<00:16,  1.37s/it][A

Epoch 1/32:  79%|███████▉  | 41/52 [01:08<00:15,  1.38s/it][AEpoch 1/32:  79%|███████▉  | 41/52 [01:08<00:15,  1.38s/it][A
Epoch 1/32:  81%|████████  | 42/52 [01:09<00:13,  1.38s/it][A
Epoch 1/32:  81%|████████  | 42/52 [01:09<00:13,  1.38s/it][A
Epoch 1/32:  83%|████████▎ | 43/52 [01:11<00:12,  1.39s/it][A
Epoch 1/32:  83%|████████▎ | 43/52 [01:11<00:12,  1.39s/it][A
Epoch 1/32:  85%|████████▍ | 44/52 [01:12<00:11,  1.41s/it][A
Epoch 1/32:  85%|████████▍ | 44/52 [01:12<00:11,  1.41s/it][A
Epoch 1/32:  87%|████████▋ | 45/52 [01:13<00:09,  1.43s/it][A
Epoch 1/32:  87%|████████▋ | 45/52 [01:13<00:09,  1.43s/it][A

Epoch 1/32:  88%|████████▊ | 46/52 [01:15<00:08,  1.44s/it][AEpoch 1/32:  88%|████████▊ | 46/52 [01:15<00:08,  1.44s/it][A
Epoch 1/32:  90%|█████████ | 47/52 [01:16<00:07,  1.46s/it][A
Epoch 1/32:  90%|█████████ | 47/52 [01:16<00:07,  1.46s/it][A
Epoch 1/32:  92%|█████████▏| 48/52 [01:18<00:05,  1.48s/it][A
Epoch 1/32:  92%|█████████▏| 48/52 [01:18<00:05,  1.48s/it][A
Epoch 1/32:  94%|█████████▍| 49/52 [01:20<00:04,  1.50s/it][A
Epoch 1/32:  94%|█████████▍| 49/52 [01:20<00:04,  1.50s/it][A
Epoch 1/32:  96%|█████████▌| 50/52 [01:21<00:03,  1.52s/it][A
Epoch 1/32:  96%|█████████▌| 50/52 [01:21<00:03,  1.52s/it][A
Epoch 1/32:  98%|█████████▊| 51/52 [01:23<00:01,  1.55s/it][A
Epoch 1/32:  98%|█████████▊| 51/52 [01:23<00:01,  1.55s/it][A
Epoch 1/32: 100%|██████████| 52/52 [01:24<00:00,  1.48s/it][AEpoch 1/32: 100%|██████████| 52/52 [01:24<00:00,  1.63s/it]

Epoch 1/32: 100%|██████████| 52/52 [01:24<00:00,  1.48s/it][AEpoch 1/32: 100%|██████████| 52/52 [01:24<00:00,  1.63s/it]

[β=2.0] Epoch 1 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=4.849e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=1.141e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=3.360e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.733e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.112e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=6.365e-02
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=3.780e-03
  layer model_7__forward_module.module.output_conv act-std: mean=6.393e-02
  layer model_6__forward_module.module.input_conv act-std: mean=3.615e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=1.132e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=3.305e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.244e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=7.849e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=7.979e-03
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=2.548e-03
  layer model_6__forward_module.module.output_conv act-std: mean=4.581e-02
  layer model_5__forward_module.module.input_conv act-std: mean=4.202e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=6.349e-02
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=3.757e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=4.530e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.090e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=9.093e-02
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=1.237e-03
  layer model_5__forward_module.module.output_conv act-std: mean=8.435e-02
  layer model_4__forward_module.module.input_conv act-std: mean=3.863e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=1.208e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=2.994e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=3.886e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=7.601e-01
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=6.641e-02
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=2.899e-03
  layer model_4__forward_module.module.output_conv act-std: mean=6.952e-02
  layer model_3__forward_module.module.input_conv act-std: mean=4.167e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=1.096e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.545e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.982e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=9.887e-01
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.179e-02
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=2.203e-03
  layer model_3__forward_module.module.output_conv act-std: mean=6.884e-02
  layer model_2__forward_module.module.input_conv act-std: mean=3.997e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=1.193e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=3.272e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=3.499e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=8.782e-01
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=6.028e-02
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=3.456e-03
  layer model_2__forward_module.module.output_conv act-std: mean=5.717e-02
  layer model_1__forward_module.module.input_conv act-std: mean=4.500e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.280e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.470e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.999e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.004e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.259e-02
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=1.886e-03
  layer model_1__forward_module.module.output_conv act-std: mean=7.008e-02
  layer model_0__forward_module.module.input_conv act-std: mean=6.104e-02
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=8.697e-02
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=2.219e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.940e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=5.774e-01
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=2.925e-02
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=4.287e-03
  layer model_0__forward_module.module.output_conv act-std: mean=3.304e-02
  param model_0__forward_module.module.output_scale grad-norm: mean=1.312e+00
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=8.573e-03
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.657e-03
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.073e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.450e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.488e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.080e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.534e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.330e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.426e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.014e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.921e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.115e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=1.522e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=6.846e-04
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.591e-03
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=5.850e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.343e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=2.777e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.333e+00
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=2.024e-02
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=3.820e-03
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=2.067e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.856e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.661e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.969e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.726e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.550e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.265e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.019e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.394e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.164e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=5.486e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.283e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=5.634e-03
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=1.183e-02
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.529e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.946e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.933e+00
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=2.337e-02

  param model_2__forward_module.module.input_conv.bias grad-norm: mean=4.316e-03
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.764e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.451e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.950e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.775e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.819e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.078e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.128e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.639e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.410e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.325e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.299e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=9.815e-04
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=3.285e-03
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=9.686e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.062e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=2.939e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.091e+00
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=2.686e-02
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.835e-03
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.926e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.759e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.538e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.900e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.320e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.207e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.703e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.747e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.783e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.291e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=4.243e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.039e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=4.028e-03
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.015e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.179e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.722e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.157e+00
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=2.345e-02
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.439e-03
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.936e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.752e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.579e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.895e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.144e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.288e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.061e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.910e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.872e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.219e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=4.871e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.293e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=4.821e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=9.741e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.269e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.809e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.154e+00
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=3.303e-02
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=5.733e-03
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.186e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.973e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.901e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.585e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.462e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.243e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.939e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.775e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.849e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.244e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=4.570e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.036e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=4.200e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=9.915e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.308e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.661e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.740e+00
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.257e-02
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.271e-03
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.337e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.110e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.389e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.795e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.350e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.620e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.195e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.215e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.905e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.488e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=3.835e-04
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=1.011e-04
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=4.447e-04
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=8.975e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.935e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.971e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.855e+00
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.103e-02
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=5.248e-03
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.681e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.475e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.082e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.054e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.752e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.950e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.056e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.540e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.164e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.696e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=4.080e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=1.146e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=3.943e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=8.458e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.996e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.620e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.93it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.93it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.94it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.92it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.96it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.92it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.97it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.92it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.96it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.92it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.95it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.92it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.94it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.92it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.94it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.91it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.95it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.91it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.96it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.92it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.95it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.91it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.95it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.92it/s][A
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.95it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.95it/s]
Training epochs:   3%|▎         | 1/32 [01:31<47:08, 91.25s/it]
Epoch 2/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.92it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.92it/s]
Epoch 1/32 - Train Loss: 27.039907 - Test Loss: 22.803172
Training epochs:   3%|▎         | 1/32 [01:31<47:12, 91.37s/it]
Epoch 2/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 2/32:   2%|▏         | 1/52 [00:01<01:30,  1.77s/it][A
Epoch 2/32:   2%|▏         | 1/52 [00:01<01:24,  1.65s/it][A
Epoch 2/32:   4%|▍         | 2/52 [00:03<01:22,  1.64s/it][A
Epoch 2/32:   4%|▍         | 2/52 [00:03<01:24,  1.70s/it][A
Epoch 2/32:   6%|▌         | 3/52 [00:05<01:22,  1.68s/it][A
Epoch 2/32:   6%|▌         | 3/52 [00:04<01:21,  1.66s/it][A
Epoch 2/32:   8%|▊         | 4/52 [00:06<01:20,  1.67s/it][A
Epoch 2/32:   8%|▊         | 4/52 [00:06<01:21,  1.69s/it][A
Epoch 2/32:  10%|▉         | 5/52 [00:08<01:18,  1.67s/it][A
Epoch 2/32:  10%|▉         | 5/52 [00:08<01:19,  1.69s/it][A
Epoch 2/32:  12%|█▏        | 6/52 [00:10<01:17,  1.68s/it][A
Epoch 2/32:  12%|█▏        | 6/52 [00:10<01:17,  1.69s/it][A
Epoch 2/32:  13%|█▎        | 7/52 [00:11<01:15,  1.69s/it][A
Epoch 2/32:  13%|█▎        | 7/52 [00:11<01:16,  1.69s/it][A
Epoch 2/32:  15%|█▌        | 8/52 [00:13<01:14,  1.69s/it][A
Epoch 2/32:  15%|█▌        | 8/52 [00:13<01:14,  1.69s/it][A
Epoch 2/32:  17%|█▋        | 9/52 [00:15<01:12,  1.69s/it][A
Epoch 2/32:  17%|█▋        | 9/52 [00:15<01:12,  1.69s/it][A
Epoch 2/32:  19%|█▉        | 10/52 [00:16<01:11,  1.69s/it][A
Epoch 2/32:  19%|█▉        | 10/52 [00:16<01:11,  1.69s/it][A
Epoch 2/32:  21%|██        | 11/52 [00:18<01:09,  1.70s/it][A
Epoch 2/32:  21%|██        | 11/52 [00:18<01:09,  1.70s/it][A
Epoch 2/32:  23%|██▎       | 12/52 [00:20<01:07,  1.69s/it][A
Epoch 2/32:  23%|██▎       | 12/52 [00:20<01:07,  1.69s/it][A
Epoch 2/32:  25%|██▌       | 13/52 [00:21<01:05,  1.69s/it]
[AEpoch 2/32:  25%|██▌       | 13/52 [00:21<01:05,  1.68s/it][A
Epoch 2/32:  27%|██▋       | 14/52 [00:23<01:03,  1.68s/it][A
Epoch 2/32:  27%|██▋       | 14/52 [00:23<01:03,  1.68s/it][A
Epoch 2/32:  29%|██▉       | 15/52 [00:25<01:01,  1.68s/it][A
Epoch 2/32:  29%|██▉       | 15/52 [00:25<01:02,  1.68s/it][A
Epoch 2/32:  31%|███       | 16/52 [00:27<01:00,  1.68s/it][A
Epoch 2/32:  31%|███       | 16/52 [00:26<01:00,  1.68s/it][A
Epoch 2/32:  33%|███▎      | 17/52 [00:28<00:58,  1.68s/it][A
Epoch 2/32:  33%|███▎      | 17/52 [00:28<00:58,  1.68s/it][A
Epoch 2/32:  35%|███▍      | 18/52 [00:30<00:57,  1.68s/it][A
Epoch 2/32:  35%|███▍      | 18/52 [00:30<00:57,  1.68s/it][A

Epoch 2/32:  37%|███▋      | 19/52 [00:32<00:55,  1.68s/it][AEpoch 2/32:  37%|███▋      | 19/52 [00:31<00:55,  1.68s/it][A
Epoch 2/32:  38%|███▊      | 20/52 [00:33<00:53,  1.68s/it][A
Epoch 2/32:  38%|███▊      | 20/52 [00:33<00:53,  1.68s/it][A
Epoch 2/32:  40%|████      | 21/52 [00:35<00:52,  1.68s/it][A
Epoch 2/32:  40%|████      | 21/52 [00:35<00:52,  1.68s/it][A
Epoch 2/32:  42%|████▏     | 22/52 [00:36<00:50,  1.68s/it][A
Epoch 2/32:  42%|████▏     | 22/52 [00:37<00:50,  1.68s/it][A
Epoch 2/32:  44%|████▍     | 23/52 [00:38<00:48,  1.67s/it][A
Epoch 2/32:  44%|████▍     | 23/52 [00:38<00:48,  1.68s/it][A
Epoch 2/32:  46%|████▌     | 24/52 [00:40<00:46,  1.68s/it][A
Epoch 2/32:  46%|████▌     | 24/52 [00:40<00:46,  1.68s/it][A
Epoch 2/32:  48%|████▊     | 25/52 [00:41<00:45,  1.68s/it][A
Epoch 2/32:  48%|████▊     | 25/52 [00:42<00:45,  1.68s/it][A
Epoch 2/32:  50%|█████     | 26/52 [00:43<00:43,  1.68s/it][A
Epoch 2/32:  50%|█████     | 26/52 [00:43<00:43,  1.68s/it][A
Epoch 2/32:  52%|█████▏    | 27/52 [00:45<00:41,  1.67s/it][A
Epoch 2/32:  52%|█████▏    | 27/52 [00:45<00:41,  1.68s/it][A
Epoch 2/32:  54%|█████▍    | 28/52 [00:47<00:40,  1.67s/it][A
Epoch 2/32:  54%|█████▍    | 28/52 [00:47<00:40,  1.67s/it][A
Epoch 2/32:  56%|█████▌    | 29/52 [00:48<00:38,  1.68s/it][A
Epoch 2/32:  56%|█████▌    | 29/52 [00:48<00:38,  1.68s/it][A
Epoch 2/32:  58%|█████▊    | 30/52 [00:50<00:36,  1.68s/it][A
Epoch 2/32:  58%|█████▊    | 30/52 [00:50<00:36,  1.68s/it][A
Epoch 2/32:  60%|█████▉    | 31/52 [00:52<00:35,  1.67s/it][A
Epoch 2/32:  60%|█████▉    | 31/52 [00:52<00:35,  1.68s/it][A
Epoch 2/32:  62%|██████▏   | 32/52 [00:53<00:33,  1.68s/it][A
Epoch 2/32:  62%|██████▏   | 32/52 [00:53<00:33,  1.69s/it][A
Epoch 2/32:  63%|██████▎   | 33/52 [00:55<00:31,  1.68s/it][A
Epoch 2/32:  63%|██████▎   | 33/52 [00:55<00:31,  1.68s/it][A
Epoch 2/32:  65%|██████▌   | 34/52 [00:57<00:30,  1.68s/it][A
Epoch 2/32:  65%|██████▌   | 34/52 [00:57<00:30,  1.69s/it][A
Epoch 2/32:  67%|██████▋   | 35/52 [00:58<00:28,  1.68s/it][A
Epoch 2/32:  67%|██████▋   | 35/52 [00:58<00:28,  1.68s/it][A
Epoch 2/32:  69%|██████▉   | 36/52 [01:00<00:26,  1.69s/it][A
Epoch 2/32:  69%|██████▉   | 36/52 [01:00<00:26,  1.69s/it][A
Epoch 2/32:  71%|███████   | 37/52 [01:02<00:25,  1.68s/it][A
Epoch 2/32:  71%|███████   | 37/52 [01:02<00:25,  1.68s/it][A
Epoch 2/32:  73%|███████▎  | 38/52 [01:03<00:23,  1.68s/it][A
Epoch 2/32:  73%|███████▎  | 38/52 [01:03<00:23,  1.68s/it][A
Epoch 2/32:  75%|███████▌  | 39/52 [01:05<00:21,  1.68s/it][A
Epoch 2/32:  75%|███████▌  | 39/52 [01:05<00:21,  1.68s/it][A
Epoch 2/32:  77%|███████▋  | 40/52 [01:07<00:20,  1.68s/it][A
Epoch 2/32:  77%|███████▋  | 40/52 [01:07<00:20,  1.68s/it][A
Epoch 2/32:  79%|███████▉  | 41/52 [01:09<00:18,  1.68s/it][A
Epoch 2/32:  79%|███████▉  | 41/52 [01:08<00:18,  1.68s/it][A
Epoch 2/32:  81%|████████  | 42/52 [01:10<00:16,  1.68s/it][A
Epoch 2/32:  81%|████████  | 42/52 [01:10<00:16,  1.68s/it][A
Epoch 2/32:  83%|████████▎ | 43/52 [01:12<00:15,  1.67s/it][A
Epoch 2/32:  83%|████████▎ | 43/52 [01:12<00:15,  1.68s/it][A
Epoch 2/32:  85%|████████▍ | 44/52 [01:14<00:13,  1.67s/it][A
Epoch 2/32:  85%|████████▍ | 44/52 [01:13<00:13,  1.67s/it][A
Epoch 2/32:  87%|████████▋ | 45/52 [01:15<00:11,  1.67s/it][A
Epoch 2/32:  87%|████████▋ | 45/52 [01:15<00:11,  1.68s/it][A
Epoch 2/32:  88%|████████▊ | 46/52 [01:17<00:10,  1.68s/it][A
Epoch 2/32:  88%|████████▊ | 46/52 [01:17<00:10,  1.68s/it][A
Epoch 2/32:  90%|█████████ | 47/52 [01:19<00:08,  1.68s/it][A
Epoch 2/32:  90%|█████████ | 47/52 [01:18<00:08,  1.68s/it][A
Epoch 2/32:  92%|█████████▏| 48/52 [01:20<00:06,  1.68s/it][A
Epoch 2/32:  92%|█████████▏| 48/52 [01:20<00:06,  1.68s/it][A
Epoch 2/32:  94%|█████████▍| 49/52 [01:22<00:05,  1.68s/it][A
Epoch 2/32:  94%|█████████▍| 49/52 [01:22<00:05,  1.68s/it][A
Epoch 2/32:  96%|█████████▌| 50/52 [01:24<00:03,  1.68s/it][A
Epoch 2/32:  96%|█████████▌| 50/52 [01:23<00:03,  1.68s/it][A

Epoch 2/32:  98%|█████████▊| 51/52 [01:25<00:01,  1.68s/it][AEpoch 2/32:  98%|█████████▊| 51/52 [01:25<00:01,  1.68s/it][A
Epoch 2/32: 100%|██████████| 52/52 [01:26<00:00,  1.57s/it][AEpoch 2/32: 100%|██████████| 52/52 [01:26<00:00,  1.67s/it]

[β=2.0] Epoch 2 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=9.372e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=2.561e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=9.724e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=1.375e+00
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=4.811e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.696e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=6.551e-03
  layer model_7__forward_module.module.output_conv act-std: mean=4.179e-01
  layer model_6__forward_module.module.input_conv act-std: mean=4.780e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=3.731e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=1.373e+00
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=1.674e+00
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=6.072e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=2.195e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=4.113e-03
  layer model_6__forward_module.module.output_conv act-std: mean=3.903e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.457e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.359e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=9.486e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.472e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=5.020e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.805e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.045e-03

  layer model_5__forward_module.module.output_conv act-std: mean=4.826e-01
  layer model_4__forward_module.module.input_conv act-std: mean=9.716e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.870e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=9.442e-01
Epoch 2/32: 100%|██████████| 52/52 [01:27<00:00,  1.58s/it]  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.542e+00[A
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=4.147e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.350e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=7.261e-03
  layer model_4__forward_module.module.output_conv act-std: mean=4.868e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.221e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.214e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=8.811e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=1.267e+00
Epoch 2/32: 100%|██████████| 52/52 [01:27<00:00,  1.68s/it]  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=4.739e+00

  layer model_3__forward_module.module.channel_attention.1 act-std: mean=5.151e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=4.267e-03
  layer model_3__forward_module.module.output_conv act-std: mean=4.169e-01
  layer model_2__forward_module.module.input_conv act-std: mean=7.351e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.230e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=1.118e+00
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.513e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=5.521e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=5.697e-01
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.173e-03
  layer model_2__forward_module.module.output_conv act-std: mean=4.462e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.536e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.669e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=9.107e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=1.339e+00
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=5.117e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=4.855e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=4.171e-03
  layer model_1__forward_module.module.output_conv act-std: mean=4.604e-01
  layer model_0__forward_module.module.input_conv act-std: mean=6.721e-02
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=4.736e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=1.666e+00
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.742e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=8.245e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=5.713e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=1.262e-02
  layer model_0__forward_module.module.output_conv act-std: mean=5.086e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.781e+00
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.153e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.581e-02
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=6.279e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=5.215e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.209e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.249e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.438e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.314e-03
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.435e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.493e-03
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.715e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.337e-04
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=8.260e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=8.631e-04
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=8.281e-03
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=5.616e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=5.761e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.025e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.393e+00
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=3.948e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=9.607e-02
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.418e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.498e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.346e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.283e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.414e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.008e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.687e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=6.200e-03
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.710e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.358e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=6.905e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=9.036e-04
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=7.049e-03
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=5.113e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=4.275e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.010e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.256e+00
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=4.681e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.096e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=6.920e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.613e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.506e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.011e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.137e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=6.579e-03
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.298e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=4.168e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.432e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.900e-04
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=5.453e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=6.640e-04
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=5.256e-03
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=4.378e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=3.549e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=7.834e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=1.166e+00
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=6.228e-01
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=1.444e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.255e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.285e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.168e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.878e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.111e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=7.768e-03
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.289e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=4.787e-03
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.297e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.053e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=4.430e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=6.200e-04
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=4.224e-03
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=3.921e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=3.523e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=8.763e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=1.447e+00
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=4.418e-01
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=1.017e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=7.246e-02
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=7.925e-02
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.918e-03
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.907e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.614e-03
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.301e-03
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.216e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.707e-03
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.230e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.050e-04
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=4.612e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=6.241e-04
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=4.682e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=3.058e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=4.112e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=9.603e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=1.455e+00
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=4.273e-01
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=9.567e-02
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.057e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.128e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.159e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.522e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.069e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=6.253e-03
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.331e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.927e-03
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.332e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.927e-04
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=4.510e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=5.292e-04
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=4.253e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=3.695e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=3.744e-01

  param model_5__forward_module.module.output_conv.bias grad-norm: mean=8.249e-02
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]  param model_6__forward_module.module.output_scale grad-norm: mean=1.490e+00[A
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=5.371e-01
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=1.190e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=7.718e-02
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=7.255e-02
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.899e-03
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.574e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.269e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.256e-03
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.417e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.608e-03
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.704e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.050e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.324e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=3.005e-04
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=2.449e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=5.892e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=5.192e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.181e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.394e+00
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=7.144e-01
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=1.570e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.225e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.255e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.271e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.830e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.310e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=8.384e-03
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.589e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=5.410e-03
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.621e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.305e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=6.008e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=8.977e-04
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=5.998e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=4.659e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.786e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=9.389e-02

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.92it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.91it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.91it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.89it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.92it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.89it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.91it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.88it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.88it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.86it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.89it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.86it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.87it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.93it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.89it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.93it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.90it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.94it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.90it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.96it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.91it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.96it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.92it/s][A
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.97it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.94it/s]
Training epochs:   6%|▋         | 2/32 [03:05<46:23, 92.80s/it]
Epoch 3/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.93it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.90it/s]
Epoch 2/32 - Train Loss: 21.689135 - Test Loss: 20.967248
Training epochs:   6%|▋         | 2/32 [03:05<46:25, 92.86s/it]
Epoch 3/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 3/32:   2%|▏         | 1/52 [00:01<01:31,  1.79s/it][A
Epoch 3/32:   2%|▏         | 1/52 [00:01<01:24,  1.65s/it][A
Epoch 3/32:   4%|▍         | 2/52 [00:03<01:22,  1.65s/it][A
Epoch 3/32:   4%|▍         | 2/52 [00:03<01:25,  1.71s/it][A
Epoch 3/32:   6%|▌         | 3/52 [00:04<01:21,  1.67s/it][A
Epoch 3/32:   6%|▌         | 3/52 [00:05<01:23,  1.70s/it][A
Epoch 3/32:   8%|▊         | 4/52 [00:06<01:20,  1.68s/it][A
Epoch 3/32:   8%|▊         | 4/52 [00:06<01:21,  1.70s/it][A
Epoch 3/32:  10%|▉         | 5/52 [00:08<01:19,  1.68s/it][A
Epoch 3/32:  10%|▉         | 5/52 [00:08<01:19,  1.69s/it][A
Epoch 3/32:  12%|█▏        | 6/52 [00:10<01:17,  1.68s/it][A
Epoch 3/32:  12%|█▏        | 6/52 [00:10<01:17,  1.69s/it][A
Epoch 3/32:  13%|█▎        | 7/52 [00:11<01:15,  1.68s/it][A
Epoch 3/32:  13%|█▎        | 7/52 [00:11<01:15,  1.69s/it][A

Epoch 3/32:  15%|█▌        | 8/52 [00:13<01:13,  1.68s/it][AEpoch 3/32:  15%|█▌        | 8/52 [00:13<01:14,  1.68s/it][A
Epoch 3/32:  17%|█▋        | 9/52 [00:15<01:12,  1.68s/it][A
Epoch 3/32:  17%|█▋        | 9/52 [00:15<01:12,  1.68s/it][A
Epoch 3/32:  19%|█▉        | 10/52 [00:16<01:10,  1.68s/it][A
Epoch 3/32:  19%|█▉        | 10/52 [00:16<01:10,  1.68s/it][A
Epoch 3/32:  21%|██        | 11/52 [00:18<01:08,  1.68s/it][A
Epoch 3/32:  21%|██        | 11/52 [00:18<01:08,  1.68s/it][A
Epoch 3/32:  23%|██▎       | 12/52 [00:20<01:07,  1.68s/it][A
Epoch 3/32:  23%|██▎       | 12/52 [00:20<01:07,  1.68s/it][A
Epoch 3/32:  25%|██▌       | 13/52 [00:21<01:05,  1.68s/it][A
Epoch 3/32:  25%|██▌       | 13/52 [00:21<01:05,  1.68s/it][A
Epoch 3/32:  27%|██▋       | 14/52 [00:23<01:03,  1.67s/it][A
Epoch 3/32:  27%|██▋       | 14/52 [00:23<01:03,  1.67s/it][A
Epoch 3/32:  29%|██▉       | 15/52 [00:25<01:02,  1.68s/it][A
Epoch 3/32:  29%|██▉       | 15/52 [00:25<01:02,  1.68s/it][A
Epoch 3/32:  31%|███       | 16/52 [00:26<01:00,  1.68s/it][A
Epoch 3/32:  31%|███       | 16/52 [00:26<01:00,  1.68s/it][A
Epoch 3/32:  33%|███▎      | 17/52 [00:28<00:58,  1.68s/it][A
Epoch 3/32:  33%|███▎      | 17/52 [00:28<00:58,  1.68s/it][A
Epoch 3/32:  35%|███▍      | 18/52 [00:30<00:57,  1.68s/it][A
Epoch 3/32:  35%|███▍      | 18/52 [00:30<00:57,  1.68s/it][A
Epoch 3/32:  37%|███▋      | 19/52 [00:32<00:55,  1.69s/it][A
Epoch 3/32:  37%|███▋      | 19/52 [00:31<00:55,  1.69s/it][A
Epoch 3/32:  38%|███▊      | 20/52 [00:33<00:54,  1.69s/it][A
Epoch 3/32:  38%|███▊      | 20/52 [00:33<00:54,  1.69s/it][A
Epoch 3/32:  40%|████      | 21/52 [00:35<00:52,  1.68s/it][A
Epoch 3/32:  40%|████      | 21/52 [00:35<00:52,  1.69s/it][A
Epoch 3/32:  42%|████▏     | 22/52 [00:36<00:50,  1.69s/it][A
Epoch 3/32:  42%|████▏     | 22/52 [00:37<00:50,  1.69s/it][A
Epoch 3/32:  44%|████▍     | 23/52 [00:38<00:48,  1.68s/it][A
Epoch 3/32:  44%|████▍     | 23/52 [00:38<00:48,  1.68s/it][A

Epoch 3/32:  46%|████▌     | 24/52 [00:40<00:46,  1.68s/it][AEpoch 3/32:  46%|████▌     | 24/52 [00:40<00:46,  1.68s/it][A
Epoch 3/32:  48%|████▊     | 25/52 [00:42<00:45,  1.68s/it][A
Epoch 3/32:  48%|████▊     | 25/52 [00:41<00:45,  1.68s/it][A
Epoch 3/32:  50%|█████     | 26/52 [00:43<00:43,  1.68s/it][A
Epoch 3/32:  50%|█████     | 26/52 [00:43<00:43,  1.68s/it][A
Epoch 3/32:  52%|█████▏    | 27/52 [00:45<00:42,  1.68s/it][A
Epoch 3/32:  52%|█████▏    | 27/52 [00:45<00:42,  1.68s/it][A

Epoch 3/32:  54%|█████▍    | 28/52 [00:47<00:40,  1.69s/it][AEpoch 3/32:  54%|█████▍    | 28/52 [00:47<00:40,  1.69s/it][A
Epoch 3/32:  56%|█████▌    | 29/52 [00:48<00:38,  1.69s/it][A
Epoch 3/32:  56%|█████▌    | 29/52 [00:48<00:38,  1.69s/it][A
Epoch 3/32:  58%|█████▊    | 30/52 [00:50<00:37,  1.69s/it][A
Epoch 3/32:  58%|█████▊    | 30/52 [00:50<00:37,  1.69s/it][A
Epoch 3/32:  60%|█████▉    | 31/52 [00:52<00:35,  1.69s/it][A
Epoch 3/32:  60%|█████▉    | 31/52 [00:52<00:35,  1.69s/it][A
Epoch 3/32:  62%|██████▏   | 32/52 [00:53<00:33,  1.69s/it][A
Epoch 3/32:  62%|██████▏   | 32/52 [00:53<00:33,  1.69s/it][A
Epoch 3/32:  63%|██████▎   | 33/52 [00:55<00:32,  1.69s/it][A
Epoch 3/32:  63%|██████▎   | 33/52 [00:55<00:32,  1.69s/it][A
Epoch 3/32:  65%|██████▌   | 34/52 [00:57<00:30,  1.68s/it][A
Epoch 3/32:  65%|██████▌   | 34/52 [00:57<00:30,  1.69s/it][A
Epoch 3/32:  67%|██████▋   | 35/52 [00:58<00:28,  1.68s/it][A
Epoch 3/32:  67%|██████▋   | 35/52 [00:59<00:28,  1.68s/it][A
Epoch 3/32:  69%|██████▉   | 36/52 [01:00<00:26,  1.68s/it][A
Epoch 3/32:  69%|██████▉   | 36/52 [01:00<00:26,  1.68s/it][A
Epoch 3/32:  71%|███████   | 37/52 [01:02<00:25,  1.68s/it][A
Epoch 3/32:  71%|███████   | 37/52 [01:02<00:25,  1.68s/it][A
Epoch 3/32:  73%|███████▎  | 38/52 [01:03<00:23,  1.68s/it][A
Epoch 3/32:  73%|███████▎  | 38/52 [01:04<00:23,  1.68s/it][A
Epoch 3/32:  75%|███████▌  | 39/52 [01:05<00:21,  1.68s/it][A
Epoch 3/32:  75%|███████▌  | 39/52 [01:05<00:21,  1.68s/it][A
Epoch 3/32:  77%|███████▋  | 40/52 [01:07<00:20,  1.68s/it][A
Epoch 3/32:  77%|███████▋  | 40/52 [01:07<00:20,  1.68s/it][A
Epoch 3/32:  79%|███████▉  | 41/52 [01:08<00:18,  1.69s/it][A
Epoch 3/32:  79%|███████▉  | 41/52 [01:09<00:18,  1.69s/it][A
Epoch 3/32:  81%|████████  | 42/52 [01:10<00:16,  1.69s/it][A
Epoch 3/32:  81%|████████  | 42/52 [01:10<00:16,  1.70s/it][A
Epoch 3/32:  83%|████████▎ | 43/52 [01:12<00:15,  1.70s/it][A
Epoch 3/32:  83%|████████▎ | 43/52 [01:12<00:15,  1.70s/it][A
Epoch 3/32:  85%|████████▍ | 44/52 [01:14<00:13,  1.70s/it][A
Epoch 3/32:  85%|████████▍ | 44/52 [01:14<00:13,  1.70s/it][A
Epoch 3/32:  87%|████████▋ | 45/52 [01:15<00:11,  1.70s/it][A
Epoch 3/32:  87%|████████▋ | 45/52 [01:15<00:11,  1.70s/it][A
Epoch 3/32:  88%|████████▊ | 46/52 [01:17<00:10,  1.70s/it][A
Epoch 3/32:  88%|████████▊ | 46/52 [01:17<00:10,  1.70s/it][A
Epoch 3/32:  90%|█████████ | 47/52 [01:19<00:08,  1.70s/it][A
Epoch 3/32:  90%|█████████ | 47/52 [01:19<00:08,  1.70s/it][A
Epoch 3/32:  92%|█████████▏| 48/52 [01:20<00:06,  1.70s/it][A
Epoch 3/32:  92%|█████████▏| 48/52 [01:21<00:06,  1.70s/it][A
Epoch 3/32:  94%|█████████▍| 49/52 [01:22<00:05,  1.70s/it][A
Epoch 3/32:  94%|█████████▍| 49/52 [01:22<00:05,  1.70s/it][A
Epoch 3/32:  96%|█████████▌| 50/52 [01:24<00:03,  1.70s/it][A
Epoch 3/32:  96%|█████████▌| 50/52 [01:24<00:03,  1.69s/it][A
Epoch 3/32:  98%|█████████▊| 51/52 [01:25<00:01,  1.70s/it][A
Epoch 3/32:  98%|█████████▊| 51/52 [01:26<00:01,  1.70s/it][A
Epoch 3/32: 100%|██████████| 52/52 [01:27<00:00,  1.60s/it][AEpoch 3/32: 100%|██████████| 52/52 [01:27<00:00,  1.68s/it]

[β=2.0] Epoch 3 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=8.878e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=2.158e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=7.861e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=1.142e+00
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=4.573e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.874e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=3.221e-02
  layer model_7__forward_module.module.output_conv act-std: mean=4.311e-01
  layer model_6__forward_module.module.input_conv act-std: mean=4.925e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=2.876e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.183e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=1.203e+00
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=5.253e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.128e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=3.453e-02
  layer model_6__forward_module.module.output_conv act-std: mean=3.745e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.634e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.848e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=1.390e+00
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=2.336e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=9.369e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=1.138e+00
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=9.717e-03
  layer model_5__forward_module.module.output_conv act-std: mean=9.803e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.145e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.473e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=1.497e+00
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=2.692e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=8.086e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=1.153e+00
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=2.870e-02
  layer model_4__forward_module.module.output_conv act-std: mean=1.080e+00
  layer model_3__forward_module.module.input_conv act-std: mean=1.014e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.191e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=8.216e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=1.253e+00
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=5.306e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.252e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.236e-02
  layer model_3__forward_module.module.output_conv act-std: mean=4.919e-01
  layer model_2__forward_module.module.input_conv act-std: mean=6.269e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.311e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=1.112e+00
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.601e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=6.129e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=6.839e-01
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=3.555e-02
  layer model_2__forward_module.module.output_conv act-std: mean=5.242e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.725e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.698e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=9.282e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=1.445e+00
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=6.713e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.530e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=1.028e-02
  layer model_1__forward_module.module.output_conv act-std: mean=6.352e-01
  layer model_0__forward_module.module.input_conv act-std: mean=6.922e-02
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.205e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=1.163e+00
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.325e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=6.769e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=6.055e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=5.072e-02
  layer model_0__forward_module.module.output_conv act-std: mean=4.806e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=9.710e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=3.236e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=7.785e-02
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=6.184e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=7.068e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.358e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.462e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.717e-03
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.047e-03
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.421e-03

  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.043e-03
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.813e-03
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.234e-04
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=6.013e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=8.126e-04
Epoch 3/32: 100%|██████████| 52/52 [01:27<00:00,  1.60s/it]  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=5.752e-03[A
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=3.151e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.858e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=3.878e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=7.896e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=8.975e-02
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.076e-02
Epoch 3/32: 100%|██████████| 52/52 [01:27<00:00,  1.68s/it]  param model_1__forward_module.module.input_norm.weight grad-norm: mean=5.408e-02

  param model_1__forward_module.module.input_norm.bias grad-norm: mean=5.186e-02
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.606e-03
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.078e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.302e-03
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.088e-03
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.112e-03
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.746e-03
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.172e-03
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.530e-04
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.936e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=3.135e-04
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=2.923e-03
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=1.552e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.272e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.418e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=9.409e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=4.389e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.031e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=5.935e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.014e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.744e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.740e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.615e-03
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.485e-03
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.029e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.509e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.095e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.145e-04
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=4.952e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=6.156e-04
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=4.799e-03
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=3.357e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.415e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=3.294e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=9.698e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=5.938e-01
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=1.377e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=9.307e-02
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.145e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.986e-03
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.557e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.529e-03
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=6.763e-03
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=9.604e-03
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=4.179e-03
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=9.638e-03
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.230e-04
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=4.089e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=5.649e-04
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=4.010e-03
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=3.005e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.211e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=3.382e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=9.290e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=2.971e-02
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=6.681e-03
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.033e-02
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.097e-02
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.457e-03
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.535e-03
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.692e-03
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=6.767e-04
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.418e-03
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=4.283e-04
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.145e-03
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.272e-04
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=1.814e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.388e-04
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=2.301e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=7.275e-04
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.164e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.654e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=9.112e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=3.233e-02
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=6.757e-03
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.661e-02
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.518e-02
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.713e-03
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.087e-03
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.565e-03
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=6.862e-04
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.019e-03
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=4.010e-04
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.374e-03
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.983e-05
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.297e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=9.581e-05
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.368e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=6.598e-04
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.711e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.179e-02
  param model_6__forward_module.module.output_scale grad-norm: mean=1.327e+00
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.166e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=2.669e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.319e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.554e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.094e-03
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.985e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.468e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.034e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.471e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=6.074e-03
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.598e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.660e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=6.694e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=1.266e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=5.873e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=6.708e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.633e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=6.224e-02
  param model_7__forward_module.module.output_scale grad-norm: mean=1.072e+00
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=9.129e-01

  param model_7__forward_module.module.input_conv.bias grad-norm: mean=1.996e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.199e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.266e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.818e-03
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.768e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.832e-03
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=7.946e-03
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.046e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=5.132e-03
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.018e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.205e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=4.390e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=7.746e-04
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=4.466e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=3.165e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.197e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=4.232e-02

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.88it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.87it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.88it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.87it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.90it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.89it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.92it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.89it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.93it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.90it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.92it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.91it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.91it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.91it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.90it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.92it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.90it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.90it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.90it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.91it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.90it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.91it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.90it/s][A
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.91it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.91it/s]
Training epochs:   9%|▉         | 3/32 [04:39<45:11, 93.51s/it]
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s][A
Epoch 4/32:   0%|          | 0/52 [00:00<?, ?it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.90it/s]
Epoch 3/32 - Train Loss: 20.671482 - Test Loss: 20.609944
Training epochs:   9%|▉         | 3/32 [04:39<45:11, 93.51s/it]
Epoch 4/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 4/32:   2%|▏         | 1/52 [00:01<01:27,  1.71s/it][A
Epoch 4/32:   2%|▏         | 1/52 [00:01<01:24,  1.66s/it][A
Epoch 4/32:   4%|▍         | 2/52 [00:03<01:24,  1.70s/it][A
Epoch 4/32:   4%|▍         | 2/52 [00:03<01:23,  1.68s/it][A
Epoch 4/32:   6%|▌         | 3/52 [00:05<01:23,  1.71s/it][A
Epoch 4/32:   6%|▌         | 3/52 [00:05<01:23,  1.70s/it][A
Epoch 4/32:   8%|▊         | 4/52 [00:06<01:21,  1.71s/it][A
Epoch 4/32:   8%|▊         | 4/52 [00:06<01:21,  1.70s/it][A
Epoch 4/32:  10%|▉         | 5/52 [00:08<01:19,  1.70s/it][A
Epoch 4/32:  10%|▉         | 5/52 [00:08<01:19,  1.69s/it][A
Epoch 4/32:  12%|█▏        | 6/52 [00:10<01:17,  1.69s/it][A
Epoch 4/32:  12%|█▏        | 6/52 [00:10<01:17,  1.69s/it][A
Epoch 4/32:  13%|█▎        | 7/52 [00:11<01:16,  1.70s/it][A
Epoch 4/32:  13%|█▎        | 7/52 [00:11<01:16,  1.70s/it][A
Epoch 4/32:  15%|█▌        | 8/52 [00:13<01:14,  1.70s/it][A
Epoch 4/32:  15%|█▌        | 8/52 [00:13<01:14,  1.69s/it][A
Epoch 4/32:  17%|█▋        | 9/52 [00:15<01:14,  1.74s/it][A
Epoch 4/32:  17%|█▋        | 9/52 [00:15<01:15,  1.75s/it][A

Epoch 4/32:  19%|█▉        | 10/52 [00:17<01:12,  1.73s/it][AEpoch 4/32:  19%|█▉        | 10/52 [00:17<01:12,  1.73s/it][A
Epoch 4/32:  21%|██        | 11/52 [00:18<01:10,  1.72s/it][A
Epoch 4/32:  21%|██        | 11/52 [00:18<01:10,  1.72s/it][A
Epoch 4/32:  23%|██▎       | 12/52 [00:20<01:09,  1.73s/it][A
Epoch 4/32:  23%|██▎       | 12/52 [00:20<01:09,  1.73s/it][A
Epoch 4/32:  25%|██▌       | 13/52 [00:22<01:06,  1.72s/it][A
Epoch 4/32:  25%|██▌       | 13/52 [00:22<01:06,  1.72s/it][A
Epoch 4/32:  27%|██▋       | 14/52 [00:23<01:04,  1.71s/it][A
Epoch 4/32:  27%|██▋       | 14/52 [00:23<01:04,  1.71s/it][A

Epoch 4/32:  29%|██▉       | 15/52 [00:25<01:03,  1.70s/it][AEpoch 4/32:  29%|██▉       | 15/52 [00:25<01:03,  1.71s/it][A
Epoch 4/32:  31%|███       | 16/52 [00:27<01:01,  1.71s/it][A
Epoch 4/32:  31%|███       | 16/52 [00:27<01:01,  1.71s/it][A
Epoch 4/32:  33%|███▎      | 17/52 [00:29<00:59,  1.71s/it][A
Epoch 4/32:  33%|███▎      | 17/52 [00:29<00:59,  1.71s/it][A
Epoch 4/32:  35%|███▍      | 18/52 [00:30<00:57,  1.70s/it][A
Epoch 4/32:  35%|███▍      | 18/52 [00:30<00:57,  1.71s/it][A
Epoch 4/32:  37%|███▋      | 19/52 [00:32<00:56,  1.71s/it][A
Epoch 4/32:  37%|███▋      | 19/52 [00:32<00:56,  1.71s/it][A
Epoch 4/32:  38%|███▊      | 20/52 [00:34<00:54,  1.70s/it][A
Epoch 4/32:  38%|███▊      | 20/52 [00:34<00:54,  1.70s/it][A
Epoch 4/32:  40%|████      | 21/52 [00:35<00:52,  1.70s/it][A
Epoch 4/32:  40%|████      | 21/52 [00:35<00:52,  1.70s/it][A
Epoch 4/32:  42%|████▏     | 22/52 [00:37<00:51,  1.70s/it][A
Epoch 4/32:  42%|████▏     | 22/52 [00:37<00:51,  1.71s/it][A
Epoch 4/32:  44%|████▍     | 23/52 [00:39<00:49,  1.71s/it][A
Epoch 4/32:  44%|████▍     | 23/52 [00:39<00:49,  1.71s/it][A
Epoch 4/32:  46%|████▌     | 24/52 [00:41<00:47,  1.71s/it][A
Epoch 4/32:  46%|████▌     | 24/52 [00:40<00:47,  1.71s/it][A
Epoch 4/32:  48%|████▊     | 25/52 [00:42<00:46,  1.71s/it][A
Epoch 4/32:  48%|████▊     | 25/52 [00:42<00:46,  1.71s/it][A
Epoch 4/32:  50%|█████     | 26/52 [00:44<00:44,  1.71s/it][A
Epoch 4/32:  50%|█████     | 26/52 [00:44<00:44,  1.71s/it][A
Epoch 4/32:  52%|█████▏    | 27/52 [00:46<00:42,  1.71s/it][A
Epoch 4/32:  52%|█████▏    | 27/52 [00:46<00:42,  1.71s/it][A
Epoch 4/32:  54%|█████▍    | 28/52 [00:47<00:40,  1.70s/it][A
Epoch 4/32:  54%|█████▍    | 28/52 [00:47<00:40,  1.70s/it][A

Epoch 4/32:  56%|█████▌    | 29/52 [00:49<00:39,  1.71s/it][AEpoch 4/32:  56%|█████▌    | 29/52 [00:49<00:39,  1.71s/it][A
Epoch 4/32:  58%|█████▊    | 30/52 [00:51<00:37,  1.71s/it][A
Epoch 4/32:  58%|█████▊    | 30/52 [00:51<00:37,  1.71s/it][A

Epoch 4/32:  60%|█████▉    | 31/52 [00:52<00:35,  1.71s/it][AEpoch 4/32:  60%|█████▉    | 31/52 [00:52<00:35,  1.71s/it][A
Epoch 4/32:  62%|██████▏   | 32/52 [00:54<00:34,  1.71s/it][A
Epoch 4/32:  62%|██████▏   | 32/52 [00:54<00:34,  1.71s/it][A
Epoch 4/32:  63%|██████▎   | 33/52 [00:56<00:32,  1.71s/it][A
Epoch 4/32:  63%|██████▎   | 33/52 [00:56<00:32,  1.71s/it][A
Epoch 4/32:  65%|██████▌   | 34/52 [00:58<00:30,  1.71s/it][A
Epoch 4/32:  65%|██████▌   | 34/52 [00:58<00:30,  1.71s/it][A
Epoch 4/32:  67%|██████▋   | 35/52 [00:59<00:29,  1.71s/it][A
Epoch 4/32:  67%|██████▋   | 35/52 [00:59<00:29,  1.71s/it][A
Epoch 4/32:  69%|██████▉   | 36/52 [01:01<00:27,  1.71s/it][A
Epoch 4/32:  69%|██████▉   | 36/52 [01:01<00:27,  1.71s/it][A
Epoch 4/32:  71%|███████   | 37/52 [01:03<00:25,  1.71s/it][A
Epoch 4/32:  71%|███████   | 37/52 [01:03<00:25,  1.71s/it][A
Epoch 4/32:  73%|███████▎  | 38/52 [01:04<00:23,  1.71s/it][A
Epoch 4/32:  73%|███████▎  | 38/52 [01:04<00:23,  1.71s/it][A
Epoch 4/32:  75%|███████▌  | 39/52 [01:06<00:22,  1.71s/it][A
Epoch 4/32:  75%|███████▌  | 39/52 [01:06<00:22,  1.71s/it][A
Epoch 4/32:  77%|███████▋  | 40/52 [01:08<00:20,  1.71s/it][A
Epoch 4/32:  77%|███████▋  | 40/52 [01:08<00:20,  1.71s/it][A
Epoch 4/32:  79%|███████▉  | 41/52 [01:10<00:18,  1.71s/it][A
Epoch 4/32:  79%|███████▉  | 41/52 [01:10<00:18,  1.71s/it][A
Epoch 4/32:  81%|████████  | 42/52 [01:11<00:17,  1.72s/it][A
Epoch 4/32:  81%|████████  | 42/52 [01:11<00:17,  1.72s/it][A
Epoch 4/32:  83%|████████▎ | 43/52 [01:13<00:15,  1.72s/it][A
Epoch 4/32:  83%|████████▎ | 43/52 [01:13<00:15,  1.72s/it][A
Epoch 4/32:  85%|████████▍ | 44/52 [01:15<00:13,  1.72s/it][A
Epoch 4/32:  85%|████████▍ | 44/52 [01:15<00:13,  1.72s/it][A
Epoch 4/32:  87%|████████▋ | 45/52 [01:16<00:12,  1.72s/it][A
Epoch 4/32:  87%|████████▋ | 45/52 [01:17<00:12,  1.72s/it][A
Epoch 4/32:  88%|████████▊ | 46/52 [01:18<00:10,  1.72s/it][A
Epoch 4/32:  88%|████████▊ | 46/52 [01:18<00:10,  1.72s/it][A
Epoch 4/32:  90%|█████████ | 47/52 [01:20<00:08,  1.71s/it][A
Epoch 4/32:  90%|█████████ | 47/52 [01:20<00:08,  1.72s/it][A
Epoch 4/32:  92%|█████████▏| 48/52 [01:22<00:06,  1.71s/it][A
Epoch 4/32:  92%|█████████▏| 48/52 [01:22<00:06,  1.71s/it][A
Epoch 4/32:  94%|█████████▍| 49/52 [01:23<00:05,  1.71s/it][A
Epoch 4/32:  94%|█████████▍| 49/52 [01:23<00:05,  1.71s/it][A
Epoch 4/32:  96%|█████████▌| 50/52 [01:25<00:03,  1.71s/it][A
Epoch 4/32:  96%|█████████▌| 50/52 [01:25<00:03,  1.71s/it][A
Epoch 4/32:  98%|█████████▊| 51/52 [01:27<00:01,  1.72s/it][A
Epoch 4/32:  98%|█████████▊| 51/52 [01:27<00:01,  1.72s/it][A
Epoch 4/32: 100%|██████████| 52/52 [01:28<00:00,  1.67s/it][AEpoch 4/32: 100%|██████████| 52/52 [01:28<00:00,  1.71s/it]

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Epoch 4/32: 100%|██████████| 52/52 [01:28<00:00,  1.67s/it][AEpoch 4/32: 100%|██████████| 52/52 [01:28<00:00,  1.71s/it]

[β=2.0] Epoch 4 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=4.860e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=1.921e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=5.173e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=7.237e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=3.128e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.501e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.499e-01
  layer model_7__forward_module.module.output_conv act-std: mean=2.847e-01
  layer model_6__forward_module.module.input_conv act-std: mean=5.377e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=2.675e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=6.430e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=6.044e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=3.309e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=6.249e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.335e-01
  layer model_6__forward_module.module.output_conv act-std: mean=2.541e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.386e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.524e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=1.059e+00
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.784e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=7.145e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=8.852e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.563e-02
  layer model_5__forward_module.module.output_conv act-std: mean=7.549e-01
  layer model_4__forward_module.module.input_conv act-std: mean=9.696e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=3.966e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=1.328e+00
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=2.375e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=7.201e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=1.026e+00
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.910e-02
  layer model_4__forward_module.module.output_conv act-std: mean=9.581e-01
  layer model_3__forward_module.module.input_conv act-std: mean=6.649e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.051e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=5.814e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=9.474e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=4.044e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.379e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=1.266e-01
  layer model_3__forward_module.module.output_conv act-std: mean=3.724e-01
  layer model_2__forward_module.module.input_conv act-std: mean=6.358e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.810e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.653e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.322e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=4.519e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=7.503e-01
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=1.423e-01
  layer model_2__forward_module.module.output_conv act-std: mean=3.927e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.225e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.280e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=7.426e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=1.133e+00
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=5.164e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=5.335e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=4.126e-02
  layer model_1__forward_module.module.output_conv act-std: mean=5.060e-01
  layer model_0__forward_module.module.input_conv act-std: mean=7.969e-02
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.583e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=7.514e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.009e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=4.931e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=7.300e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=2.160e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.756e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=8.567e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.005e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.508e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.117e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.919e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.192e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.936e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.894e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.438e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.747e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=8.339e-03
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.914e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.054e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=1.226e-02
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.207e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.388e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.467e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.719e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=5.711e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=6.686e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=2.413e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=5.748e-02
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.149e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.200e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.370e-03
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.472e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.485e-03
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=7.244e-03
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=8.745e-03
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=4.260e-03
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.307e-03
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.787e-04
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=3.797e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=5.308e-04
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=3.931e-03
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=2.534e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.258e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=3.515e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=8.950e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.448e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.236e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.079e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.566e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.164e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.261e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.602e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.196e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.789e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=7.352e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.766e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.905e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=9.407e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.644e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=9.666e-03
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=6.284e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.541e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=5.068e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=9.175e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.100e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=2.605e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.498e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=2.153e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.028e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.658e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.778e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.558e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.817e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=9.084e-03
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.623e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.220e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=7.752e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.546e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=7.473e-03
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=5.655e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.365e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=5.459e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=6.555e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=6.807e-02
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=1.530e-02
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.166e-02
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.120e-02
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.954e-03
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.709e-03
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.230e-03
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.298e-03
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.030e-03
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=8.010e-04
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.220e-03
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.327e-04
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=1.949e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.754e-04
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=2.889e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.008e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.642e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=3.694e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=6.672e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.446e-01
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.226e-02
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=5.188e-02
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=5.136e-02
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.469e-03
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.060e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.492e-03
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.459e-03
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.298e-03
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.429e-03
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.604e-03
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.360e-04
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.984e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.040e-04
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=2.118e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.314e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.228e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=3.846e-02
  param model_6__forward_module.module.output_scale grad-norm: mean=1.348e+00
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.596e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=5.961e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.146e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.668e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.750e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.225e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.744e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.322e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.712e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.631e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.746e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.770e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.267e-02
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=3.689e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.619e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.273e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.937e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.144e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.053e+00
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.413e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=5.511e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.293e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.759e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.607e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.859e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.381e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.249e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.483e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.384e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.143e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.696e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.169e-02
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=3.371e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.049e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=7.923e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.525e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=8.303e-02

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.90it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.85it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.91it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.85it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.90it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.85it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.91it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.86it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.90it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.86it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.90it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.86it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.90it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.85it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.88it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.84it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.85it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.83it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.83it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.81it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.84it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.82it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.86it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.84it/s][A
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.86it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s]
Training epochs:  12%|█▎        | 4/32 [06:15<44:04, 94.44s/it]
Epoch 5/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.85it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.84it/s]
Epoch 4/32 - Train Loss: 20.254842 - Test Loss: 20.216836
Training epochs:  12%|█▎        | 4/32 [06:15<44:04, 94.46s/it]
Epoch 5/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 5/32:   2%|▏         | 1/52 [00:01<01:32,  1.81s/it][A
Epoch 5/32:   2%|▏         | 1/52 [00:01<01:26,  1.70s/it][A
Epoch 5/32:   4%|▍         | 2/52 [00:03<01:27,  1.74s/it][A
Epoch 5/32:   4%|▍         | 2/52 [00:03<01:29,  1.79s/it][A
Epoch 5/32:   6%|▌         | 3/52 [00:05<01:25,  1.74s/it][A
Epoch 5/32:   6%|▌         | 3/52 [00:05<01:26,  1.77s/it][A

Epoch 5/32:   8%|▊         | 4/52 [00:07<01:23,  1.74s/it][AEpoch 5/32:   8%|▊         | 4/52 [00:06<01:23,  1.73s/it][A
Epoch 5/32:  10%|▉         | 5/52 [00:08<01:21,  1.72s/it][A
Epoch 5/32:  10%|▉         | 5/52 [00:08<01:21,  1.74s/it][A
Epoch 5/32:  12%|█▏        | 6/52 [00:10<01:19,  1.74s/it][A
Epoch 5/32:  12%|█▏        | 6/52 [00:10<01:19,  1.73s/it][A
Epoch 5/32:  13%|█▎        | 7/52 [00:12<01:18,  1.74s/it][A
Epoch 5/32:  13%|█▎        | 7/52 [00:12<01:18,  1.73s/it][A
Epoch 5/32:  15%|█▌        | 8/52 [00:13<01:16,  1.73s/it][A
Epoch 5/32:  15%|█▌        | 8/52 [00:13<01:15,  1.72s/it][A
Epoch 5/32:  17%|█▋        | 9/52 [00:15<01:13,  1.72s/it][A
Epoch 5/32:  17%|█▋        | 9/52 [00:15<01:13,  1.72s/it][A
Epoch 5/32:  19%|█▉        | 10/52 [00:17<01:12,  1.72s/it][A
Epoch 5/32:  19%|█▉        | 10/52 [00:17<01:12,  1.72s/it][A
Epoch 5/32:  21%|██        | 11/52 [00:19<01:10,  1.73s/it][A
Epoch 5/32:  21%|██        | 11/52 [00:18<01:10,  1.73s/it][A
Epoch 5/32:  23%|██▎       | 12/52 [00:20<01:08,  1.72s/it][A
Epoch 5/32:  23%|██▎       | 12/52 [00:20<01:09,  1.73s/it][A
Epoch 5/32:  25%|██▌       | 13/52 [00:22<01:07,  1.72s/it][A
Epoch 5/32:  25%|██▌       | 13/52 [00:22<01:07,  1.72s/it][A
Epoch 5/32:  27%|██▋       | 14/52 [00:24<01:05,  1.72s/it][A
Epoch 5/32:  27%|██▋       | 14/52 [00:24<01:05,  1.72s/it][A

Epoch 5/32:  29%|██▉       | 15/52 [00:25<01:03,  1.72s/it][AEpoch 5/32:  29%|██▉       | 15/52 [00:25<01:03,  1.72s/it][A
Epoch 5/32:  31%|███       | 16/52 [00:27<01:02,  1.73s/it][A
Epoch 5/32:  31%|███       | 16/52 [00:27<01:02,  1.73s/it][A

Epoch 5/32:  33%|███▎      | 17/52 [00:29<01:00,  1.72s/it][AEpoch 5/32:  33%|███▎      | 17/52 [00:29<01:00,  1.73s/it][A
Epoch 5/32:  35%|███▍      | 18/52 [00:31<00:58,  1.73s/it][A
Epoch 5/32:  35%|███▍      | 18/52 [00:31<00:58,  1.73s/it][A

Epoch 5/32:  37%|███▋      | 19/52 [00:32<00:57,  1.73s/it][AEpoch 5/32:  37%|███▋      | 19/52 [00:32<00:57,  1.73s/it][A
Epoch 5/32:  38%|███▊      | 20/52 [00:34<00:55,  1.73s/it][A
Epoch 5/32:  38%|███▊      | 20/52 [00:34<00:55,  1.73s/it][A
Epoch 5/32:  40%|████      | 21/52 [00:36<00:53,  1.73s/it][A
Epoch 5/32:  40%|████      | 21/52 [00:36<00:53,  1.73s/it][A
Epoch 5/32:  42%|████▏     | 22/52 [00:37<00:51,  1.73s/it][A
Epoch 5/32:  42%|████▏     | 22/52 [00:38<00:51,  1.73s/it][A
Epoch 5/32:  44%|████▍     | 23/52 [00:39<00:50,  1.72s/it][A
Epoch 5/32:  44%|████▍     | 23/52 [00:39<00:50,  1.73s/it][A
Epoch 5/32:  46%|████▌     | 24/52 [00:41<00:48,  1.73s/it][A
Epoch 5/32:  46%|████▌     | 24/52 [00:41<00:48,  1.73s/it][A
Epoch 5/32:  48%|████▊     | 25/52 [00:43<00:46,  1.73s/it][A
Epoch 5/32:  48%|████▊     | 25/52 [00:43<00:46,  1.73s/it][A
Epoch 5/32:  50%|█████     | 26/52 [00:44<00:45,  1.74s/it][A
Epoch 5/32:  50%|█████     | 26/52 [00:45<00:45,  1.74s/it][A
Epoch 5/32:  52%|█████▏    | 27/52 [00:46<00:43,  1.74s/it][A
Epoch 5/32:  52%|█████▏    | 27/52 [00:46<00:43,  1.74s/it][A
Epoch 5/32:  54%|█████▍    | 28/52 [00:48<00:41,  1.74s/it][A
Epoch 5/32:  54%|█████▍    | 28/52 [00:48<00:41,  1.74s/it][A
Epoch 5/32:  56%|█████▌    | 29/52 [00:50<00:40,  1.74s/it][A
Epoch 5/32:  56%|█████▌    | 29/52 [00:50<00:40,  1.74s/it][A
Epoch 5/32:  58%|█████▊    | 30/52 [00:51<00:38,  1.74s/it][A
Epoch 5/32:  58%|█████▊    | 30/52 [00:52<00:38,  1.74s/it][A

Epoch 5/32:  60%|█████▉    | 31/52 [00:53<00:36,  1.74s/it][AEpoch 5/32:  60%|█████▉    | 31/52 [00:53<00:36,  1.74s/it][A

Epoch 5/32:  62%|██████▏   | 32/52 [00:55<00:34,  1.75s/it][AEpoch 5/32:  62%|██████▏   | 32/52 [00:55<00:34,  1.75s/it][A
Epoch 5/32:  63%|██████▎   | 33/52 [00:57<00:33,  1.75s/it][A
Epoch 5/32:  63%|██████▎   | 33/52 [00:57<00:33,  1.75s/it][A
Epoch 5/32:  65%|██████▌   | 34/52 [00:58<00:31,  1.75s/it][A
Epoch 5/32:  65%|██████▌   | 34/52 [00:59<00:31,  1.75s/it][A
Epoch 5/32:  67%|██████▋   | 35/52 [01:00<00:29,  1.74s/it][A
Epoch 5/32:  67%|██████▋   | 35/52 [01:00<00:29,  1.74s/it][A
Epoch 5/32:  69%|██████▉   | 36/52 [01:02<00:27,  1.74s/it][A
Epoch 5/32:  69%|██████▉   | 36/52 [01:02<00:27,  1.74s/it][A
Epoch 5/32:  71%|███████   | 37/52 [01:04<00:26,  1.75s/it][A
Epoch 5/32:  71%|███████   | 37/52 [01:04<00:26,  1.75s/it][A
Epoch 5/32:  73%|███████▎  | 38/52 [01:05<00:24,  1.75s/it][A
Epoch 5/32:  73%|███████▎  | 38/52 [01:06<00:24,  1.75s/it][A

Epoch 5/32:  75%|███████▌  | 39/52 [01:07<00:22,  1.75s/it][AEpoch 5/32:  75%|███████▌  | 39/52 [01:07<00:22,  1.75s/it][A
Epoch 5/32:  77%|███████▋  | 40/52 [01:09<00:20,  1.75s/it][A
Epoch 5/32:  77%|███████▋  | 40/52 [01:09<00:20,  1.75s/it][A
Epoch 5/32:  79%|███████▉  | 41/52 [01:11<00:19,  1.74s/it][A
Epoch 5/32:  79%|███████▉  | 41/52 [01:11<00:19,  1.75s/it][A
Epoch 5/32:  81%|████████  | 42/52 [01:12<00:17,  1.74s/it][A
Epoch 5/32:  81%|████████  | 42/52 [01:12<00:17,  1.74s/it][A
Epoch 5/32:  83%|████████▎ | 43/52 [01:14<00:15,  1.75s/it][A
Epoch 5/32:  83%|████████▎ | 43/52 [01:14<00:15,  1.75s/it][A
Epoch 5/32:  85%|████████▍ | 44/52 [01:16<00:13,  1.75s/it][A
Epoch 5/32:  85%|████████▍ | 44/52 [01:16<00:13,  1.75s/it][A
Epoch 5/32:  87%|████████▋ | 45/52 [01:18<00:12,  1.75s/it][A
Epoch 5/32:  87%|████████▋ | 45/52 [01:18<00:12,  1.75s/it][A
Epoch 5/32:  88%|████████▊ | 46/52 [01:19<00:10,  1.75s/it][A
Epoch 5/32:  88%|████████▊ | 46/52 [01:19<00:10,  1.75s/it][A
Epoch 5/32:  90%|█████████ | 47/52 [01:21<00:08,  1.75s/it][A
Epoch 5/32:  90%|█████████ | 47/52 [01:21<00:08,  1.75s/it][A
Epoch 5/32:  92%|█████████▏| 48/52 [01:23<00:06,  1.75s/it][A
Epoch 5/32:  92%|█████████▏| 48/52 [01:23<00:07,  1.75s/it][A
Epoch 5/32:  94%|█████████▍| 49/52 [01:25<00:05,  1.75s/it][A
Epoch 5/32:  94%|█████████▍| 49/52 [01:25<00:05,  1.75s/it][A
Epoch 5/32:  96%|█████████▌| 50/52 [01:26<00:03,  1.75s/it][A
Epoch 5/32:  96%|█████████▌| 50/52 [01:26<00:03,  1.75s/it][A
Epoch 5/32:  98%|█████████▊| 51/52 [01:28<00:01,  1.76s/it][A
Epoch 5/32:  98%|█████████▊| 51/52 [01:28<00:01,  1.76s/it][A
Epoch 5/32: 100%|██████████| 52/52 [01:30<00:00,  1.66s/it][AEpoch 5/32: 100%|██████████| 52/52 [01:30<00:00,  1.73s/it]

Epoch 5/32: 100%|██████████| 52/52 [01:30<00:00,  1.66s/it][AEpoch 5/32: 100%|██████████| 52/52 [01:30<00:00,  1.73s/it]

[β=2.0] Epoch 5 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=4.455e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=2.224e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=4.445e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=4.185e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=2.266e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=5.370e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=3.159e-01
  layer model_7__forward_module.module.output_conv act-std: mean=2.202e-01
  layer model_6__forward_module.module.input_conv act-std: mean=6.703e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=2.671e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=6.307e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=4.373e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=2.282e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=5.851e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.519e-01
  layer model_6__forward_module.module.output_conv act-std: mean=2.069e-01
  layer model_5__forward_module.module.input_conv act-std: mean=9.114e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.312e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=7.760e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.311e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=4.605e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=6.539e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=9.105e-02
  layer model_5__forward_module.module.output_conv act-std: mean=5.095e-01
  layer model_4__forward_module.module.input_conv act-std: mean=7.511e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.300e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=7.733e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.279e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=3.801e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.250e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=7.887e-02
  layer model_4__forward_module.module.output_conv act-std: mean=4.844e-01
  layer model_3__forward_module.module.input_conv act-std: mean=6.237e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.422e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=4.800e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=7.419e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=3.591e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=9.306e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.049e-01
  layer model_3__forward_module.module.output_conv act-std: mean=3.445e-01
  layer model_2__forward_module.module.input_conv act-std: mean=6.677e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.394e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.422e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.664e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=4.258e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.108e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=3.049e-01

  layer model_2__forward_module.module.output_conv act-std: mean=4.028e-01
  layer model_1__forward_module.module.input_conv act-std: mean=8.795e-02Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.348e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=6.635e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=1.014e+00
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=4.656e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=5.293e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=1.356e-01
  layer model_1__forward_module.module.output_conv act-std: mean=4.504e-01
  layer model_0__forward_module.module.input_conv act-std: mean=8.143e-02
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.760e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=6.972e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.019e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=4.338e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.942e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=3.754e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.647e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=7.348e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.218e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=3.048e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=9.823e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=2.009e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.348e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.859e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.285e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.333e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.429e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.130e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.256e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.972e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=8.890e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=1.776e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.775e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.840e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.931e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=6.876e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=7.011e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=4.333e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.050e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.331e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.552e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.004e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.299e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.153e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.043e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.300e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=6.199e-03
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.223e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.356e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=6.102e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=9.425e-04
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=6.029e-03
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=3.867e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.506e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=4.407e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=7.947e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.426e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=3.386e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.270e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.918e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.760e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.517e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.690e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.322e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.510e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.177e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.829e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.307e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=6.893e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.225e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.889e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=8.364e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.089e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=7.190e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=8.688e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=2.134e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=5.092e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.503e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=2.945e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.695e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.705e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.654e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.502e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.893e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.689e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.703e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.449e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=8.616e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.965e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.603e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=8.713e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.870e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=8.398e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=6.425e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=4.329e-01
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=9.968e-02
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.291e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.498e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.253e-03
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.338e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.852e-03
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=9.038e-03
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.330e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=6.009e-03
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.296e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.376e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=5.313e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=8.224e-04
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=5.464e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=3.463e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.527e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=4.646e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=7.529e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=7.077e-01
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=1.620e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.905e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=2.108e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.517e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.503e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.280e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.088e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.547e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=6.597e-03
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.550e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.510e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=6.051e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=8.344e-04
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=6.029e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=4.565e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.316e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=6.488e-02
  param model_6__forward_module.module.output_scale grad-norm: mean=1.009e+00
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.610e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.571e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.382e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.581e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.227e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.674e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.544e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.983e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.236e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.828e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.417e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.465e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=3.643e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=1.713e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.161e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=9.284e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.544e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.047e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.122e+00
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=4.255e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=9.697e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.398e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=3.428e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.265e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.285e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.252e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.858e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.095e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.128e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.724e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.214e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=9.191e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=3.895e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.847e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.136e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.858e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.403e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.79it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.74it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.81it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.80it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.84it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.81it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.85it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.82it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.85it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.83it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.84it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.81it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.83it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.81it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.83it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.80it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.84it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.81it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.84it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.82it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.84it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.81it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.80it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.78it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s]
Training epochs:  16%|█▌        | 5/32 [07:52<42:58, 95.49s/it]
Epoch 6/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]
Epoch 5/32 - Train Loss: 19.808609 - Test Loss: 19.718011
Training epochs:  16%|█▌        | 5/32 [07:52<42:58, 95.50s/it]
Epoch 6/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 6/32:   2%|▏         | 1/52 [00:01<01:33,  1.83s/it][A
Epoch 6/32:   2%|▏         | 1/52 [00:01<01:27,  1.72s/it][A
Epoch 6/32:   4%|▍         | 2/52 [00:03<01:29,  1.79s/it][A
Epoch 6/32:   4%|▍         | 2/52 [00:03<01:27,  1.75s/it][A
Epoch 6/32:   6%|▌         | 3/52 [00:05<01:25,  1.75s/it][A
Epoch 6/32:   6%|▌         | 3/52 [00:05<01:27,  1.78s/it][A
Epoch 6/32:   8%|▊         | 4/52 [00:06<01:23,  1.75s/it][A
Epoch 6/32:   8%|▊         | 4/52 [00:07<01:24,  1.76s/it][A
Epoch 6/32:  10%|▉         | 5/52 [00:08<01:21,  1.74s/it][A
Epoch 6/32:  10%|▉         | 5/52 [00:08<01:22,  1.75s/it][A
Epoch 6/32:  12%|█▏        | 6/52 [00:10<01:20,  1.74s/it][A
Epoch 6/32:  12%|█▏        | 6/52 [00:10<01:20,  1.75s/it][A
Epoch 6/32:  13%|█▎        | 7/52 [00:12<01:18,  1.75s/it][A
Epoch 6/32:  13%|█▎        | 7/52 [00:12<01:18,  1.75s/it][A
Epoch 6/32:  15%|█▌        | 8/52 [00:13<01:17,  1.75s/it][A
Epoch 6/32:  15%|█▌        | 8/52 [00:14<01:17,  1.76s/it][A
Epoch 6/32:  17%|█▋        | 9/52 [00:15<01:15,  1.76s/it][A
Epoch 6/32:  17%|█▋        | 9/52 [00:15<01:15,  1.76s/it][A
Epoch 6/32:  19%|█▉        | 10/52 [00:17<01:13,  1.76s/it][A
Epoch 6/32:  19%|█▉        | 10/52 [00:17<01:13,  1.76s/it][A
Epoch 6/32:  21%|██        | 11/52 [00:19<01:12,  1.76s/it][A
Epoch 6/32:  21%|██        | 11/52 [00:19<01:12,  1.76s/it][A
Epoch 6/32:  23%|██▎       | 12/52 [00:21<01:10,  1.76s/it][A
Epoch 6/32:  23%|██▎       | 12/52 [00:21<01:10,  1.76s/it][A
Epoch 6/32:  25%|██▌       | 13/52 [00:22<01:08,  1.75s/it][A
Epoch 6/32:  25%|██▌       | 13/52 [00:22<01:08,  1.75s/it][A
Epoch 6/32:  27%|██▋       | 14/52 [00:24<01:06,  1.74s/it][A
Epoch 6/32:  27%|██▋       | 14/52 [00:24<01:06,  1.74s/it][A
Epoch 6/32:  29%|██▉       | 15/52 [00:26<01:04,  1.73s/it][A
Epoch 6/32:  29%|██▉       | 15/52 [00:26<01:04,  1.73s/it][A

Epoch 6/32:  31%|███       | 16/52 [00:27<01:02,  1.73s/it][AEpoch 6/32:  31%|███       | 16/52 [00:28<01:02,  1.73s/it][A
Epoch 6/32:  33%|███▎      | 17/52 [00:29<01:00,  1.73s/it][A
Epoch 6/32:  33%|███▎      | 17/52 [00:29<01:00,  1.73s/it][A
Epoch 6/32:  35%|███▍      | 18/52 [00:31<00:58,  1.73s/it][A
Epoch 6/32:  35%|███▍      | 18/52 [00:31<00:59,  1.74s/it][A
Epoch 6/32:  37%|███▋      | 19/52 [00:33<00:59,  1.80s/it][A
Epoch 6/32:  37%|███▋      | 19/52 [00:33<00:59,  1.80s/it][A
Epoch 6/32:  38%|███▊      | 20/52 [00:35<00:57,  1.79s/it][A
Epoch 6/32:  38%|███▊      | 20/52 [00:35<00:57,  1.79s/it][A
Epoch 6/32:  40%|████      | 21/52 [00:36<00:55,  1.78s/it][A
Epoch 6/32:  40%|████      | 21/52 [00:36<00:55,  1.78s/it][A
Epoch 6/32:  42%|████▏     | 22/52 [00:38<00:52,  1.77s/it][A
Epoch 6/32:  42%|████▏     | 22/52 [00:38<00:52,  1.77s/it][A

Epoch 6/32:  44%|████▍     | 23/52 [00:40<00:51,  1.76s/it][AEpoch 6/32:  44%|████▍     | 23/52 [00:40<00:51,  1.76s/it][A
Epoch 6/32:  46%|████▌     | 24/52 [00:42<00:49,  1.76s/it][A
Epoch 6/32:  46%|████▌     | 24/52 [00:42<00:49,  1.76s/it][A
Epoch 6/32:  48%|████▊     | 25/52 [00:43<00:47,  1.76s/it][A
Epoch 6/32:  48%|████▊     | 25/52 [00:43<00:47,  1.76s/it][A
Epoch 6/32:  50%|█████     | 26/52 [00:45<00:45,  1.76s/it][A
Epoch 6/32:  50%|█████     | 26/52 [00:45<00:45,  1.76s/it][A
Epoch 6/32:  52%|█████▏    | 27/52 [00:47<00:43,  1.75s/it][A
Epoch 6/32:  52%|█████▏    | 27/52 [00:47<00:43,  1.75s/it][A
Epoch 6/32:  54%|█████▍    | 28/52 [00:49<00:42,  1.76s/it][A
Epoch 6/32:  54%|█████▍    | 28/52 [00:49<00:42,  1.76s/it][A
Epoch 6/32:  56%|█████▌    | 29/52 [00:50<00:40,  1.76s/it][A
Epoch 6/32:  56%|█████▌    | 29/52 [00:50<00:40,  1.76s/it][A
Epoch 6/32:  58%|█████▊    | 30/52 [00:52<00:38,  1.75s/it][A
Epoch 6/32:  58%|█████▊    | 30/52 [00:52<00:38,  1.76s/it][A
Epoch 6/32:  60%|█████▉    | 31/52 [00:54<00:36,  1.75s/it][A
Epoch 6/32:  60%|█████▉    | 31/52 [00:54<00:36,  1.76s/it][A
Epoch 6/32:  62%|██████▏   | 32/52 [00:56<00:35,  1.76s/it][A
Epoch 6/32:  62%|██████▏   | 32/52 [00:56<00:35,  1.76s/it][A
Epoch 6/32:  63%|██████▎   | 33/52 [00:57<00:33,  1.77s/it][A
Epoch 6/32:  63%|██████▎   | 33/52 [00:58<00:33,  1.77s/it][A
Epoch 6/32:  65%|██████▌   | 34/52 [00:59<00:31,  1.77s/it][A
Epoch 6/32:  65%|██████▌   | 34/52 [00:59<00:31,  1.77s/it][A
Epoch 6/32:  67%|██████▋   | 35/52 [01:01<00:29,  1.76s/it][A
Epoch 6/32:  67%|██████▋   | 35/52 [01:01<00:29,  1.76s/it][A
Epoch 6/32:  69%|██████▉   | 36/52 [01:03<00:28,  1.75s/it][A
Epoch 6/32:  69%|██████▉   | 36/52 [01:03<00:28,  1.75s/it][A
Epoch 6/32:  71%|███████   | 37/52 [01:04<00:26,  1.76s/it][A
Epoch 6/32:  71%|███████   | 37/52 [01:05<00:26,  1.75s/it][A
Epoch 6/32:  73%|███████▎  | 38/52 [01:06<00:24,  1.76s/it][A
Epoch 6/32:  73%|███████▎  | 38/52 [01:06<00:24,  1.76s/it][A
Epoch 6/32:  75%|███████▌  | 39/52 [01:08<00:22,  1.75s/it][A
Epoch 6/32:  75%|███████▌  | 39/52 [01:08<00:22,  1.75s/it][A
Epoch 6/32:  77%|███████▋  | 40/52 [01:10<00:21,  1.76s/it][A
Epoch 6/32:  77%|███████▋  | 40/52 [01:10<00:21,  1.76s/it][A
Epoch 6/32:  79%|███████▉  | 41/52 [01:11<00:19,  1.76s/it][A
Epoch 6/32:  79%|███████▉  | 41/52 [01:12<00:19,  1.76s/it][A
Epoch 6/32:  81%|████████  | 42/52 [01:13<00:17,  1.75s/it][A
Epoch 6/32:  81%|████████  | 42/52 [01:13<00:17,  1.75s/it][A
Epoch 6/32:  83%|████████▎ | 43/52 [01:15<00:15,  1.76s/it][A
Epoch 6/32:  83%|████████▎ | 43/52 [01:15<00:15,  1.76s/it][A
Epoch 6/32:  85%|████████▍ | 44/52 [01:17<00:14,  1.75s/it][A
Epoch 6/32:  85%|████████▍ | 44/52 [01:17<00:14,  1.75s/it][A
Epoch 6/32:  87%|████████▋ | 45/52 [01:18<00:12,  1.75s/it][A
Epoch 6/32:  87%|████████▋ | 45/52 [01:19<00:12,  1.75s/it][A
Epoch 6/32:  88%|████████▊ | 46/52 [01:20<00:10,  1.75s/it][A
Epoch 6/32:  88%|████████▊ | 46/52 [01:20<00:10,  1.75s/it][A
Epoch 6/32:  90%|█████████ | 47/52 [01:22<00:08,  1.76s/it][A
Epoch 6/32:  90%|█████████ | 47/52 [01:22<00:08,  1.76s/it][A
Epoch 6/32:  92%|█████████▏| 48/52 [01:24<00:07,  1.77s/it][A
Epoch 6/32:  92%|█████████▏| 48/52 [01:24<00:07,  1.77s/it][A
Epoch 6/32:  94%|█████████▍| 49/52 [01:26<00:05,  1.76s/it][A
Epoch 6/32:  94%|█████████▍| 49/52 [01:26<00:05,  1.76s/it][A
Epoch 6/32:  96%|█████████▌| 50/52 [01:27<00:03,  1.76s/it][A
Epoch 6/32:  96%|█████████▌| 50/52 [01:27<00:03,  1.76s/it][A
Epoch 6/32:  98%|█████████▊| 51/52 [01:29<00:01,  1.81s/it][A
Epoch 6/32:  98%|█████████▊| 51/52 [01:29<00:01,  1.81s/it][A
Epoch 6/32: 100%|██████████| 52/52 [01:31<00:00,  1.69s/it][AEpoch 6/32: 100%|██████████| 52/52 [01:31<00:00,  1.75s/it]

[β=2.0] Epoch 6 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=5.047e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=2.229e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=4.802e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.553e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.675e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.589e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=2.910e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.931e-01
  layer model_6__forward_module.module.input_conv act-std: mean=8.282e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=2.457e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=5.431e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.419e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.528e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.295e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.039e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.741e-01
  layer model_5__forward_module.module.input_conv act-std: mean=5.446e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.348e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=5.808e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.213e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=3.746e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=7.210e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=2.242e-01
  layer model_5__forward_module.module.output_conv act-std: mean=3.836e-01
  layer model_4__forward_module.module.input_conv act-std: mean=5.435e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.237e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=6.425e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.187e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=3.258e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.046e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=1.782e-01
  layer model_4__forward_module.module.output_conv act-std: mean=3.899e-01
  layer model_3__forward_module.module.input_conv act-std: mean=6.045e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.480e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=4.488e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=5.942e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=3.160e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=1.045e+00
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.967e-01
  layer model_3__forward_module.module.output_conv act-std: mean=3.098e-01
  layer model_2__forward_module.module.input_conv act-std: mean=7.205e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.674e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.315e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.745e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=3.936e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.176e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=3.530e-01
  layer model_2__forward_module.module.output_conv act-std: mean=3.760e-01
  layer model_1__forward_module.module.input_conv act-std: mean=7.123e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.429e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=5.468e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=8.024e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=4.081e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.616e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=3.153e-01
  layer model_1__forward_module.module.output_conv act-std: mean=3.996e-01
  layer model_0__forward_module.module.input_conv act-std: mean=8.491e-02
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.907e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=7.901e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.196e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.916e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.841e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=3.665e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.446e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=6.037e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=6.394e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.546e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=6.058e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=9.980e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.358e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.577e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.412e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.963e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.117e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=8.457e-03
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.655e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.542e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=5.804e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=1.333e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.255e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=4.878e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.608e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=5.634e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=7.145e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=8.220e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.026e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.428e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.680e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.295e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.450e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.809e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.539e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.854e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=8.421e-03
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.435e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.932e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=5.992e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.145e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=8.581e-03
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=4.394e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.357e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=4.783e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=5.702e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=6.179e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.419e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=5.376e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.028e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.575e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.155e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.360e-02

  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.663e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.611e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=7.405e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.761e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.250e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.128e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=5.959e-04
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.283e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=5.130e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.617e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=5.293e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=6.435e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.253e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=2.964e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=7.806e-02
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.388e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.927e-03
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.023e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.599e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.917e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.278e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.223e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.801e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.456e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.838e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=7.156e-04
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.234e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=5.614e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.374e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=6.154e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=6.357e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=8.657e-01
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.001e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.190e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.636e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.866e-03
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.680e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.163e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.101e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.624e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=7.921e-03
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.518e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.060e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=4.861e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.026e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=6.000e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=4.101e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.361e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=5.534e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=6.746e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.055e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.423e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.433e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=2.164e-01Epoch 6/32: 100%|██████████| 52/52 [01:31<00:00,  1.69s/it]
[A  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.166e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.734e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.709e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.494e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.868e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=8.934e-03
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.799e-02
Epoch 6/32: 100%|██████████| 52/52 [01:31<00:00,  1.76s/it]  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.443e-03

  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=6.324e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.246e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=7.959e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=5.481e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.757e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=7.006e-02
  param model_6__forward_module.module.output_scale grad-norm: mean=7.623e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=8.458e-01
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=1.838e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=7.719e-02
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=8.388e-02
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.152e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.070e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.588e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.064e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.525e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.127e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.535e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.816e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.828e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=1.544e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=5.381e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=6.002e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.199e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=8.442e-02
  param model_7__forward_module.module.output_scale grad-norm: mean=7.878e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=1.567e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=3.412e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.213e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.224e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.175e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.180e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.911e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.403e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.476e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.793e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.156e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.502e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.446e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=1.467e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=8.503e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=5.919e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.163e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=9.130e-02

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.87it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.85it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.88it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:05,  1.86it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.87it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.86it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.87it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:04,  1.87it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.86it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.87it/s][A

Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.87it/s][AEvaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.88it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.86it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.86it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.86it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.85it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.87it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:04<00:02,  1.85it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.87it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.86it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.87it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:05<00:01,  1.86it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.87it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.86it/s][A
Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s]

Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s][AEvaluating: 100%|██████████| 13/13 [00:06<00:00,  1.86it/s]
Epoch 6/32 - Train Loss: 19.479466 - Test Loss: 19.538266
Training epochs:  19%|█▉        | 6/32 [09:30<41:47, 96.44s/it]
Epoch 7/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  19%|█▉        | 6/32 [09:31<41:46, 96.42s/it]
Epoch 7/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 7/32:   2%|▏         | 1/52 [00:01<01:28,  1.73s/it][A
Epoch 7/32:   2%|▏         | 1/52 [00:01<01:27,  1.72s/it][A
Epoch 7/32:   4%|▍         | 2/52 [00:03<01:27,  1.75s/it][A
Epoch 7/32:   4%|▍         | 2/52 [00:03<01:27,  1.76s/it][A

Epoch 7/32:   6%|▌         | 3/52 [00:05<01:26,  1.77s/it][AEpoch 7/32:   6%|▌         | 3/52 [00:05<01:26,  1.77s/it][A
Epoch 7/32:   8%|▊         | 4/52 [00:07<01:24,  1.76s/it][A
Epoch 7/32:   8%|▊         | 4/52 [00:07<01:24,  1.76s/it][A
Epoch 7/32:  10%|▉         | 5/52 [00:08<01:22,  1.75s/it][A
Epoch 7/32:  10%|▉         | 5/52 [00:08<01:22,  1.75s/it][A
Epoch 7/32:  12%|█▏        | 6/52 [00:10<01:20,  1.75s/it][A
Epoch 7/32:  12%|█▏        | 6/52 [00:10<01:20,  1.75s/it][A
Epoch 7/32:  13%|█▎        | 7/52 [00:12<01:18,  1.75s/it][A
Epoch 7/32:  13%|█▎        | 7/52 [00:12<01:18,  1.75s/it][A
Epoch 7/32:  15%|█▌        | 8/52 [00:14<01:17,  1.76s/it][A
Epoch 7/32:  15%|█▌        | 8/52 [00:14<01:17,  1.76s/it][A
Epoch 7/32:  17%|█▋        | 9/52 [00:15<01:15,  1.75s/it][A
Epoch 7/32:  17%|█▋        | 9/52 [00:15<01:15,  1.75s/it][A
Epoch 7/32:  19%|█▉        | 10/52 [00:17<01:13,  1.75s/it][A
Epoch 7/32:  19%|█▉        | 10/52 [00:17<01:13,  1.75s/it][A
Epoch 7/32:  21%|██        | 11/52 [00:19<01:11,  1.75s/it][A
Epoch 7/32:  21%|██        | 11/52 [00:19<01:11,  1.75s/it][A
Epoch 7/32:  23%|██▎       | 12/52 [00:21<01:10,  1.76s/it][A
Epoch 7/32:  23%|██▎       | 12/52 [00:21<01:10,  1.76s/it][A
Epoch 7/32:  25%|██▌       | 13/52 [00:22<01:08,  1.76s/it][A
Epoch 7/32:  25%|██▌       | 13/52 [00:22<01:08,  1.76s/it][A
Epoch 7/32:  27%|██▋       | 14/52 [00:24<01:06,  1.76s/it][A
Epoch 7/32:  27%|██▋       | 14/52 [00:24<01:06,  1.76s/it][A
Epoch 7/32:  29%|██▉       | 15/52 [00:26<01:04,  1.76s/it][A
Epoch 7/32:  29%|██▉       | 15/52 [00:26<01:05,  1.76s/it][A
Epoch 7/32:  31%|███       | 16/52 [00:28<01:03,  1.76s/it][A
Epoch 7/32:  31%|███       | 16/52 [00:28<01:03,  1.76s/it][A
Epoch 7/32:  33%|███▎      | 17/52 [00:29<01:01,  1.77s/it][A
Epoch 7/32:  33%|███▎      | 17/52 [00:29<01:01,  1.77s/it][A
Epoch 7/32:  35%|███▍      | 18/52 [00:31<01:00,  1.77s/it][A
Epoch 7/32:  35%|███▍      | 18/52 [00:31<01:00,  1.77s/it][A
Epoch 7/32:  37%|███▋      | 19/52 [00:33<00:58,  1.77s/it][A
Epoch 7/32:  37%|███▋      | 19/52 [00:33<00:58,  1.77s/it][A
Epoch 7/32:  38%|███▊      | 20/52 [00:35<00:56,  1.77s/it][A
Epoch 7/32:  38%|███▊      | 20/52 [00:35<00:56,  1.77s/it][A
Epoch 7/32:  40%|████      | 21/52 [00:36<00:54,  1.77s/it][A
Epoch 7/32:  40%|████      | 21/52 [00:36<00:55,  1.78s/it][A
Epoch 7/32:  42%|████▏     | 22/52 [00:38<00:53,  1.77s/it][A
Epoch 7/32:  42%|████▏     | 22/52 [00:38<00:53,  1.77s/it][A
Epoch 7/32:  44%|████▍     | 23/52 [00:40<00:51,  1.76s/it][A
Epoch 7/32:  44%|████▍     | 23/52 [00:40<00:51,  1.77s/it][A
Epoch 7/32:  46%|████▌     | 24/52 [00:42<00:49,  1.76s/it][A
Epoch 7/32:  46%|████▌     | 24/52 [00:42<00:49,  1.76s/it][A
Epoch 7/32:  48%|████▊     | 25/52 [00:43<00:47,  1.76s/it][A
Epoch 7/32:  48%|████▊     | 25/52 [00:44<00:47,  1.76s/it][A
Epoch 7/32:  50%|█████     | 26/52 [00:45<00:45,  1.76s/it][A
Epoch 7/32:  50%|█████     | 26/52 [00:45<00:45,  1.77s/it][A
Epoch 7/32:  52%|█████▏    | 27/52 [00:47<00:44,  1.77s/it][A
Epoch 7/32:  52%|█████▏    | 27/52 [00:47<00:44,  1.77s/it][A
Epoch 7/32:  54%|█████▍    | 28/52 [00:49<00:42,  1.77s/it][A
Epoch 7/32:  54%|█████▍    | 28/52 [00:49<00:42,  1.77s/it][A
Epoch 7/32:  56%|█████▌    | 29/52 [00:51<00:40,  1.77s/it][A
Epoch 7/32:  56%|█████▌    | 29/52 [00:51<00:40,  1.77s/it][A
Epoch 7/32:  58%|█████▊    | 30/52 [00:52<00:39,  1.78s/it][A
Epoch 7/32:  58%|█████▊    | 30/52 [00:52<00:39,  1.78s/it][A
Epoch 7/32:  60%|█████▉    | 31/52 [00:54<00:37,  1.78s/it][A
Epoch 7/32:  60%|█████▉    | 31/52 [00:54<00:37,  1.78s/it][A
Epoch 7/32:  62%|██████▏   | 32/52 [00:56<00:35,  1.78s/it][A
Epoch 7/32:  62%|██████▏   | 32/52 [00:56<00:35,  1.78s/it][A
Epoch 7/32:  63%|██████▎   | 33/52 [00:58<00:33,  1.79s/it][A
Epoch 7/32:  63%|██████▎   | 33/52 [00:58<00:33,  1.79s/it][A
Epoch 7/32:  65%|██████▌   | 34/52 [01:00<00:32,  1.78s/it][A
Epoch 7/32:  65%|██████▌   | 34/52 [01:00<00:32,  1.78s/it][A
Epoch 7/32:  67%|██████▋   | 35/52 [01:01<00:30,  1.78s/it][A
Epoch 7/32:  67%|██████▋   | 35/52 [01:01<00:30,  1.78s/it][A
Epoch 7/32:  69%|██████▉   | 36/52 [01:03<00:28,  1.78s/it][A
Epoch 7/32:  69%|██████▉   | 36/52 [01:03<00:28,  1.78s/it][A

Epoch 7/32:  71%|███████   | 37/52 [01:05<00:26,  1.78s/it][AEpoch 7/32:  71%|███████   | 37/52 [01:05<00:26,  1.78s/it][A
Epoch 7/32:  73%|███████▎  | 38/52 [01:07<00:24,  1.78s/it][A
Epoch 7/32:  73%|███████▎  | 38/52 [01:07<00:24,  1.78s/it][A
Epoch 7/32:  75%|███████▌  | 39/52 [01:08<00:23,  1.78s/it][A
Epoch 7/32:  75%|███████▌  | 39/52 [01:08<00:23,  1.78s/it][A
Epoch 7/32:  77%|███████▋  | 40/52 [01:10<00:21,  1.78s/it][A
Epoch 7/32:  77%|███████▋  | 40/52 [01:10<00:21,  1.78s/it][A
Epoch 7/32:  79%|███████▉  | 41/52 [01:12<00:19,  1.77s/it][A
Epoch 7/32:  79%|███████▉  | 41/52 [01:12<00:19,  1.77s/it][A
Epoch 7/32:  81%|████████  | 42/52 [01:14<00:17,  1.77s/it][A
Epoch 7/32:  81%|████████  | 42/52 [01:14<00:17,  1.77s/it][A
Epoch 7/32:  83%|████████▎ | 43/52 [01:15<00:15,  1.77s/it][A
Epoch 7/32:  83%|████████▎ | 43/52 [01:16<00:15,  1.77s/it][A

Epoch 7/32:  85%|████████▍ | 44/52 [01:17<00:14,  1.77s/it][AEpoch 7/32:  85%|████████▍ | 44/52 [01:17<00:14,  1.77s/it][A

Epoch 7/32:  87%|████████▋ | 45/52 [01:19<00:12,  1.78s/it][AEpoch 7/32:  87%|████████▋ | 45/52 [01:19<00:12,  1.78s/it][A
Epoch 7/32:  88%|████████▊ | 46/52 [01:21<00:10,  1.77s/it][A
Epoch 7/32:  88%|████████▊ | 46/52 [01:21<00:10,  1.78s/it][A
Epoch 7/32:  90%|█████████ | 47/52 [01:23<00:08,  1.77s/it][A
Epoch 7/32:  90%|█████████ | 47/52 [01:23<00:08,  1.77s/it][A
Epoch 7/32:  92%|█████████▏| 48/52 [01:24<00:07,  1.77s/it][A
Epoch 7/32:  92%|█████████▏| 48/52 [01:24<00:07,  1.77s/it][A
Epoch 7/32:  94%|█████████▍| 49/52 [01:26<00:05,  1.76s/it][A
Epoch 7/32:  94%|█████████▍| 49/52 [01:26<00:05,  1.77s/it][A

Epoch 7/32:  96%|█████████▌| 50/52 [01:28<00:03,  1.77s/it][AEpoch 7/32:  96%|█████████▌| 50/52 [01:28<00:03,  1.77s/it][A
Epoch 7/32:  98%|█████████▊| 51/52 [01:30<00:01,  1.76s/it][A
Epoch 7/32:  98%|█████████▊| 51/52 [01:30<00:01,  1.77s/it][A
Epoch 7/32: 100%|██████████| 52/52 [01:31<00:00,  1.66s/it][AEpoch 7/32: 100%|██████████| 52/52 [01:31<00:00,  1.76s/it]

Epoch 7/32: 100%|██████████| 52/52 [01:31<00:00,  1.66s/it][AEpoch 7/32: 100%|██████████| 52/52 [01:31<00:00,  1.76s/it]

[β=2.0] Epoch 7 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=5.636e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=2.109e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=4.573e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.978e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.266e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.577e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=2.154e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.700e-01
  layer model_6__forward_module.module.input_conv act-std: mean=9.224e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=2.574e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=4.572e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.933e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.242e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=3.589e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=7.811e-02
  layer model_6__forward_module.module.output_conv act-std: mean=1.577e-01
  layer model_5__forward_module.module.input_conv act-std: mean=5.674e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.461e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.717e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.336e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=3.051e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=8.560e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.790e-01
  layer model_5__forward_module.module.output_conv act-std: mean=3.071e-01
  layer model_4__forward_module.module.input_conv act-std: mean=5.266e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.401e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=5.337e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.380e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=2.662e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.944e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=3.376e-01
  layer model_4__forward_module.module.output_conv act-std: mean=3.201e-01
  layer model_3__forward_module.module.input_conv act-std: mean=6.125e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.380e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=4.107e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=5.314e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=2.817e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=9.852e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.881e-01
  layer model_3__forward_module.module.output_conv act-std: mean=2.776e-01
  layer model_2__forward_module.module.input_conv act-std: mean=7.791e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.522e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=6.613e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.574e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=3.662e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.111e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=3.440e-01
  layer model_2__forward_module.module.output_conv act-std: mean=3.423e-01
  layer model_1__forward_module.module.input_conv act-std: mean=7.180e-02

  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.639e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=4.913e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=6.490e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=3.671e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.108e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=5.049e-01
  layer model_1__forward_module.module.output_conv act-std: mean=3.701e-01
  layer model_0__forward_module.module.input_conv act-std: mean=9.191e-02
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.893e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.101e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.236e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.464e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.200e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=3.111e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.189e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=5.487e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=7.675e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.923e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=6.450e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.281e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.873e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.600e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.014e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.380e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.540e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.470e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.354e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.314e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=9.061e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.293e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.749e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.193e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.877e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=7.693e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=6.837e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=1.291e+00
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=3.225e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=2.088e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=2.352e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.072e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.456e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.808e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.953e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.693e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.525e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.342e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.395e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=5.254e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.099e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.549e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=6.632e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.646e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=6.470e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=5.249e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=7.661e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.771e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=6.267e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.011e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.791e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.376e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.727e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.437e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.793e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.131e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.348e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.496e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=4.695e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.036e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.674e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=7.216e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.820e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=6.935e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=5.903e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.585e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.733e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=9.548e-02
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.768e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.186e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.367e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.835e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.444e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.766e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.886e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.499e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=5.743e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=3.013e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=8.986e-04
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.668e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=8.008e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.664e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=8.877e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=7.400e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.650e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.881e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.865e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.416e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.756e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.234e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.580e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.240e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.453e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.554e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.892e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.881e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=6.309e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.663e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.353e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=7.911e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.825e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=9.440e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=6.583e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.250e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.876e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.564e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=2.236e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.441e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.472e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.943e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.309e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.893e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.253e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.408e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.991e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=4.479e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.097e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.202e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=7.080e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.720e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=8.160e-02
  param model_6__forward_module.module.output_scale grad-norm: mean=6.525e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.017e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=2.226e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=9.778e-02
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.147e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.442e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.817e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.109e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.919e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.412e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.222e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.579e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.187e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.914e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=2.241e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=4.459e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=5.924e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.254e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.439e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=6.908e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=1.363e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=2.975e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.029e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.158e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.132e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=9.376e-03
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.310e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.433e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.226e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.892e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.037e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.639e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.813e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=1.778e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=5.685e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=5.282e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.033e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.073e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.78it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.77it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.78it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.79it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.78it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.78it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.80it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.79it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.81it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.80it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.82it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.80it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.75it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.74it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.75it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.75it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.76it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.76it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.77it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.77it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.77it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.78it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.78it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.79it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]

Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]
Epoch 7/32 - Train Loss: 19.214636 - Test Loss: 19.182581
Training epochs:  22%|██▏       | 7/32 [11:10<40:31, 97.26s/it]Training epochs:  22%|██▏       | 7/32 [11:10<40:31, 97.28s/it]
Epoch 8/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 8/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 8/32:   2%|▏         | 1/52 [00:01<01:29,  1.75s/it][A
Epoch 8/32:   2%|▏         | 1/52 [00:01<01:29,  1.75s/it][A
Epoch 8/32:   4%|▍         | 2/52 [00:03<01:27,  1.76s/it][A
Epoch 8/32:   4%|▍         | 2/52 [00:03<01:27,  1.76s/it][A
Epoch 8/32:   6%|▌         | 3/52 [00:05<01:26,  1.76s/it][A
Epoch 8/32:   6%|▌         | 3/52 [00:05<01:26,  1.76s/it][A

Epoch 8/32:   8%|▊         | 4/52 [00:07<01:24,  1.77s/it][AEpoch 8/32:   8%|▊         | 4/52 [00:07<01:24,  1.77s/it][A
Epoch 8/32:  10%|▉         | 5/52 [00:08<01:23,  1.77s/it][A
Epoch 8/32:  10%|▉         | 5/52 [00:08<01:23,  1.77s/it][A

Epoch 8/32:  12%|█▏        | 6/52 [00:10<01:20,  1.76s/it][AEpoch 8/32:  12%|█▏        | 6/52 [00:10<01:20,  1.76s/it][A
Epoch 8/32:  13%|█▎        | 7/52 [00:12<01:22,  1.83s/it][A
Epoch 8/32:  13%|█▎        | 7/52 [00:12<01:22,  1.83s/it][A
Epoch 8/32:  15%|█▌        | 8/52 [00:14<01:19,  1.81s/it][A
Epoch 8/32:  15%|█▌        | 8/52 [00:14<01:19,  1.81s/it][A
Epoch 8/32:  17%|█▋        | 9/52 [00:16<01:17,  1.80s/it][A
Epoch 8/32:  17%|█▋        | 9/52 [00:16<01:17,  1.80s/it][A
Epoch 8/32:  19%|█▉        | 10/52 [00:17<01:15,  1.80s/it][A
Epoch 8/32:  19%|█▉        | 10/52 [00:17<01:15,  1.80s/it][A
Epoch 8/32:  21%|██        | 11/52 [00:19<01:13,  1.79s/it][A
Epoch 8/32:  21%|██        | 11/52 [00:19<01:13,  1.79s/it][A
Epoch 8/32:  23%|██▎       | 12/52 [00:21<01:11,  1.80s/it][A
Epoch 8/32:  23%|██▎       | 12/52 [00:21<01:11,  1.80s/it][A

Epoch 8/32:  25%|██▌       | 13/52 [00:23<01:10,  1.80s/it][AEpoch 8/32:  25%|██▌       | 13/52 [00:23<01:10,  1.80s/it][A

Epoch 8/32:  27%|██▋       | 14/52 [00:25<01:07,  1.79s/it][AEpoch 8/32:  27%|██▋       | 14/52 [00:25<01:07,  1.79s/it][A
Epoch 8/32:  29%|██▉       | 15/52 [00:26<01:05,  1.78s/it][A
Epoch 8/32:  29%|██▉       | 15/52 [00:26<01:05,  1.78s/it][A
Epoch 8/32:  31%|███       | 16/52 [00:28<01:03,  1.77s/it][A
Epoch 8/32:  31%|███       | 16/52 [00:28<01:03,  1.77s/it][A
Epoch 8/32:  33%|███▎      | 17/52 [00:30<01:02,  1.78s/it][A
Epoch 8/32:  33%|███▎      | 17/52 [00:30<01:02,  1.78s/it][A
Epoch 8/32:  35%|███▍      | 18/52 [00:32<01:00,  1.78s/it][A
Epoch 8/32:  35%|███▍      | 18/52 [00:32<01:00,  1.78s/it][A
Epoch 8/32:  37%|███▋      | 19/52 [00:33<00:58,  1.77s/it][A
Epoch 8/32:  37%|███▋      | 19/52 [00:33<00:58,  1.77s/it][A
Epoch 8/32:  38%|███▊      | 20/52 [00:35<00:56,  1.77s/it][A
Epoch 8/32:  38%|███▊      | 20/52 [00:35<00:56,  1.77s/it][A
Epoch 8/32:  40%|████      | 21/52 [00:37<00:54,  1.77s/it][A
Epoch 8/32:  40%|████      | 21/52 [00:37<00:54,  1.77s/it][A
Epoch 8/32:  42%|████▏     | 22/52 [00:39<00:53,  1.78s/it][A
Epoch 8/32:  42%|████▏     | 22/52 [00:39<00:53,  1.78s/it][A
Epoch 8/32:  44%|████▍     | 23/52 [00:40<00:51,  1.78s/it][A
Epoch 8/32:  44%|████▍     | 23/52 [00:40<00:51,  1.78s/it][A
Epoch 8/32:  46%|████▌     | 24/52 [00:42<00:49,  1.77s/it][A
Epoch 8/32:  46%|████▌     | 24/52 [00:42<00:49,  1.78s/it][A
Epoch 8/32:  48%|████▊     | 25/52 [00:44<00:47,  1.77s/it][A
Epoch 8/32:  48%|████▊     | 25/52 [00:44<00:47,  1.77s/it][A
Epoch 8/32:  50%|█████     | 26/52 [00:46<00:45,  1.77s/it]
[AEpoch 8/32:  50%|█████     | 26/52 [00:46<00:45,  1.77s/it][A
Epoch 8/32:  52%|█████▏    | 27/52 [00:48<00:44,  1.77s/it][A
Epoch 8/32:  52%|█████▏    | 27/52 [00:48<00:44,  1.77s/it][A
Epoch 8/32:  54%|█████▍    | 28/52 [00:49<00:42,  1.77s/it][A
Epoch 8/32:  54%|█████▍    | 28/52 [00:49<00:42,  1.77s/it][A
Epoch 8/32:  56%|█████▌    | 29/52 [00:51<00:40,  1.77s/it][A
Epoch 8/32:  56%|█████▌    | 29/52 [00:51<00:40,  1.77s/it][A
Epoch 8/32:  58%|█████▊    | 30/52 [00:53<00:38,  1.77s/it][A
Epoch 8/32:  58%|█████▊    | 30/52 [00:53<00:39,  1.77s/it][A
Epoch 8/32:  60%|█████▉    | 31/52 [00:55<00:37,  1.78s/it][A
Epoch 8/32:  60%|█████▉    | 31/52 [00:55<00:37,  1.78s/it][A
Epoch 8/32:  62%|██████▏   | 32/52 [00:56<00:35,  1.78s/it][A
Epoch 8/32:  62%|██████▏   | 32/52 [00:56<00:35,  1.78s/it][A
Epoch 8/32:  63%|██████▎   | 33/52 [00:58<00:33,  1.78s/it][A
Epoch 8/32:  63%|██████▎   | 33/52 [00:58<00:33,  1.78s/it][A
Epoch 8/32:  65%|██████▌   | 34/52 [01:00<00:31,  1.77s/it][A
Epoch 8/32:  65%|██████▌   | 34/52 [01:00<00:31,  1.77s/it][A
Epoch 8/32:  67%|██████▋   | 35/52 [01:02<00:30,  1.77s/it][A
Epoch 8/32:  67%|██████▋   | 35/52 [01:02<00:30,  1.77s/it][A
Epoch 8/32:  69%|██████▉   | 36/52 [01:03<00:28,  1.76s/it][A
Epoch 8/32:  69%|██████▉   | 36/52 [01:03<00:28,  1.77s/it][A
Epoch 8/32:  71%|███████   | 37/52 [01:05<00:26,  1.77s/it][A
Epoch 8/32:  71%|███████   | 37/52 [01:05<00:26,  1.77s/it][A
Epoch 8/32:  73%|███████▎  | 38/52 [01:07<00:24,  1.78s/it][A
Epoch 8/32:  73%|███████▎  | 38/52 [01:07<00:24,  1.78s/it][A

Epoch 8/32:  75%|███████▌  | 39/52 [01:09<00:23,  1.78s/it][AEpoch 8/32:  75%|███████▌  | 39/52 [01:09<00:23,  1.78s/it][A
Epoch 8/32:  77%|███████▋  | 40/52 [01:11<00:21,  1.79s/it][A
Epoch 8/32:  77%|███████▋  | 40/52 [01:11<00:21,  1.79s/it][A
Epoch 8/32:  79%|███████▉  | 41/52 [01:12<00:19,  1.79s/it][A
Epoch 8/32:  79%|███████▉  | 41/52 [01:12<00:19,  1.79s/it][A
Epoch 8/32:  81%|████████  | 42/52 [01:14<00:18,  1.84s/it][A
Epoch 8/32:  81%|████████  | 42/52 [01:14<00:18,  1.84s/it][A

Epoch 8/32:  83%|████████▎ | 43/52 [01:16<00:16,  1.82s/it][AEpoch 8/32:  83%|████████▎ | 43/52 [01:16<00:16,  1.82s/it][A
Epoch 8/32:  85%|████████▍ | 44/52 [01:18<00:14,  1.81s/it][A
Epoch 8/32:  85%|████████▍ | 44/52 [01:18<00:14,  1.81s/it][A
Epoch 8/32:  87%|████████▋ | 45/52 [01:20<00:12,  1.80s/it][A
Epoch 8/32:  87%|████████▋ | 45/52 [01:20<00:12,  1.80s/it][A
Epoch 8/32:  88%|████████▊ | 46/52 [01:22<00:10,  1.80s/it][A
Epoch 8/32:  88%|████████▊ | 46/52 [01:22<00:10,  1.80s/it][A
Epoch 8/32:  90%|█████████ | 47/52 [01:23<00:08,  1.79s/it][A
Epoch 8/32:  90%|█████████ | 47/52 [01:23<00:08,  1.79s/it][A
Epoch 8/32:  92%|█████████▏| 48/52 [01:25<00:07,  1.79s/it][A
Epoch 8/32:  92%|█████████▏| 48/52 [01:25<00:07,  1.79s/it][A
Epoch 8/32:  94%|█████████▍| 49/52 [01:27<00:05,  1.78s/it][A
Epoch 8/32:  94%|█████████▍| 49/52 [01:27<00:05,  1.78s/it][A
Epoch 8/32:  96%|█████████▌| 50/52 [01:29<00:03,  1.79s/it][A
Epoch 8/32:  96%|█████████▌| 50/52 [01:29<00:03,  1.79s/it][A
Epoch 8/32:  98%|█████████▊| 51/52 [01:30<00:01,  1.79s/it][A
Epoch 8/32:  98%|█████████▊| 51/52 [01:30<00:01,  1.79s/it][A
Epoch 8/32: 100%|██████████| 52/52 [01:32<00:00,  1.69s/it][AEpoch 8/32: 100%|██████████| 52/52 [01:32<00:00,  1.78s/it]

Epoch 8/32: 100%|██████████| 52/52 [01:32<00:00,  1.69s/it][A
Epoch 8/32: 100%|██████████| 52/52 [01:32<00:00,  1.78s/it]
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
[β=2.0] Epoch 8 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=5.860e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=1.939e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=4.099e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.579e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.065e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.191e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.819e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.563e-01
  layer model_6__forward_module.module.input_conv act-std: mean=9.654e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=3.198e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=4.508e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.889e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.180e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=3.507e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=7.123e-02
  layer model_6__forward_module.module.output_conv act-std: mean=1.556e-01
  layer model_5__forward_module.module.input_conv act-std: mean=6.122e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.601e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.580e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.339e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=2.445e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=8.425e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.187e-01
  layer model_5__forward_module.module.output_conv act-std: mean=2.556e-01
  layer model_4__forward_module.module.input_conv act-std: mean=5.280e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.495e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.729e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.417e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.975e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=6.212e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.193e-01
  layer model_4__forward_module.module.output_conv act-std: mean=2.587e-01
  layer model_3__forward_module.module.input_conv act-std: mean=6.437e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.279e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.884e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=4.757e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=2.552e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=9.222e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.749e-01
  layer model_3__forward_module.module.output_conv act-std: mean=2.530e-01
  layer model_2__forward_module.module.input_conv act-std: mean=8.335e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.332e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=6.037e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.399e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=3.488e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.085e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=3.564e-01
  layer model_2__forward_module.module.output_conv act-std: mean=3.204e-01
  layer model_1__forward_module.module.input_conv act-std: mean=7.245e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.844e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=4.606e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=5.515e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=3.332e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.582e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=5.938e-01
  layer model_1__forward_module.module.output_conv act-std: mean=3.449e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.029e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.887e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=7.776e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.171e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.230e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=7.989e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=2.947e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.097e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=4.366e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=5.245e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.307e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=4.287e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.705e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.945e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=7.451e-03
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.064e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.847e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.878e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.267e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.997e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.911e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=7.946e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.194e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.397e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=5.832e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.750e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=7.531e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=5.740e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=1.249e+00
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=3.143e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.742e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=2.056e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.751e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.340e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.914e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.829e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.217e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.844e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.732e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.054e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=5.257e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.202e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.878e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.398e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.849e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=7.538e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=4.847e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=6.283e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.430e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=6.131e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.462e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.071e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=9.206e-03
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.755e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.524e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.709e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.193e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.313e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.850e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=4.504e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.109e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.642e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=7.192e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.796e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=7.210e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=5.460e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.408e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.366e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=8.929e-02
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.547e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.021e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.584e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.344e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.961e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.936e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.152e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.625e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.011e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=3.617e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.266e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.607e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=8.340e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.620e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=9.785e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=6.513e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.097e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.546e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.303e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.251e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.165e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.297e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.838e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.938e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.176e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.196e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.176e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.601e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.938e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.376e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.153e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=6.188e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.399e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=8.055e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=6.095e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=9.226e-01
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.060e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.354e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.437e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.197e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.347e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.563e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.347e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.965e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.127e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.034e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.113e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=2.939e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=9.122e-04
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.134e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=6.457e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.481e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=7.692e-02
  param model_6__forward_module.module.output_scale grad-norm: mean=5.854e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=9.547e-01
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=2.097e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=9.457e-02
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=9.340e-02
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.588e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.530e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.706e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.701e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.108e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.462e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.742e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.369e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.972e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=2.824e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=4.777e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=6.433e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.294e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.179e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=6.026e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=1.187e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=2.528e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.003e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=8.827e-02
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.036e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.061e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.065e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.346e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.414e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.562e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.580e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.428e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.592e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=2.173e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=4.681e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=4.910e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.097e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.027e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.75it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.71it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.78it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.73it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.76it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.73it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.74it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.72it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.75it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.73it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.77it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.74it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.77it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.76it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.78it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.77it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.78it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.77it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.78it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.77it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.78it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.78it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.78it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.78it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.77it/s]
Training epochs:  25%|██▌       | 8/32 [12:49<39:13, 98.08s/it]
Epoch 9/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s]
Epoch 8/32 - Train Loss: 18.996634 - Test Loss: 18.962810
Training epochs:  25%|██▌       | 8/32 [12:49<39:13, 98.08s/it]
Epoch 9/32:   0%|          | 0/52 [00:00<?, ?it/s][A

Epoch 9/32:   2%|▏         | 1/52 [00:01<01:32,  1.81s/it][AEpoch 9/32:   2%|▏         | 1/52 [00:01<01:29,  1.75s/it][A
Epoch 9/32:   4%|▍         | 2/52 [00:03<01:27,  1.76s/it][A
Epoch 9/32:   4%|▍         | 2/52 [00:03<01:29,  1.78s/it][A
Epoch 9/32:   6%|▌         | 3/52 [00:05<01:27,  1.78s/it][A
Epoch 9/32:   6%|▌         | 3/52 [00:05<01:26,  1.77s/it][A

Epoch 9/32:   8%|▊         | 4/52 [00:07<01:24,  1.77s/it]Epoch 9/32:   8%|▊         | 4/52 [00:07<01:25,  1.78s/it][A[A
Epoch 9/32:  10%|▉         | 5/52 [00:08<01:23,  1.77s/it][A
Epoch 9/32:  10%|▉         | 5/52 [00:08<01:23,  1.77s/it][A
Epoch 9/32:  12%|█▏        | 6/52 [00:10<01:21,  1.76s/it][A
Epoch 9/32:  12%|█▏        | 6/52 [00:10<01:21,  1.77s/it][A
Epoch 9/32:  13%|█▎        | 7/52 [00:12<01:20,  1.79s/it][A
Epoch 9/32:  13%|█▎        | 7/52 [00:12<01:20,  1.80s/it][A
Epoch 9/32:  15%|█▌        | 8/52 [00:14<01:19,  1.80s/it][A
Epoch 9/32:  15%|█▌        | 8/52 [00:14<01:19,  1.80s/it][A
Epoch 9/32:  17%|█▋        | 9/52 [00:16<01:17,  1.81s/it][A
Epoch 9/32:  17%|█▋        | 9/52 [00:16<01:17,  1.81s/it][A
Epoch 9/32:  19%|█▉        | 10/52 [00:17<01:15,  1.81s/it][A
Epoch 9/32:  19%|█▉        | 10/52 [00:17<01:15,  1.81s/it][A
Epoch 9/32:  21%|██        | 11/52 [00:19<01:13,  1.80s/it][A
Epoch 9/32:  21%|██        | 11/52 [00:19<01:13,  1.80s/it][A
Epoch 9/32:  23%|██▎       | 12/52 [00:21<01:11,  1.80s/it][A
Epoch 9/32:  23%|██▎       | 12/52 [00:21<01:11,  1.80s/it][A
Epoch 9/32:  25%|██▌       | 13/52 [00:23<01:10,  1.80s/it][A
Epoch 9/32:  25%|██▌       | 13/52 [00:23<01:10,  1.80s/it][A
Epoch 9/32:  27%|██▋       | 14/52 [00:25<01:08,  1.79s/it][A
Epoch 9/32:  27%|██▋       | 14/52 [00:25<01:08,  1.79s/it][A
Epoch 9/32:  29%|██▉       | 15/52 [00:26<01:06,  1.79s/it][A
Epoch 9/32:  29%|██▉       | 15/52 [00:26<01:06,  1.79s/it][A
Epoch 9/32:  31%|███       | 16/52 [00:28<01:04,  1.79s/it][A
Epoch 9/32:  31%|███       | 16/52 [00:28<01:04,  1.79s/it][A
Epoch 9/32:  33%|███▎      | 17/52 [00:30<01:02,  1.79s/it][A
Epoch 9/32:  33%|███▎      | 17/52 [00:30<01:02,  1.79s/it][A
Epoch 9/32:  35%|███▍      | 18/52 [00:32<01:00,  1.79s/it][A
Epoch 9/32:  35%|███▍      | 18/52 [00:32<01:00,  1.79s/it][A
Epoch 9/32:  37%|███▋      | 19/52 [00:33<00:59,  1.80s/it][A
Epoch 9/32:  37%|███▋      | 19/52 [00:34<00:59,  1.79s/it][A
Epoch 9/32:  38%|███▊      | 20/52 [00:35<00:57,  1.79s/it][A
Epoch 9/32:  38%|███▊      | 20/52 [00:35<00:57,  1.79s/it][A
Epoch 9/32:  40%|████      | 21/52 [00:37<00:55,  1.79s/it][A
Epoch 9/32:  40%|████      | 21/52 [00:37<00:55,  1.79s/it][A
Epoch 9/32:  42%|████▏     | 22/52 [00:39<00:53,  1.80s/it][A
Epoch 9/32:  42%|████▏     | 22/52 [00:39<00:53,  1.80s/it][A
Epoch 9/32:  44%|████▍     | 23/52 [00:41<00:52,  1.80s/it][A
Epoch 9/32:  44%|████▍     | 23/52 [00:41<00:52,  1.80s/it][A
Epoch 9/32:  46%|████▌     | 24/52 [00:42<00:50,  1.80s/it][A
Epoch 9/32:  46%|████▌     | 24/52 [00:43<00:50,  1.80s/it][A
Epoch 9/32:  48%|████▊     | 25/52 [00:44<00:48,  1.80s/it][A
Epoch 9/32:  48%|████▊     | 25/52 [00:44<00:48,  1.80s/it][A
Epoch 9/32:  50%|█████     | 26/52 [00:46<00:46,  1.80s/it][A
Epoch 9/32:  50%|█████     | 26/52 [00:46<00:46,  1.80s/it][A
Epoch 9/32:  52%|█████▏    | 27/52 [00:48<00:44,  1.80s/it][A
Epoch 9/32:  52%|█████▏    | 27/52 [00:48<00:44,  1.80s/it][A
Epoch 9/32:  54%|█████▍    | 28/52 [00:50<00:43,  1.79s/it][A
Epoch 9/32:  54%|█████▍    | 28/52 [00:50<00:43,  1.79s/it][A
Epoch 9/32:  56%|█████▌    | 29/52 [00:51<00:41,  1.80s/it][A
Epoch 9/32:  56%|█████▌    | 29/52 [00:52<00:41,  1.80s/it][A
Epoch 9/32:  58%|█████▊    | 30/52 [00:53<00:39,  1.80s/it][A
Epoch 9/32:  58%|█████▊    | 30/52 [00:53<00:39,  1.80s/it][A
Epoch 9/32:  60%|█████▉    | 31/52 [00:55<00:37,  1.80s/it][A
Epoch 9/32:  60%|█████▉    | 31/52 [00:55<00:37,  1.80s/it][A
Epoch 9/32:  62%|██████▏   | 32/52 [00:57<00:36,  1.81s/it][A
Epoch 9/32:  62%|██████▏   | 32/52 [00:57<00:36,  1.81s/it][A
Epoch 9/32:  63%|██████▎   | 33/52 [00:59<00:34,  1.81s/it][A
Epoch 9/32:  63%|██████▎   | 33/52 [00:59<00:34,  1.81s/it][A
Epoch 9/32:  65%|██████▌   | 34/52 [01:01<00:32,  1.81s/it][A
Epoch 9/32:  65%|██████▌   | 34/52 [01:01<00:32,  1.81s/it][A
Epoch 9/32:  67%|██████▋   | 35/52 [01:02<00:30,  1.80s/it][A
Epoch 9/32:  67%|██████▋   | 35/52 [01:02<00:30,  1.81s/it][A
Epoch 9/32:  69%|██████▉   | 36/52 [01:04<00:28,  1.81s/it][A
Epoch 9/32:  69%|██████▉   | 36/52 [01:04<00:28,  1.81s/it][A
Epoch 9/32:  71%|███████   | 37/52 [01:06<00:27,  1.80s/it][A
Epoch 9/32:  71%|███████   | 37/52 [01:06<00:27,  1.81s/it][A
Epoch 9/32:  73%|███████▎  | 38/52 [01:08<00:25,  1.80s/it][A
Epoch 9/32:  73%|███████▎  | 38/52 [01:08<00:25,  1.80s/it][A
Epoch 9/32:  75%|███████▌  | 39/52 [01:10<00:23,  1.80s/it][A
Epoch 9/32:  75%|███████▌  | 39/52 [01:10<00:23,  1.80s/it][A
Epoch 9/32:  77%|███████▋  | 40/52 [01:11<00:21,  1.80s/it][A
Epoch 9/32:  77%|███████▋  | 40/52 [01:11<00:21,  1.80s/it][A
Epoch 9/32:  79%|███████▉  | 41/52 [01:13<00:19,  1.80s/it][A
Epoch 9/32:  79%|███████▉  | 41/52 [01:13<00:19,  1.80s/it][A
Epoch 9/32:  81%|████████  | 42/52 [01:15<00:17,  1.80s/it][A
Epoch 9/32:  81%|████████  | 42/52 [01:15<00:17,  1.80s/it][A
Epoch 9/32:  83%|████████▎ | 43/52 [01:17<00:16,  1.81s/it][A
Epoch 9/32:  83%|████████▎ | 43/52 [01:17<00:16,  1.81s/it][A
Epoch 9/32:  85%|████████▍ | 44/52 [01:19<00:14,  1.81s/it][A
Epoch 9/32:  85%|████████▍ | 44/52 [01:19<00:14,  1.81s/it][A
Epoch 9/32:  87%|████████▋ | 45/52 [01:20<00:12,  1.81s/it][A
Epoch 9/32:  87%|████████▋ | 45/52 [01:20<00:12,  1.81s/it][A
Epoch 9/32:  88%|████████▊ | 46/52 [01:22<00:10,  1.80s/it][A
Epoch 9/32:  88%|████████▊ | 46/52 [01:22<00:10,  1.80s/it][A
Epoch 9/32:  90%|█████████ | 47/52 [01:24<00:09,  1.80s/it][A
Epoch 9/32:  90%|█████████ | 47/52 [01:24<00:09,  1.80s/it][A
Epoch 9/32:  92%|█████████▏| 48/52 [01:26<00:07,  1.81s/it][A
Epoch 9/32:  92%|█████████▏| 48/52 [01:26<00:07,  1.81s/it][A
Epoch 9/32:  94%|█████████▍| 49/52 [01:28<00:05,  1.81s/it][A
Epoch 9/32:  94%|█████████▍| 49/52 [01:28<00:05,  1.81s/it][A
Epoch 9/32:  96%|█████████▌| 50/52 [01:29<00:03,  1.81s/it][A
Epoch 9/32:  96%|█████████▌| 50/52 [01:29<00:03,  1.81s/it][A
Epoch 9/32:  98%|█████████▊| 51/52 [01:31<00:01,  1.81s/it][A
Epoch 9/32:  98%|█████████▊| 51/52 [01:31<00:01,  1.81s/it][A
Epoch 9/32: 100%|██████████| 52/52 [01:33<00:00,  1.70s/it][AEpoch 9/32: 100%|██████████| 52/52 [01:33<00:00,  1.79s/it]

Epoch 9/32: 100%|██████████| 52/52 [01:33<00:00,  1.70s/it][AEpoch 9/32: 100%|██████████| 52/52 [01:33<00:00,  1.79s/it]

[β=2.0] Epoch 9 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=5.943e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=1.791e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=3.632e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.354e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.729e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.146e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.719e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.477e-01
  layer model_6__forward_module.module.input_conv act-std: mean=9.895e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=4.353e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=5.635e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.994e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.158e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=3.600e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=7.042e-02
  layer model_6__forward_module.module.output_conv act-std: mean=1.590e-01
  layer model_5__forward_module.module.input_conv act-std: mean=6.767e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.812e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.818e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.220e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=2.062e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=7.975e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.189e-01
  layer model_5__forward_module.module.output_conv act-std: mean=2.222e-01
  layer model_4__forward_module.module.input_conv act-std: mean=5.612e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.474e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.421e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.283e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.471e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=6.015e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.340e-01
  layer model_4__forward_module.module.output_conv act-std: mean=2.202e-01
  layer model_3__forward_module.module.input_conv act-std: mean=6.829e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.248e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.849e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=4.257e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=2.304e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=9.022e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.819e-01
  layer model_3__forward_module.module.output_conv act-std: mean=2.361e-01
  layer model_2__forward_module.module.input_conv act-std: mean=8.735e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.085e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=5.648e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.277e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=3.211e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.071e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=3.816e-01
  layer model_2__forward_module.module.output_conv act-std: mean=2.994e-01
  layer model_1__forward_module.module.input_conv act-std: mean=7.568e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.869e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=4.255e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=5.039e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=3.096e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.639e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=6.427e-01
  layer model_1__forward_module.module.output_conv act-std: mean=3.273e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.180e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.095e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.153e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.150e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.076e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.172e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=3.306e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.061e-01

  param model_0__forward_module.module.output_scale grad-norm: mean=4.182e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=4.965e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.230e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=4.142e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=7.882e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.951e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.366e-03
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.789e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.154e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.135e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.417e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.116e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.371e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=8.140e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.389e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.457e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.064e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.817e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=8.083e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=5.549e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=9.459e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.373e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.175e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.539e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.278e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.393e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.609e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.435e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.693e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.670e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.326e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.808e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=4.823e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.185e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.637e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=6.355e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.710e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=7.067e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=4.341e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=4.499e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=9.199e-02
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=5.420e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=4.936e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.785e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.183e-03
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.881e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.561e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.221e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=7.391e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.472e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.683e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.596e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=9.994e-04
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.047e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=4.691e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.580e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=6.314e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=5.247e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=8.402e-01
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=1.900e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=8.511e-02
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=8.242e-02
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.825e-03
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=8.338e-03
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.824e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.099e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.238e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.355e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.687e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.742e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=3.330e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.287e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.067e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=5.667e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.395e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=7.990e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=6.421e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.208e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.777e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.390e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.191e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.199e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.758e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.923e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.613e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.888e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.480e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.270e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.221e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=4.040e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.879e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.215e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=6.835e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.439e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=9.967e-02
  param model_5__forward_module.module.output_scale grad-norm: mean=6.107e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.082e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.368e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.603e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.496e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.335e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.860e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.290e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.449e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.934e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.500e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.507e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.841e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=2.548e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=9.829e-04
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.354e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=8.284e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.571e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.031e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=5.322e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.007e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=2.204e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.130e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.045e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.032e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.526e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.446e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.156e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.328e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.438e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.045e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.282e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.749e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=2.799e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=5.191e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=6.911e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.455e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.287e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=5.217e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=1.377e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=2.872e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.070e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.097e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.618e-03
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=7.931e-03
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.964e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.873e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.495e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.983e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.792e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.797e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.707e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=2.357e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=4.950e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=5.279e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.136e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.274e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.76it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.74it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.76it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.75it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.76it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.75it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.75it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.75it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.77it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.75it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.78it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.76it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.78it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.77it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.78it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.78it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.78it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.78it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.79it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.79it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.78it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.78it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.76it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.76it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.77it/s]

Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s]
Epoch 9/32 - Train Loss: 18.739197 - Test Loss: 18.705176
Training epochs:  28%|██▊       | 9/32 [14:30<37:53, 98.86s/it]
Epoch 10/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  28%|██▊       | 9/32 [14:30<37:55, 98.93s/it]
Epoch 10/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 10/32:   2%|▏         | 1/52 [00:01<01:38,  1.94s/it][A
Epoch 10/32:   2%|▏         | 1/52 [00:01<01:30,  1.78s/it][A
Epoch 10/32:   4%|▍         | 2/52 [00:03<01:32,  1.86s/it][A
Epoch 10/32:   4%|▍         | 2/52 [00:03<01:29,  1.79s/it][A
Epoch 10/32:   6%|▌         | 3/52 [00:05<01:30,  1.84s/it][A
Epoch 10/32:   6%|▌         | 3/52 [00:05<01:28,  1.80s/it][A
Epoch 10/32:   8%|▊         | 4/52 [00:07<01:27,  1.83s/it][A
Epoch 10/32:   8%|▊         | 4/52 [00:07<01:26,  1.81s/it][A
Epoch 10/32:  10%|▉         | 5/52 [00:09<01:25,  1.82s/it][A
Epoch 10/32:  10%|▉         | 5/52 [00:09<01:24,  1.80s/it][A
Epoch 10/32:  12%|█▏        | 6/52 [00:10<01:23,  1.81s/it][A
Epoch 10/32:  12%|█▏        | 6/52 [00:10<01:22,  1.80s/it][A
Epoch 10/32:  13%|█▎        | 7/52 [00:12<01:21,  1.81s/it][A
Epoch 10/32:  13%|█▎        | 7/52 [00:12<01:21,  1.81s/it][A
Epoch 10/32:  15%|█▌        | 8/52 [00:14<01:19,  1.81s/it][A
Epoch 10/32:  15%|█▌        | 8/52 [00:14<01:19,  1.82s/it][A
Epoch 10/32:  17%|█▋        | 9/52 [00:16<01:18,  1.82s/it][A
Epoch 10/32:  17%|█▋        | 9/52 [00:16<01:18,  1.82s/it][A
Epoch 10/32:  19%|█▉        | 10/52 [00:18<01:16,  1.82s/it][A
Epoch 10/32:  19%|█▉        | 10/52 [00:18<01:16,  1.82s/it][A
Epoch 10/32:  21%|██        | 11/52 [00:20<01:14,  1.82s/it][A
Epoch 10/32:  21%|██        | 11/52 [00:19<01:14,  1.82s/it][A
Epoch 10/32:  23%|██▎       | 12/52 [00:21<01:12,  1.82s/it][A
Epoch 10/32:  23%|██▎       | 12/52 [00:21<01:12,  1.82s/it][A
Epoch 10/32:  25%|██▌       | 13/52 [00:23<01:10,  1.82s/it][A
Epoch 10/32:  25%|██▌       | 13/52 [00:23<01:10,  1.82s/it][A
Epoch 10/32:  27%|██▋       | 14/52 [00:25<01:09,  1.82s/it][A
Epoch 10/32:  27%|██▋       | 14/52 [00:25<01:09,  1.82s/it][A
Epoch 10/32:  29%|██▉       | 15/52 [00:27<01:07,  1.81s/it][A
Epoch 10/32:  29%|██▉       | 15/52 [00:27<01:07,  1.82s/it][A
Epoch 10/32:  31%|███       | 16/52 [00:29<01:05,  1.82s/it][A
Epoch 10/32:  31%|███       | 16/52 [00:29<01:05,  1.82s/it][A
Epoch 10/32:  33%|███▎      | 17/52 [00:30<01:03,  1.82s/it][A
Epoch 10/32:  33%|███▎      | 17/52 [00:30<01:03,  1.82s/it][A
Epoch 10/32:  35%|███▍      | 18/52 [00:32<01:02,  1.82s/it][A
Epoch 10/32:  35%|███▍      | 18/52 [00:32<01:01,  1.82s/it][A
Epoch 10/32:  37%|███▋      | 19/52 [00:34<01:00,  1.82s/it][A
Epoch 10/32:  37%|███▋      | 19/52 [00:34<01:00,  1.82s/it][A
Epoch 10/32:  38%|███▊      | 20/52 [00:36<00:58,  1.82s/it][A
Epoch 10/32:  38%|███▊      | 20/52 [00:36<00:58,  1.82s/it][A

Epoch 10/32:  40%|████      | 21/52 [00:38<00:56,  1.81s/it][AEpoch 10/32:  40%|████      | 21/52 [00:38<00:56,  1.81s/it][A
Epoch 10/32:  42%|████▏     | 22/52 [00:40<00:54,  1.81s/it][A
Epoch 10/32:  42%|████▏     | 22/52 [00:39<00:54,  1.81s/it][A
Epoch 10/32:  44%|████▍     | 23/52 [00:41<00:52,  1.81s/it][A
Epoch 10/32:  44%|████▍     | 23/52 [00:41<00:52,  1.81s/it][A
Epoch 10/32:  46%|████▌     | 24/52 [00:43<00:50,  1.81s/it][A
Epoch 10/32:  46%|████▌     | 24/52 [00:43<00:50,  1.81s/it][A
Epoch 10/32:  48%|████▊     | 25/52 [00:45<00:48,  1.81s/it][A
Epoch 10/32:  48%|████▊     | 25/52 [00:45<00:48,  1.81s/it][A
Epoch 10/32:  50%|█████     | 26/52 [00:47<00:47,  1.82s/it][A
Epoch 10/32:  50%|█████     | 26/52 [00:47<00:47,  1.82s/it][A

Epoch 10/32:  52%|█████▏    | 27/52 [00:49<00:45,  1.82s/it][AEpoch 10/32:  52%|█████▏    | 27/52 [00:48<00:45,  1.82s/it][A
Epoch 10/32:  54%|█████▍    | 28/52 [00:50<00:43,  1.82s/it][A
Epoch 10/32:  54%|█████▍    | 28/52 [00:50<00:43,  1.82s/it][A
Epoch 10/32:  56%|█████▌    | 29/52 [00:52<00:41,  1.82s/it][A
Epoch 10/32:  56%|█████▌    | 29/52 [00:52<00:41,  1.82s/it][A
Epoch 10/32:  58%|█████▊    | 30/52 [00:54<00:39,  1.81s/it][A
Epoch 10/32:  58%|█████▊    | 30/52 [00:54<00:39,  1.81s/it][A
Epoch 10/32:  60%|█████▉    | 31/52 [00:56<00:38,  1.81s/it][A
Epoch 10/32:  60%|█████▉    | 31/52 [00:56<00:38,  1.81s/it][A
Epoch 10/32:  62%|██████▏   | 32/52 [00:58<00:36,  1.81s/it][A
Epoch 10/32:  62%|██████▏   | 32/52 [00:58<00:36,  1.81s/it][A
Epoch 10/32:  63%|██████▎   | 33/52 [00:59<00:34,  1.82s/it][A
Epoch 10/32:  63%|██████▎   | 33/52 [01:00<00:34,  1.82s/it][A
Epoch 10/32:  65%|██████▌   | 34/52 [01:01<00:32,  1.82s/it][A
Epoch 10/32:  65%|██████▌   | 34/52 [01:01<00:32,  1.82s/it][A
Epoch 10/32:  67%|██████▋   | 35/52 [01:03<00:31,  1.88s/it][A
Epoch 10/32:  67%|██████▋   | 35/52 [01:03<00:32,  1.88s/it][A
Epoch 10/32:  69%|██████▉   | 36/52 [01:05<00:29,  1.86s/it][A
Epoch 10/32:  69%|██████▉   | 36/52 [01:05<00:29,  1.86s/it][A
Epoch 10/32:  71%|███████   | 37/52 [01:07<00:27,  1.85s/it][A
Epoch 10/32:  71%|███████   | 37/52 [01:07<00:27,  1.85s/it][A
Epoch 10/32:  73%|███████▎  | 38/52 [01:09<00:25,  1.84s/it][A
Epoch 10/32:  73%|███████▎  | 38/52 [01:09<00:25,  1.84s/it][A
Epoch 10/32:  75%|███████▌  | 39/52 [01:10<00:23,  1.83s/it][A
Epoch 10/32:  75%|███████▌  | 39/52 [01:11<00:23,  1.83s/it][A
Epoch 10/32:  77%|███████▋  | 40/52 [01:12<00:21,  1.82s/it][A
Epoch 10/32:  77%|███████▋  | 40/52 [01:12<00:21,  1.83s/it][A
Epoch 10/32:  79%|███████▉  | 41/52 [01:14<00:20,  1.83s/it][A
Epoch 10/32:  79%|███████▉  | 41/52 [01:14<00:20,  1.83s/it][A
Epoch 10/32:  81%|████████  | 42/52 [01:16<00:18,  1.83s/it][A
Epoch 10/32:  81%|████████  | 42/52 [01:16<00:18,  1.83s/it][A
Epoch 10/32:  83%|████████▎ | 43/52 [01:18<00:16,  1.82s/it][A
Epoch 10/32:  83%|████████▎ | 43/52 [01:18<00:16,  1.82s/it][A
Epoch 10/32:  85%|████████▍ | 44/52 [01:20<00:14,  1.82s/it][A
Epoch 10/32:  85%|████████▍ | 44/52 [01:20<00:14,  1.82s/it][A
Epoch 10/32:  87%|████████▋ | 45/52 [01:22<00:12,  1.82s/it][A
Epoch 10/32:  87%|████████▋ | 45/52 [01:21<00:12,  1.82s/it][A
Epoch 10/32:  88%|████████▊ | 46/52 [01:23<00:10,  1.83s/it][A
Epoch 10/32:  88%|████████▊ | 46/52 [01:23<00:10,  1.83s/it][A
Epoch 10/32:  90%|█████████ | 47/52 [01:25<00:09,  1.82s/it][A
Epoch 10/32:  90%|█████████ | 47/52 [01:25<00:09,  1.82s/it][A
Epoch 10/32:  92%|█████████▏| 48/52 [01:27<00:07,  1.82s/it][A
Epoch 10/32:  92%|█████████▏| 48/52 [01:27<00:07,  1.82s/it][A
Epoch 10/32:  94%|█████████▍| 49/52 [01:29<00:05,  1.82s/it][A
Epoch 10/32:  94%|█████████▍| 49/52 [01:29<00:05,  1.82s/it][A
Epoch 10/32:  96%|█████████▌| 50/52 [01:31<00:03,  1.82s/it][A
Epoch 10/32:  96%|█████████▌| 50/52 [01:31<00:03,  1.82s/it][A
Epoch 10/32:  98%|█████████▊| 51/52 [01:32<00:01,  1.82s/it][A
Epoch 10/32:  98%|█████████▊| 51/52 [01:32<00:01,  1.82s/it][A
Epoch 10/32: 100%|██████████| 52/52 [01:34<00:00,  1.72s/it][AEpoch 10/32: 100%|██████████| 52/52 [01:34<00:00,  1.82s/it]

[β=2.0] Epoch 10 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=6.157e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=1.832e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=3.384e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.353e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.780e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.165e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.688e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.411e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.026e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.170e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=7.433e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.139e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.137e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=3.712e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=7.089e-02
  layer model_6__forward_module.module.output_conv act-std: mean=1.643e-01
  layer model_5__forward_module.module.input_conv act-std: mean=7.007e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.932e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.860e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=1.056e+00
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.811e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=7.774e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.191e-01
  layer model_5__forward_module.module.output_conv act-std: mean=2.022e-01
  layer model_4__forward_module.module.input_conv act-std: mean=6.130e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.347e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.283e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.124e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.290e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.907e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.636e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.998e-01
  layer model_3__forward_module.module.input_conv act-std: mean=7.049e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.139e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.787e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.708e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.981e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=8.607e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.808e-01
  layer model_3__forward_module.module.output_conv act-std: mean=2.193e-01
  layer model_2__forward_module.module.input_conv act-std: mean=9.024e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.840e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=5.419e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.236e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.933e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.062e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=4.076e-01
  layer model_2__forward_module.module.output_conv act-std: mean=2.804e-01
  layer model_1__forward_module.module.input_conv act-std: mean=7.880e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.846e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.989e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=4.654e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.874e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.887e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=7.016e-01
  layer model_1__forward_module.module.output_conv act-std: mean=3.157e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.307e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.292e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.903e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.127e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.061e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.740e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=4.013e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.133e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=3.196e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=3.080e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=7.249e-02
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=3.331e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=4.649e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.176e-03
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.277e-03
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.376e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.162e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.333e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=9.492e-03
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.514e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.030e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=5.192e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=1.521e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.074e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=4.129e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=1.821e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=7.016e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=4.541e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=6.005e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.486e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=7.821e-02

  param model_1__forward_module.module.input_norm.bias grad-norm: mean=9.497e-02
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.414e-03
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.190e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.680e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.729e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.555e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.244e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.760e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.988e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=3.435e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=9.260e-04
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.208e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=4.527e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=1.706e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=7.176e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=3.953e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=5.016e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.009e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=6.495e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=5.329e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.058e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.561e-03
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.977e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.801e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.268e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=8.764e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.640e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=3.613e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.605e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.146e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.171e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=5.264e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.658e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=7.434e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=4.899e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=9.444e-01
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=2.085e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=9.385e-02
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=8.510e-02
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.430e-03
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=9.078e-03
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.119e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.124e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.055e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.464e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.725e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.085e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=3.025e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.403e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=9.743e-03
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=5.471e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.417e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=9.198e-02
  param model_4__forward_module.module.output_scale grad-norm: mean=6.039e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.306e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.971e-01
Epoch 10/32: 100%|██████████| 52/52 [01:34<00:00,  1.72s/it]  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.341e-01[A
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.201e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.256e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.850e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.982e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.746e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.746e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.500e-02Epoch 10/32: 100%|██████████| 52/52 [01:34<00:00,  1.81s/it]

  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.091e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.615e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.372e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.981e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.072e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=6.135e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.295e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=1.065e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=5.536e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=9.065e-01
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=1.869e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.283e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=9.764e-02
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.251e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.104e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.499e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.626e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.072e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.100e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.902e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=5.703e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=2.637e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.245e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.027e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=6.381e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.429e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=8.960e-02
  param model_6__forward_module.module.output_scale grad-norm: mean=4.720e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.300e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=2.888e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.575e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.424e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.500e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.588e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.100e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.272e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.051e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.449e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.066e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.839e-03
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.243e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=3.718e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=7.107e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=9.092e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.587e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.288e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=4.596e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=1.180e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=2.446e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.214e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=9.102e-02
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.076e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=8.235e-03
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.355e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.989e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.023e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.414e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.671e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.869e-03
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.466e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=2.345e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=5.440e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=5.712e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.303e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.156e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.75it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.74it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.74it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.73it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.75it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.74it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.76it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.74it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.76it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.75it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.77it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:03,  1.76it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.78it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:03<00:03,  1.76it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.77it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.76it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.77it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.77it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.77it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.77it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.76it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.77it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.76it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:06<00:00,  1.77it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s]
Epoch 10/32 - Train Loss: 18.523119 - Test Loss: 18.540735

Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s]
Training epochs:  31%|███▏      | 10/32 [16:12<36:35, 99.80s/it]
Epoch 11/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  31%|███▏      | 10/32 [16:12<36:35, 99.80s/it]
Epoch 11/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 11/32:   2%|▏         | 1/52 [00:01<01:30,  1.78s/it][A
Epoch 11/32:   2%|▏         | 1/52 [00:01<01:31,  1.79s/it][A
Epoch 11/32:   4%|▍         | 2/52 [00:03<01:29,  1.79s/it][A
Epoch 11/32:   4%|▍         | 2/52 [00:03<01:29,  1.79s/it][A
Epoch 11/32:   6%|▌         | 3/52 [00:05<01:28,  1.81s/it][A
Epoch 11/32:   6%|▌         | 3/52 [00:05<01:28,  1.82s/it][A
Epoch 11/32:   8%|▊         | 4/52 [00:07<01:27,  1.82s/it][A
Epoch 11/32:   8%|▊         | 4/52 [00:07<01:27,  1.82s/it][A
Epoch 11/32:  10%|▉         | 5/52 [00:09<01:25,  1.83s/it][A
Epoch 11/32:  10%|▉         | 5/52 [00:09<01:25,  1.83s/it][A
Epoch 11/32:  12%|█▏        | 6/52 [00:10<01:23,  1.82s/it][A
Epoch 11/32:  12%|█▏        | 6/52 [00:10<01:23,  1.82s/it][A
Epoch 11/32:  13%|█▎        | 7/52 [00:12<01:21,  1.81s/it][A
Epoch 11/32:  13%|█▎        | 7/52 [00:12<01:21,  1.81s/it][A
Epoch 11/32:  15%|█▌        | 8/52 [00:14<01:19,  1.82s/it][A
Epoch 11/32:  15%|█▌        | 8/52 [00:14<01:20,  1.82s/it][A
Epoch 11/32:  17%|█▋        | 9/52 [00:16<01:18,  1.82s/it][A
Epoch 11/32:  17%|█▋        | 9/52 [00:16<01:18,  1.82s/it][A
Epoch 11/32:  19%|█▉        | 10/52 [00:18<01:16,  1.82s/it][A
Epoch 11/32:  19%|█▉        | 10/52 [00:18<01:16,  1.82s/it][A
Epoch 11/32:  21%|██        | 11/52 [00:19<01:14,  1.81s/it][A
Epoch 11/32:  21%|██        | 11/52 [00:19<01:14,  1.81s/it][A
Epoch 11/32:  23%|██▎       | 12/52 [00:21<01:12,  1.81s/it][A
Epoch 11/32:  23%|██▎       | 12/52 [00:21<01:12,  1.81s/it][A
Epoch 11/32:  25%|██▌       | 13/52 [00:23<01:10,  1.82s/it][A
Epoch 11/32:  25%|██▌       | 13/52 [00:23<01:10,  1.82s/it][A
Epoch 11/32:  27%|██▋       | 14/52 [00:25<01:09,  1.82s/it][A
Epoch 11/32:  27%|██▋       | 14/52 [00:25<01:09,  1.82s/it][A
Epoch 11/32:  29%|██▉       | 15/52 [00:27<01:07,  1.82s/it][A
Epoch 11/32:  29%|██▉       | 15/52 [00:27<01:07,  1.82s/it][A
Epoch 11/32:  31%|███       | 16/52 [00:29<01:05,  1.81s/it][A
Epoch 11/32:  31%|███       | 16/52 [00:29<01:05,  1.81s/it][A
Epoch 11/32:  33%|███▎      | 17/52 [00:30<01:03,  1.81s/it][A
Epoch 11/32:  33%|███▎      | 17/52 [00:30<01:03,  1.81s/it][A
Epoch 11/32:  35%|███▍      | 18/52 [00:32<01:01,  1.82s/it][A
Epoch 11/32:  35%|███▍      | 18/52 [00:32<01:01,  1.82s/it][A
Epoch 11/32:  37%|███▋      | 19/52 [00:34<01:00,  1.82s/it][A
Epoch 11/32:  37%|███▋      | 19/52 [00:34<01:00,  1.82s/it][A
Epoch 11/32:  38%|███▊      | 20/52 [00:36<00:58,  1.82s/it][A
Epoch 11/32:  38%|███▊      | 20/52 [00:36<00:58,  1.82s/it][A
Epoch 11/32:  40%|████      | 21/52 [00:38<00:56,  1.82s/it][A
Epoch 11/32:  40%|████      | 21/52 [00:38<00:56,  1.82s/it][A
Epoch 11/32:  42%|████▏     | 22/52 [00:39<00:54,  1.81s/it][A
Epoch 11/32:  42%|████▏     | 22/52 [00:39<00:54,  1.82s/it][A
Epoch 11/32:  44%|████▍     | 23/52 [00:41<00:52,  1.82s/it][A
Epoch 11/32:  44%|████▍     | 23/52 [00:41<00:52,  1.82s/it][A
Epoch 11/32:  46%|████▌     | 24/52 [00:43<00:50,  1.82s/it][A
Epoch 11/32:  46%|████▌     | 24/52 [00:43<00:50,  1.82s/it][A
Epoch 11/32:  48%|████▊     | 25/52 [00:45<00:49,  1.82s/it][A
Epoch 11/32:  48%|████▊     | 25/52 [00:45<00:49,  1.82s/it][A
Epoch 11/32:  50%|█████     | 26/52 [00:47<00:47,  1.81s/it][A
Epoch 11/32:  50%|█████     | 26/52 [00:47<00:47,  1.81s/it][A
Epoch 11/32:  52%|█████▏    | 27/52 [00:49<00:45,  1.81s/it][A
Epoch 11/32:  52%|█████▏    | 27/52 [00:49<00:45,  1.81s/it][A
Epoch 11/32:  54%|█████▍    | 28/52 [00:50<00:43,  1.81s/it][A
Epoch 11/32:  54%|█████▍    | 28/52 [00:50<00:43,  1.81s/it][A
Epoch 11/32:  56%|█████▌    | 29/52 [00:52<00:41,  1.81s/it][A
Epoch 11/32:  56%|█████▌    | 29/52 [00:52<00:41,  1.81s/it][A
Epoch 11/32:  58%|█████▊    | 30/52 [00:54<00:39,  1.81s/it][A
Epoch 11/32:  58%|█████▊    | 30/52 [00:54<00:39,  1.81s/it][A
Epoch 11/32:  60%|█████▉    | 31/52 [00:56<00:38,  1.81s/it][A
Epoch 11/32:  60%|█████▉    | 31/52 [00:56<00:38,  1.81s/it][A

Epoch 11/32:  62%|██████▏   | 32/52 [00:58<00:36,  1.81s/it][AEpoch 11/32:  62%|██████▏   | 32/52 [00:58<00:36,  1.81s/it][A
Epoch 11/32:  63%|██████▎   | 33/52 [00:59<00:34,  1.81s/it][A
Epoch 11/32:  63%|██████▎   | 33/52 [00:59<00:34,  1.81s/it][A
Epoch 11/32:  65%|██████▌   | 34/52 [01:01<00:32,  1.81s/it][A
Epoch 11/32:  65%|██████▌   | 34/52 [01:01<00:32,  1.81s/it][A
Epoch 11/32:  67%|██████▋   | 35/52 [01:03<00:30,  1.81s/it][A
Epoch 11/32:  67%|██████▋   | 35/52 [01:03<00:30,  1.81s/it][A
Epoch 11/32:  69%|██████▉   | 36/52 [01:05<00:29,  1.81s/it][A
Epoch 11/32:  69%|██████▉   | 36/52 [01:05<00:29,  1.81s/it][A
Epoch 11/32:  71%|███████   | 37/52 [01:07<00:27,  1.81s/it][A
Epoch 11/32:  71%|███████   | 37/52 [01:07<00:27,  1.81s/it][A
Epoch 11/32:  73%|███████▎  | 38/52 [01:08<00:25,  1.81s/it][A
Epoch 11/32:  73%|███████▎  | 38/52 [01:08<00:25,  1.81s/it][A
Epoch 11/32:  75%|███████▌  | 39/52 [01:10<00:23,  1.81s/it][A
Epoch 11/32:  75%|███████▌  | 39/52 [01:10<00:23,  1.81s/it][A
Epoch 11/32:  77%|███████▋  | 40/52 [01:12<00:21,  1.82s/it][A
Epoch 11/32:  77%|███████▋  | 40/52 [01:12<00:21,  1.82s/it][A
Epoch 11/32:  79%|███████▉  | 41/52 [01:14<00:20,  1.82s/it][A
Epoch 11/32:  79%|███████▉  | 41/52 [01:14<00:20,  1.82s/it][A
Epoch 11/32:  81%|████████  | 42/52 [01:16<00:18,  1.82s/it][A
Epoch 11/32:  81%|████████  | 42/52 [01:16<00:18,  1.82s/it][A
Epoch 11/32:  83%|████████▎ | 43/52 [01:18<00:16,  1.83s/it][A
Epoch 11/32:  83%|████████▎ | 43/52 [01:18<00:16,  1.83s/it][A
Epoch 11/32:  85%|████████▍ | 44/52 [01:19<00:14,  1.83s/it][A
Epoch 11/32:  85%|████████▍ | 44/52 [01:19<00:14,  1.83s/it][A
Epoch 11/32:  87%|████████▋ | 45/52 [01:21<00:12,  1.83s/it][A
Epoch 11/32:  87%|████████▋ | 45/52 [01:21<00:12,  1.83s/it][A
Epoch 11/32:  88%|████████▊ | 46/52 [01:23<00:10,  1.83s/it][A
Epoch 11/32:  88%|████████▊ | 46/52 [01:23<00:11,  1.83s/it][A
Epoch 11/32:  90%|█████████ | 47/52 [01:25<00:09,  1.83s/it][A
Epoch 11/32:  90%|█████████ | 47/52 [01:25<00:09,  1.83s/it][A
Epoch 11/32:  92%|█████████▏| 48/52 [01:27<00:07,  1.83s/it][A
Epoch 11/32:  92%|█████████▏| 48/52 [01:27<00:07,  1.83s/it][A
Epoch 11/32:  94%|█████████▍| 49/52 [01:29<00:05,  1.83s/it][A
Epoch 11/32:  94%|█████████▍| 49/52 [01:29<00:05,  1.83s/it][A
Epoch 11/32:  96%|█████████▌| 50/52 [01:30<00:03,  1.83s/it][A
Epoch 11/32:  96%|█████████▌| 50/52 [01:30<00:03,  1.83s/it][A
Epoch 11/32:  98%|█████████▊| 51/52 [01:32<00:01,  1.83s/it][A
Epoch 11/32:  98%|█████████▊| 51/52 [01:32<00:01,  1.83s/it][A
Epoch 11/32: 100%|██████████| 52/52 [01:34<00:00,  1.72s/it][AEpoch 11/32: 100%|██████████| 52/52 [01:34<00:00,  1.81s/it]

[β=2.0] Epoch 11 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=6.458e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=2.441e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=3.613e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.518e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.002e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.264e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.730e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.385e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.019e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.654e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=8.807e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.313e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.137e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=3.980e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=7.823e-02
  layer model_6__forward_module.module.output_conv act-std: mean=1.719e-01
  layer model_5__forward_module.module.input_conv act-std: mean=7.424e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.897e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.514e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=9.602e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.642e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=7.377e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.114e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.843e-01
  layer model_4__forward_module.module.input_conv act-std: mean=6.756e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.204e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.116e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=1.028e+00
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.167e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.724e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.721e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.851e-01
  layer model_3__forward_module.module.input_conv act-std: mean=7.384e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.142e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.699e-01

  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.378e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.777e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=8.255e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.840e-01
  layer model_3__forward_module.module.output_conv act-std: mean=2.092e-01
  layer model_2__forward_module.module.input_conv act-std: mean=9.483e-02Epoch 11/32: 100%|██████████| 52/52 [01:34<00:00,  1.72s/it]
[A  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.674e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=5.286e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.235e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.782e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.042e+00
Epoch 11/32: 100%|██████████| 52/52 [01:34<00:00,  1.81s/it]  layer model_2__forward_module.module.channel_attention.3 act-std: mean=4.266e-01

  layer model_2__forward_module.module.output_conv act-std: mean=2.625e-01
  layer model_1__forward_module.module.input_conv act-std: mean=8.492e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.821e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.728e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=4.374e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.712e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.963e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=7.627e-01
  layer model_1__forward_module.module.output_conv act-std: mean=3.028e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.376e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.154e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.647e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.048e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.987e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.780e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=4.418e-01
  layer model_0__forward_module.module.output_conv act-std: mean=3.048e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=3.343e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=4.949e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.223e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=4.781e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.041e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.093e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.148e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.127e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.890e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.899e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.757e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.632e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=5.837e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=8.544e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.642e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.888e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.270e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.267e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.008e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=4.747e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=1.004e+00
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.522e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.051e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.577e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.161e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.689e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.476e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.119e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.227e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.385e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.044e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.028e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=6.017e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.744e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=2.090e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.944e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.134e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=9.888e-02
  param model_2__forward_module.module.output_scale grad-norm: mean=3.882e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=7.545e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.657e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=7.574e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.953e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.115e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.097e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.573e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.138e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.764e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.601e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.717e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.651e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=4.904e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.715e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.847e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=8.431e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.165e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.111e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=4.415e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.307e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.009e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.085e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.206e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.218e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.401e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.490e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.554e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.289e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.240e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.327e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.508e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=3.640e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.926e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.245e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=7.157e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.657e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.333e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=5.354e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.669e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.817e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.708e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.628e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.571e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.238e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.945e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.207e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.814e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.229e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.657e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.479e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.776e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=2.623e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.406e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=8.249e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.511e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=1.654e-01

  param model_5__forward_module.module.output_scale grad-norm: mean=4.860e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.103e+00Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.395e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.573e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.376e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.335e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.320e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.827e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.712e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.693e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.617e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.316e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.900e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.972e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.092e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.243e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=8.023e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.551e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.227e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=4.376e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.425e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.100e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.811e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.658e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.172e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.174e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.339e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.414e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.895e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.849e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.305e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.361e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.239e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=3.703e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=8.443e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.011e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.745e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.966e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=4.300e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=1.673e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=3.652e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.630e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.289e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.510e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.255e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.761e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.353e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.297e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.813e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.284e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.126e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.748e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=3.588e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=7.445e-03
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=7.807e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.426e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.551e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.75it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:06,  1.75it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.75it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.74it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.74it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.73it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.73it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.73it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.74it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.73it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.73it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.72it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.72it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.58it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.73it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.62it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.72it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.65it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.72it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.62it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.66it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.64it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.68it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.67it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]
Epoch 11/32 - Train Loss: 18.359836 - Test Loss: 18.375362
Training epochs:  34%|███▍      | 11/32 [17:54<35:09, 100.44s/it]
Epoch 12/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.68it/s]
Training epochs:  34%|███▍      | 11/32 [17:54<35:10, 100.48s/it]
Epoch 12/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 12/32:   2%|▏         | 1/52 [00:01<01:38,  1.94s/it][A
Epoch 12/32:   2%|▏         | 1/52 [00:01<01:31,  1.79s/it][A

Epoch 12/32:   4%|▍         | 2/52 [00:03<01:33,  1.87s/it][AEpoch 12/32:   4%|▍         | 2/52 [00:03<01:30,  1.81s/it][A
Epoch 12/32:   6%|▌         | 3/52 [00:05<01:30,  1.85s/it][A
Epoch 12/32:   6%|▌         | 3/52 [00:05<01:29,  1.82s/it][A
Epoch 12/32:   8%|▊         | 4/52 [00:07<01:28,  1.84s/it][A
Epoch 12/32:   8%|▊         | 4/52 [00:07<01:27,  1.82s/it][A
Epoch 12/32:  10%|▉         | 5/52 [00:09<01:26,  1.84s/it][A
Epoch 12/32:  10%|▉         | 5/52 [00:09<01:25,  1.83s/it][A
Epoch 12/32:  12%|█▏        | 6/52 [00:11<01:24,  1.83s/it][A
Epoch 12/32:  12%|█▏        | 6/52 [00:10<01:24,  1.83s/it][A
Epoch 12/32:  13%|█▎        | 7/52 [00:12<01:22,  1.82s/it][A
Epoch 12/32:  13%|█▎        | 7/52 [00:12<01:22,  1.83s/it][A
Epoch 12/32:  15%|█▌        | 8/52 [00:14<01:20,  1.83s/it][A
Epoch 12/32:  15%|█▌        | 8/52 [00:14<01:20,  1.83s/it][A
Epoch 12/32:  17%|█▋        | 9/52 [00:16<01:18,  1.83s/it][A
Epoch 12/32:  17%|█▋        | 9/52 [00:16<01:18,  1.83s/it][A
Epoch 12/32:  19%|█▉        | 10/52 [00:18<01:17,  1.83s/it][A
Epoch 12/32:  19%|█▉        | 10/52 [00:18<01:17,  1.83s/it][A
Epoch 12/32:  21%|██        | 11/52 [00:20<01:14,  1.83s/it][A
Epoch 12/32:  21%|██        | 11/52 [00:20<01:14,  1.83s/it][A
Epoch 12/32:  23%|██▎       | 12/52 [00:22<01:13,  1.83s/it][A
Epoch 12/32:  23%|██▎       | 12/52 [00:21<01:13,  1.83s/it][A
Epoch 12/32:  25%|██▌       | 13/52 [00:23<01:11,  1.83s/it][A
Epoch 12/32:  25%|██▌       | 13/52 [00:23<01:11,  1.83s/it][A
Epoch 12/32:  27%|██▋       | 14/52 [00:25<01:09,  1.83s/it][A
Epoch 12/32:  27%|██▋       | 14/52 [00:25<01:09,  1.83s/it][A
Epoch 12/32:  29%|██▉       | 15/52 [00:27<01:07,  1.84s/it][A
Epoch 12/32:  29%|██▉       | 15/52 [00:27<01:07,  1.83s/it][A
Epoch 12/32:  31%|███       | 16/52 [00:29<01:05,  1.83s/it][A
Epoch 12/32:  31%|███       | 16/52 [00:29<01:05,  1.83s/it][A
Epoch 12/32:  33%|███▎      | 17/52 [00:31<01:04,  1.83s/it][A
Epoch 12/32:  33%|███▎      | 17/52 [00:31<01:03,  1.83s/it][A

Epoch 12/32:  35%|███▍      | 18/52 [00:33<01:02,  1.83s/it][AEpoch 12/32:  35%|███▍      | 18/52 [00:32<01:02,  1.83s/it][A
Epoch 12/32:  37%|███▋      | 19/52 [00:34<01:00,  1.83s/it][A
Epoch 12/32:  37%|███▋      | 19/52 [00:34<01:00,  1.83s/it][A
Epoch 12/32:  38%|███▊      | 20/52 [00:36<00:58,  1.83s/it][A
Epoch 12/32:  38%|███▊      | 20/52 [00:36<00:58,  1.83s/it][A
Epoch 12/32:  40%|████      | 21/52 [00:38<00:56,  1.83s/it][A
Epoch 12/32:  40%|████      | 21/52 [00:38<00:56,  1.83s/it][A
Epoch 12/32:  42%|████▏     | 22/52 [00:40<00:54,  1.82s/it][A
Epoch 12/32:  42%|████▏     | 22/52 [00:40<00:54,  1.82s/it][A
Epoch 12/32:  44%|████▍     | 23/52 [00:42<00:52,  1.83s/it][A
Epoch 12/32:  44%|████▍     | 23/52 [00:42<00:52,  1.83s/it][A
Epoch 12/32:  46%|████▌     | 24/52 [00:43<00:51,  1.83s/it][A
Epoch 12/32:  46%|████▌     | 24/52 [00:44<00:51,  1.83s/it][A
Epoch 12/32:  48%|████▊     | 25/52 [00:45<00:49,  1.83s/it][A
Epoch 12/32:  48%|████▊     | 25/52 [00:45<00:49,  1.83s/it][A
Epoch 12/32:  50%|█████     | 26/52 [00:47<00:48,  1.88s/it][A
Epoch 12/32:  50%|█████     | 26/52 [00:47<00:48,  1.88s/it][A
Epoch 12/32:  52%|█████▏    | 27/52 [00:49<00:46,  1.86s/it][A
Epoch 12/32:  52%|█████▏    | 27/52 [00:49<00:46,  1.86s/it][A
Epoch 12/32:  54%|█████▍    | 28/52 [00:51<00:44,  1.84s/it][A
Epoch 12/32:  54%|█████▍    | 28/52 [00:51<00:44,  1.84s/it][A
Epoch 12/32:  56%|█████▌    | 29/52 [00:53<00:42,  1.84s/it][A
Epoch 12/32:  56%|█████▌    | 29/52 [00:53<00:42,  1.84s/it][A
Epoch 12/32:  58%|█████▊    | 30/52 [00:54<00:40,  1.85s/it][A
Epoch 12/32:  58%|█████▊    | 30/52 [00:55<00:40,  1.85s/it][A
Epoch 12/32:  60%|█████▉    | 31/52 [00:56<00:38,  1.84s/it][A
Epoch 12/32:  60%|█████▉    | 31/52 [00:56<00:38,  1.84s/it][A
Epoch 12/32:  62%|██████▏   | 32/52 [00:58<00:36,  1.84s/it][A
Epoch 12/32:  62%|██████▏   | 32/52 [00:58<00:36,  1.84s/it][A
Epoch 12/32:  63%|██████▎   | 33/52 [01:00<00:34,  1.84s/it][A
Epoch 12/32:  63%|██████▎   | 33/52 [01:00<00:34,  1.84s/it][A
Epoch 12/32:  65%|██████▌   | 34/52 [01:02<00:32,  1.83s/it][A
Epoch 12/32:  65%|██████▌   | 34/52 [01:02<00:32,  1.83s/it][A
Epoch 12/32:  67%|██████▋   | 35/52 [01:04<00:31,  1.83s/it][A
Epoch 12/32:  67%|██████▋   | 35/52 [01:04<00:31,  1.83s/it][A
Epoch 12/32:  69%|██████▉   | 36/52 [01:06<00:29,  1.83s/it][A
Epoch 12/32:  69%|██████▉   | 36/52 [01:05<00:29,  1.84s/it][A
Epoch 12/32:  71%|███████   | 37/52 [01:07<00:27,  1.83s/it][A
Epoch 12/32:  71%|███████   | 37/52 [01:07<00:27,  1.84s/it][A
Epoch 12/32:  73%|███████▎  | 38/52 [01:09<00:25,  1.83s/it][A
Epoch 12/32:  73%|███████▎  | 38/52 [01:09<00:25,  1.83s/it][A

Epoch 12/32:  75%|███████▌  | 39/52 [01:11<00:23,  1.83s/it][AEpoch 12/32:  75%|███████▌  | 39/52 [01:11<00:23,  1.83s/it][A
Epoch 12/32:  77%|███████▋  | 40/52 [01:13<00:21,  1.83s/it][A
Epoch 12/32:  77%|███████▋  | 40/52 [01:13<00:21,  1.83s/it][A
Epoch 12/32:  79%|███████▉  | 41/52 [01:15<00:20,  1.83s/it][A
Epoch 12/32:  79%|███████▉  | 41/52 [01:15<00:20,  1.83s/it][A
Epoch 12/32:  81%|████████  | 42/52 [01:17<00:18,  1.83s/it][A
Epoch 12/32:  81%|████████  | 42/52 [01:16<00:18,  1.83s/it][A
Epoch 12/32:  83%|████████▎ | 43/52 [01:18<00:16,  1.83s/it][A
Epoch 12/32:  83%|████████▎ | 43/52 [01:18<00:16,  1.83s/it][A
Epoch 12/32:  85%|████████▍ | 44/52 [01:20<00:14,  1.83s/it][A
Epoch 12/32:  85%|████████▍ | 44/52 [01:20<00:14,  1.83s/it][A
Epoch 12/32:  87%|████████▋ | 45/52 [01:22<00:12,  1.83s/it][A
Epoch 12/32:  87%|████████▋ | 45/52 [01:22<00:12,  1.83s/it][A
Epoch 12/32:  88%|████████▊ | 46/52 [01:24<00:10,  1.83s/it][A
Epoch 12/32:  88%|████████▊ | 46/52 [01:24<00:10,  1.83s/it][A
Epoch 12/32:  90%|█████████ | 47/52 [01:26<00:09,  1.83s/it][A
Epoch 12/32:  90%|█████████ | 47/52 [01:26<00:09,  1.83s/it][A

Epoch 12/32:  92%|█████████▏| 48/52 [01:27<00:07,  1.84s/it]Epoch 12/32:  92%|█████████▏| 48/52 [01:28<00:07,  1.84s/it][A[A
Epoch 12/32:  94%|█████████▍| 49/52 [01:29<00:05,  1.84s/it][A
Epoch 12/32:  94%|█████████▍| 49/52 [01:29<00:05,  1.84s/it][A

Epoch 12/32:  96%|█████████▌| 50/52 [01:31<00:03,  1.85s/it][AEpoch 12/32:  96%|█████████▌| 50/52 [01:31<00:03,  1.85s/it][A
Epoch 12/32:  98%|█████████▊| 51/52 [01:33<00:01,  1.85s/it][A
Epoch 12/32:  98%|█████████▊| 51/52 [01:33<00:01,  1.85s/it][A
Epoch 12/32: 100%|██████████| 52/52 [01:35<00:00,  1.74s/it]
[AEpoch 12/32: 100%|██████████| 52/52 [01:35<00:00,  1.83s/it]

[β=2.0] Epoch 12 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=6.724e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=3.784e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=4.726e-01
Epoch 12/32: 100%|██████████| 52/52 [01:35<00:00,  1.74s/it]  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.799e-01[A
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.061e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.557e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.818e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.451e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.011e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.878e-01Epoch 12/32: 100%|██████████| 52/52 [01:35<00:00,  1.83s/it]

  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.528e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.442e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.138e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.224e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=8.874e-02
  layer model_6__forward_module.module.output_conv act-std: mean=1.790e-01
  layer model_5__forward_module.module.input_conv act-std: mean=8.026e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.844e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.197e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=9.068e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.526e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=7.051e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.068e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.693e-01
  layer model_4__forward_module.module.input_conv act-std: mean=7.355e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.012e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.821e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=9.774e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.139e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.596e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.748e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.753e-01
  layer model_3__forward_module.module.input_conv act-std: mean=7.576e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.114e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.461e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.259e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.673e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.935e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.912e-01
  layer model_3__forward_module.module.output_conv act-std: mean=2.002e-01
  layer model_2__forward_module.module.input_conv act-std: mean=9.880e-02
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.626e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=5.213e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.209e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.726e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.041e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=4.484e-01
  layer model_2__forward_module.module.output_conv act-std: mean=2.466e-01
  layer model_1__forward_module.module.input_conv act-std: mean=9.213e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.709e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.604e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=4.095e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.518e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.986e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=8.330e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.871e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.403e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.923e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.100e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.009e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.004e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.707e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=4.685e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.952e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=2.827e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=5.118e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.270e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=5.240e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.399e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.158e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.219e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.267e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.841e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.728e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.871e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.700e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.110e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=8.366e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.721e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.944e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.505e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.109e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=9.558e-02
  param model_1__forward_module.module.output_scale grad-norm: mean=4.166e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=1.021e+00
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.570e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.067e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.651e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.098e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.279e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.516e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=6.010e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.742e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.796e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.454e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.807e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=6.195e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.997e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=2.332e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=8.838e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.008e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.054e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=3.576e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=7.612e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.623e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=9.118e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.335e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.455e-03
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=7.895e-03
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.991e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.761e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.626e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.554e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.630e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.640e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=4.678e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.745e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.800e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=8.221e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.806e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.014e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=3.964e-01

  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.303e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=2.980e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.209e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.152e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.245e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.473e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.315e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.013e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.364e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.100e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.039e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.971e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=3.349e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=2.030e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.213e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=7.350e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.512e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.255e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=4.543e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.352e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.999e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.569e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.364e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.485e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.824e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.370e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.337e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.397e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.774e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.134e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.348e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.895e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=2.269e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.114e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=6.755e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.380e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=1.540e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=4.184e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.170e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.546e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.013e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.574e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.442e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.522e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.836e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.993e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.892e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.814e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.658e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.097e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.900e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.259e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.371e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=9.405e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.659e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.440e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=3.859e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.388e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=2.943e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.840e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.570e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.712e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.556e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.307e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.233e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.753e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.406e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.589e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.124e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.002e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=3.316e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=8.561e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=9.687e-03
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.772e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.762e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=4.019e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.222e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=4.933e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.255e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.626e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.436e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.691e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.753e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.920e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.948e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.937e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.216e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.324e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.218e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=4.561e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.045e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=9.997e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.605e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.765e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.71it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.71it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.72it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.71it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.72it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.71it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.72it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.72it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.72it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.72it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.73it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.72it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.73it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.71it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.73it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.71it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.72it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.71it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.72it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.71it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.71it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.69it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.69it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.69it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]
Epoch 12/32 - Train Loss: 18.195632 - Test Loss: 18.208312

Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]
Training epochs:  38%|███▊      | 12/32 [19:37<33:43, 101.16s/it]
Epoch 13/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  38%|███▊      | 12/32 [19:37<33:42, 101.15s/it]
Epoch 13/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 13/32:   2%|▏         | 1/52 [00:01<01:34,  1.86s/it][A
Epoch 13/32:   2%|▏         | 1/52 [00:01<01:33,  1.84s/it][A
Epoch 13/32:   4%|▍         | 2/52 [00:03<01:32,  1.84s/it][A
Epoch 13/32:   4%|▍         | 2/52 [00:03<01:31,  1.84s/it][A
Epoch 13/32:   6%|▌         | 3/52 [00:05<01:30,  1.84s/it][A
Epoch 13/32:   6%|▌         | 3/52 [00:05<01:30,  1.84s/it][A
Epoch 13/32:   8%|▊         | 4/52 [00:07<01:29,  1.86s/it][A
Epoch 13/32:   8%|▊         | 4/52 [00:07<01:29,  1.86s/it][A
Epoch 13/32:  10%|▉         | 5/52 [00:09<01:27,  1.86s/it][A
Epoch 13/32:  10%|▉         | 5/52 [00:09<01:27,  1.86s/it][A
Epoch 13/32:  12%|█▏        | 6/52 [00:11<01:25,  1.87s/it][A
Epoch 13/32:  12%|█▏        | 6/52 [00:11<01:25,  1.87s/it][A
Epoch 13/32:  13%|█▎        | 7/52 [00:12<01:23,  1.85s/it][A
Epoch 13/32:  13%|█▎        | 7/52 [00:12<01:23,  1.85s/it][A
Epoch 13/32:  15%|█▌        | 8/52 [00:14<01:20,  1.84s/it][A
Epoch 13/32:  15%|█▌        | 8/52 [00:14<01:20,  1.84s/it][A
Epoch 13/32:  17%|█▋        | 9/52 [00:16<01:18,  1.83s/it][A
Epoch 13/32:  17%|█▋        | 9/52 [00:16<01:18,  1.83s/it][A
Epoch 13/32:  19%|█▉        | 10/52 [00:18<01:16,  1.83s/it][A
Epoch 13/32:  19%|█▉        | 10/52 [00:18<01:16,  1.83s/it][A
Epoch 13/32:  21%|██        | 11/52 [00:20<01:14,  1.82s/it][A
Epoch 13/32:  21%|██        | 11/52 [00:20<01:14,  1.83s/it][A
Epoch 13/32:  23%|██▎       | 12/52 [00:22<01:13,  1.83s/it][A
Epoch 13/32:  23%|██▎       | 12/52 [00:22<01:13,  1.83s/it][A
Epoch 13/32:  25%|██▌       | 13/52 [00:23<01:11,  1.82s/it][A
Epoch 13/32:  25%|██▌       | 13/52 [00:23<01:11,  1.83s/it][A
Epoch 13/32:  27%|██▋       | 14/52 [00:25<01:09,  1.83s/it][A
Epoch 13/32:  27%|██▋       | 14/52 [00:25<01:09,  1.83s/it][A
Epoch 13/32:  29%|██▉       | 15/52 [00:27<01:07,  1.83s/it][A
Epoch 13/32:  29%|██▉       | 15/52 [00:27<01:07,  1.83s/it][A
Epoch 13/32:  31%|███       | 16/52 [00:29<01:05,  1.83s/it][A
Epoch 13/32:  31%|███       | 16/52 [00:29<01:06,  1.83s/it][A
Epoch 13/32:  33%|███▎      | 17/52 [00:31<01:03,  1.83s/it][A
Epoch 13/32:  33%|███▎      | 17/52 [00:31<01:03,  1.83s/it][A
Epoch 13/32:  35%|███▍      | 18/52 [00:33<01:02,  1.83s/it][A
Epoch 13/32:  35%|███▍      | 18/52 [00:33<01:02,  1.83s/it][A
Epoch 13/32:  37%|███▋      | 19/52 [00:34<01:00,  1.82s/it][A
Epoch 13/32:  37%|███▋      | 19/52 [00:34<01:00,  1.82s/it][A
Epoch 13/32:  38%|███▊      | 20/52 [00:36<00:58,  1.82s/it][A
Epoch 13/32:  38%|███▊      | 20/52 [00:36<00:58,  1.83s/it][A
Epoch 13/32:  40%|████      | 21/52 [00:38<00:56,  1.84s/it][A
Epoch 13/32:  40%|████      | 21/52 [00:38<00:57,  1.84s/it][A
Epoch 13/32:  42%|████▏     | 22/52 [00:40<00:55,  1.84s/it][A
Epoch 13/32:  42%|████▏     | 22/52 [00:40<00:55,  1.84s/it][A
Epoch 13/32:  44%|████▍     | 23/52 [00:42<00:53,  1.84s/it][A
Epoch 13/32:  44%|████▍     | 23/52 [00:42<00:53,  1.84s/it][A
Epoch 13/32:  46%|████▌     | 24/52 [00:44<00:51,  1.84s/it][A
Epoch 13/32:  46%|████▌     | 24/52 [00:44<00:51,  1.84s/it][A
Epoch 13/32:  48%|████▊     | 25/52 [00:45<00:49,  1.84s/it][A
Epoch 13/32:  48%|████▊     | 25/52 [00:45<00:49,  1.84s/it][A
Epoch 13/32:  50%|█████     | 26/52 [00:47<00:47,  1.84s/it][A
Epoch 13/32:  50%|█████     | 26/52 [00:47<00:47,  1.84s/it][A
Epoch 13/32:  52%|█████▏    | 27/52 [00:49<00:46,  1.84s/it][A
Epoch 13/32:  52%|█████▏    | 27/52 [00:49<00:46,  1.84s/it][A
Epoch 13/32:  54%|█████▍    | 28/52 [00:51<00:44,  1.84s/it][A
Epoch 13/32:  54%|█████▍    | 28/52 [00:51<00:44,  1.85s/it][A
Epoch 13/32:  56%|█████▌    | 29/52 [00:53<00:42,  1.85s/it][A
Epoch 13/32:  56%|█████▌    | 29/52 [00:53<00:42,  1.85s/it][A
Epoch 13/32:  58%|█████▊    | 30/52 [00:55<00:41,  1.87s/it][A
Epoch 13/32:  58%|█████▊    | 30/52 [00:55<00:41,  1.87s/it][A
Epoch 13/32:  60%|█████▉    | 31/52 [00:57<00:39,  1.86s/it][A
Epoch 13/32:  60%|█████▉    | 31/52 [00:57<00:39,  1.86s/it][A
Epoch 13/32:  62%|██████▏   | 32/52 [00:58<00:37,  1.86s/it][A
Epoch 13/32:  62%|██████▏   | 32/52 [00:58<00:37,  1.86s/it][A
Epoch 13/32:  63%|██████▎   | 33/52 [01:00<00:35,  1.86s/it][A
Epoch 13/32:  63%|██████▎   | 33/52 [01:00<00:35,  1.86s/it][A
Epoch 13/32:  65%|██████▌   | 34/52 [01:02<00:33,  1.85s/it][A
Epoch 13/32:  65%|██████▌   | 34/52 [01:02<00:33,  1.85s/it][A
Epoch 13/32:  67%|██████▋   | 35/52 [01:04<00:31,  1.85s/it][A
Epoch 13/32:  67%|██████▋   | 35/52 [01:04<00:31,  1.85s/it][A
Epoch 13/32:  69%|██████▉   | 36/52 [01:06<00:29,  1.85s/it][A
Epoch 13/32:  69%|██████▉   | 36/52 [01:06<00:29,  1.85s/it][A
Epoch 13/32:  71%|███████   | 37/52 [01:08<00:27,  1.86s/it][A
Epoch 13/32:  71%|███████   | 37/52 [01:08<00:27,  1.86s/it][A
Epoch 13/32:  73%|███████▎  | 38/52 [01:10<00:26,  1.87s/it][A
Epoch 13/32:  73%|███████▎  | 38/52 [01:10<00:26,  1.87s/it][A
Epoch 13/32:  75%|███████▌  | 39/52 [01:11<00:24,  1.86s/it][A
Epoch 13/32:  75%|███████▌  | 39/52 [01:11<00:24,  1.86s/it][A
Epoch 13/32:  77%|███████▋  | 40/52 [01:13<00:22,  1.86s/it][A
Epoch 13/32:  77%|███████▋  | 40/52 [01:13<00:22,  1.86s/it][A
Epoch 13/32:  79%|███████▉  | 41/52 [01:15<00:20,  1.85s/it][A
Epoch 13/32:  79%|███████▉  | 41/52 [01:15<00:20,  1.85s/it][A
Epoch 13/32:  81%|████████  | 42/52 [01:17<00:18,  1.85s/it][A
Epoch 13/32:  81%|████████  | 42/52 [01:17<00:18,  1.85s/it][A
Epoch 13/32:  83%|████████▎ | 43/52 [01:19<00:16,  1.85s/it][A
Epoch 13/32:  83%|████████▎ | 43/52 [01:19<00:16,  1.85s/it][A
Epoch 13/32:  85%|████████▍ | 44/52 [01:21<00:14,  1.85s/it][A
Epoch 13/32:  85%|████████▍ | 44/52 [01:21<00:14,  1.85s/it][A
Epoch 13/32:  87%|████████▋ | 45/52 [01:23<00:12,  1.85s/it][A
Epoch 13/32:  87%|████████▋ | 45/52 [01:23<00:12,  1.85s/it][A
Epoch 13/32:  88%|████████▊ | 46/52 [01:24<00:11,  1.85s/it][A
Epoch 13/32:  88%|████████▊ | 46/52 [01:24<00:11,  1.85s/it][A
Epoch 13/32:  90%|█████████ | 47/52 [01:26<00:09,  1.85s/it][A
Epoch 13/32:  90%|█████████ | 47/52 [01:26<00:09,  1.85s/it][A
Epoch 13/32:  92%|█████████▏| 48/52 [01:28<00:07,  1.86s/it][A
Epoch 13/32:  92%|█████████▏| 48/52 [01:28<00:07,  1.86s/it][A
Epoch 13/32:  94%|█████████▍| 49/52 [01:30<00:05,  1.86s/it][A
Epoch 13/32:  94%|█████████▍| 49/52 [01:30<00:05,  1.86s/it][A
Epoch 13/32:  96%|█████████▌| 50/52 [01:32<00:03,  1.85s/it][A
Epoch 13/32:  96%|█████████▌| 50/52 [01:32<00:03,  1.85s/it][A
Epoch 13/32:  98%|█████████▊| 51/52 [01:34<00:01,  1.86s/it][A
Epoch 13/32:  98%|█████████▊| 51/52 [01:34<00:01,  1.85s/it][A
Epoch 13/32: 100%|██████████| 52/52 [01:35<00:00,  1.80s/it][AEpoch 13/32: 100%|██████████| 52/52 [01:35<00:00,  1.84s/it]

[β=2.0] Epoch 13 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=6.867e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=4.540e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=5.992e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.989e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.105e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.721e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.794e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.487e-01
  layer model_6__forward_module.module.input_conv act-std: mean=9.932e-02
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.900e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.623e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.465e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.105e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.352e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=9.559e-02
  layer model_6__forward_module.module.output_conv act-std: mean=1.821e-01
  layer model_5__forward_module.module.input_conv act-std: mean=8.527e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.829e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.007e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=8.518e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.441e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=6.658e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.001e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.585e-01
  layer model_4__forward_module.module.input_conv act-std: mean=7.954e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=1.906e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.703e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=9.935e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.121e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.579e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.830e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.687e-01
  layer model_3__forward_module.module.input_conv act-std: mean=7.651e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.054e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.293e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.244e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.647e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.733e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.953e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.947e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.019e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.475e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=4.962e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.123e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.562e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.021e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=4.630e-01
  layer model_2__forward_module.module.output_conv act-std: mean=2.234e-01
  layer model_1__forward_module.module.input_conv act-std: mean=9.815e-02
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.632e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.554e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.867e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.365e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.959e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=8.769e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.753e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.397e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.799e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=7.889e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=1.013e+00
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.011e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.807e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=5.072e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.900e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=2.461e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=4.959e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.214e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=5.193e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=7.717e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.351e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.414e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.226e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.469e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.285e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.750e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.455e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=5.762e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=7.081e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.435e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.749e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.775e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.286e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.032e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=3.731e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=8.045e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.974e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=9.185e-02
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.249e-01

  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.107e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.982e-03
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.848e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.958e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.329e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.276e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.910e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.979e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=4.774e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.671e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.847e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.060e-03
Epoch 13/32: 100%|██████████| 52/52 [01:35<00:00,  1.80s/it]  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.064e-01[A
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.041e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=3.207e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=6.601e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.322e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=7.494e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=5.857e-02Epoch 13/32: 100%|██████████| 52/52 [01:35<00:00,  1.84s/it]

  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.088e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=8.639e-03
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.132e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.037e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.257e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.008e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.835e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=5.078e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.731e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.560e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.384e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=6.425e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.865e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.015e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=3.494e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=8.372e-01
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=1.687e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=8.876e-02
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=6.342e-02
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.107e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.165e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.926e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.302e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.980e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.210e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.401e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.618e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.184e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.437e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=8.002e-03
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=4.915e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.605e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.109e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=3.973e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=9.926e-01
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.007e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.448e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=9.924e-02
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.418e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.261e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.826e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.369e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.649e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.227e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.809e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.857e-03
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.157e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.800e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=9.102e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=5.457e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.232e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=1.245e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=3.677e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=8.809e-01
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=1.754e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.603e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.214e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.307e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.007e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.572e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.741e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.743e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.325e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.199e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.433e-03
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.648e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.218e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=9.106e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=6.499e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.422e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.307e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=3.572e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.387e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=2.915e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.689e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.377e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.071e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.991e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.445e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.224e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.109e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.479e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.379e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.108e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.069e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=3.656e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=9.370e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.024e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.868e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.635e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=3.605e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.197e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=4.810e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=1.877e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.381e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.613e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.792e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.425e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.667e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.579e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.598e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.100e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.156e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.737e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=3.807e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.071e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=9.709e-03
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=1.848e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.598e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.71it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.70it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.72it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.72it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.72it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.72it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.72it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.72it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.73it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.73it/s][A

Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.72it/s][AEvaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.72it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.72it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.71it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.71it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.69it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.71it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.68it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.72it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.70it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.69it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.66it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.67it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.65it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.68it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s]
Training epochs:  41%|████      | 13/32 [21:20<32:15, 101.86s/it]
Epoch 14/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s]
Epoch 13/32 - Train Loss: 18.041440 - Test Loss: 18.073524
Training epochs:  41%|████      | 13/32 [21:20<32:16, 101.90s/it]
Epoch 14/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 14/32:   2%|▏         | 1/52 [00:01<01:36,  1.89s/it][A
Epoch 14/32:   2%|▏         | 1/52 [00:01<01:32,  1.81s/it][A
Epoch 14/32:   4%|▍         | 2/52 [00:03<01:30,  1.82s/it][A
Epoch 14/32:   4%|▍         | 2/52 [00:03<01:32,  1.85s/it][A
Epoch 14/32:   6%|▌         | 3/52 [00:05<01:29,  1.83s/it][A
Epoch 14/32:   6%|▌         | 3/52 [00:05<01:30,  1.85s/it][A
Epoch 14/32:   8%|▊         | 4/52 [00:07<01:28,  1.84s/it][A
Epoch 14/32:   8%|▊         | 4/52 [00:07<01:28,  1.85s/it][A
Epoch 14/32:  10%|▉         | 5/52 [00:09<01:26,  1.84s/it][A
Epoch 14/32:  10%|▉         | 5/52 [00:09<01:26,  1.85s/it][A
Epoch 14/32:  12%|█▏        | 6/52 [00:11<01:25,  1.86s/it][A
Epoch 14/32:  12%|█▏        | 6/52 [00:11<01:25,  1.86s/it][A
Epoch 14/32:  13%|█▎        | 7/52 [00:12<01:24,  1.87s/it][A
Epoch 14/32:  13%|█▎        | 7/52 [00:13<01:24,  1.87s/it][A
Epoch 14/32:  15%|█▌        | 8/52 [00:14<01:22,  1.87s/it][A
Epoch 14/32:  15%|█▌        | 8/52 [00:14<01:22,  1.87s/it][A
Epoch 14/32:  17%|█▋        | 9/52 [00:16<01:20,  1.87s/it][A
Epoch 14/32:  17%|█▋        | 9/52 [00:16<01:20,  1.87s/it][A
Epoch 14/32:  19%|█▉        | 10/52 [00:18<01:18,  1.87s/it][A
Epoch 14/32:  19%|█▉        | 10/52 [00:18<01:18,  1.87s/it][A
Epoch 14/32:  21%|██        | 11/52 [00:20<01:16,  1.86s/it][A
Epoch 14/32:  21%|██        | 11/52 [00:20<01:16,  1.86s/it][A
Epoch 14/32:  23%|██▎       | 12/52 [00:22<01:14,  1.85s/it][A
Epoch 14/32:  23%|██▎       | 12/52 [00:22<01:14,  1.86s/it][A
Epoch 14/32:  25%|██▌       | 13/52 [00:24<01:12,  1.85s/it][A
Epoch 14/32:  25%|██▌       | 13/52 [00:24<01:12,  1.85s/it][A
Epoch 14/32:  27%|██▋       | 14/52 [00:25<01:10,  1.85s/it][A
Epoch 14/32:  27%|██▋       | 14/52 [00:26<01:10,  1.85s/it][A
Epoch 14/32:  29%|██▉       | 15/52 [00:27<01:08,  1.85s/it][A
Epoch 14/32:  29%|██▉       | 15/52 [00:27<01:08,  1.85s/it][A
Epoch 14/32:  31%|███       | 16/52 [00:29<01:06,  1.85s/it][A
Epoch 14/32:  31%|███       | 16/52 [00:29<01:06,  1.85s/it][A
Epoch 14/32:  33%|███▎      | 17/52 [00:31<01:06,  1.90s/it][A
Epoch 14/32:  33%|███▎      | 17/52 [00:31<01:06,  1.90s/it][A
Epoch 14/32:  35%|███▍      | 18/52 [00:33<01:04,  1.89s/it][A
Epoch 14/32:  35%|███▍      | 18/52 [00:33<01:04,  1.89s/it][A
Epoch 14/32:  37%|███▋      | 19/52 [00:35<01:02,  1.88s/it][A
Epoch 14/32:  37%|███▋      | 19/52 [00:35<01:02,  1.88s/it][A

Epoch 14/32:  38%|███▊      | 20/52 [00:37<01:00,  1.88s/it][AEpoch 14/32:  38%|███▊      | 20/52 [00:37<01:00,  1.88s/it][A
Epoch 14/32:  40%|████      | 21/52 [00:39<00:58,  1.87s/it][A
Epoch 14/32:  40%|████      | 21/52 [00:39<00:58,  1.87s/it][A
Epoch 14/32:  42%|████▏     | 22/52 [00:41<00:56,  1.87s/it][A
Epoch 14/32:  42%|████▏     | 22/52 [00:40<00:56,  1.87s/it][A
Epoch 14/32:  44%|████▍     | 23/52 [00:42<00:54,  1.87s/it][A
Epoch 14/32:  44%|████▍     | 23/52 [00:42<00:54,  1.87s/it][A
Epoch 14/32:  46%|████▌     | 24/52 [00:44<00:52,  1.86s/it][A
Epoch 14/32:  46%|████▌     | 24/52 [00:44<00:52,  1.86s/it][A
Epoch 14/32:  48%|████▊     | 25/52 [00:46<00:50,  1.88s/it][A
Epoch 14/32:  48%|████▊     | 25/52 [00:46<00:50,  1.88s/it][A
Epoch 14/32:  50%|█████     | 26/52 [00:48<00:48,  1.88s/it][A
Epoch 14/32:  50%|█████     | 26/52 [00:48<00:48,  1.88s/it][A
Epoch 14/32:  52%|█████▏    | 27/52 [00:50<00:46,  1.87s/it][A
Epoch 14/32:  52%|█████▏    | 27/52 [00:50<00:46,  1.87s/it][A
Epoch 14/32:  54%|█████▍    | 28/52 [00:52<00:44,  1.87s/it][A
Epoch 14/32:  54%|█████▍    | 28/52 [00:52<00:44,  1.87s/it][A
Epoch 14/32:  56%|█████▌    | 29/52 [00:54<00:42,  1.87s/it][A
Epoch 14/32:  56%|█████▌    | 29/52 [00:54<00:42,  1.87s/it][A
Epoch 14/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.87s/it][A
Epoch 14/32:  58%|█████▊    | 30/52 [00:55<00:41,  1.87s/it][A
Epoch 14/32:  60%|█████▉    | 31/52 [00:57<00:39,  1.88s/it][A
Epoch 14/32:  60%|█████▉    | 31/52 [00:57<00:39,  1.88s/it][A
Epoch 14/32:  62%|██████▏   | 32/52 [00:59<00:37,  1.87s/it][A
Epoch 14/32:  62%|██████▏   | 32/52 [00:59<00:37,  1.87s/it][A
Epoch 14/32:  63%|██████▎   | 33/52 [01:01<00:35,  1.87s/it][A
Epoch 14/32:  63%|██████▎   | 33/52 [01:01<00:35,  1.87s/it][A
Epoch 14/32:  65%|██████▌   | 34/52 [01:03<00:33,  1.87s/it][A
Epoch 14/32:  65%|██████▌   | 34/52 [01:03<00:33,  1.87s/it][A
Epoch 14/32:  67%|██████▋   | 35/52 [01:05<00:31,  1.87s/it][A
Epoch 14/32:  67%|██████▋   | 35/52 [01:05<00:31,  1.87s/it][A
Epoch 14/32:  69%|██████▉   | 36/52 [01:07<00:29,  1.86s/it][A
Epoch 14/32:  69%|██████▉   | 36/52 [01:07<00:29,  1.87s/it][A
Epoch 14/32:  71%|███████   | 37/52 [01:09<00:27,  1.86s/it][A
Epoch 14/32:  71%|███████   | 37/52 [01:09<00:27,  1.87s/it][A
Epoch 14/32:  73%|███████▎  | 38/52 [01:10<00:25,  1.85s/it][A
Epoch 14/32:  73%|███████▎  | 38/52 [01:10<00:25,  1.85s/it][A
Epoch 14/32:  75%|███████▌  | 39/52 [01:12<00:23,  1.84s/it][A
Epoch 14/32:  75%|███████▌  | 39/52 [01:12<00:23,  1.85s/it][A

Epoch 14/32:  77%|███████▋  | 40/52 [01:14<00:22,  1.84s/it][AEpoch 14/32:  77%|███████▋  | 40/52 [01:14<00:22,  1.84s/it][A
Epoch 14/32:  79%|███████▉  | 41/52 [01:16<00:20,  1.85s/it][A
Epoch 14/32:  79%|███████▉  | 41/52 [01:16<00:20,  1.85s/it][A
Epoch 14/32:  81%|████████  | 42/52 [01:18<00:18,  1.85s/it][A
Epoch 14/32:  81%|████████  | 42/52 [01:18<00:18,  1.85s/it][A
Epoch 14/32:  83%|████████▎ | 43/52 [01:20<00:16,  1.86s/it][A
Epoch 14/32:  83%|████████▎ | 43/52 [01:20<00:16,  1.86s/it][A
Epoch 14/32:  85%|████████▍ | 44/52 [01:22<00:14,  1.86s/it][A
Epoch 14/32:  85%|████████▍ | 44/52 [01:21<00:14,  1.86s/it][A
Epoch 14/32:  87%|████████▋ | 45/52 [01:23<00:13,  1.86s/it][A
Epoch 14/32:  87%|████████▋ | 45/52 [01:23<00:13,  1.87s/it][A
Epoch 14/32:  88%|████████▊ | 46/52 [01:25<00:11,  1.87s/it][A
Epoch 14/32:  88%|████████▊ | 46/52 [01:25<00:11,  1.87s/it][A
Epoch 14/32:  90%|█████████ | 47/52 [01:27<00:09,  1.87s/it][A
Epoch 14/32:  90%|█████████ | 47/52 [01:27<00:09,  1.87s/it][A
Epoch 14/32:  92%|█████████▏| 48/52 [01:29<00:07,  1.88s/it][A
Epoch 14/32:  92%|█████████▏| 48/52 [01:29<00:07,  1.88s/it][A
Epoch 14/32:  94%|█████████▍| 49/52 [01:31<00:05,  1.87s/it][A
Epoch 14/32:  94%|█████████▍| 49/52 [01:31<00:05,  1.87s/it][A
Epoch 14/32:  96%|█████████▌| 50/52 [01:33<00:03,  1.87s/it][A
Epoch 14/32:  96%|█████████▌| 50/52 [01:33<00:03,  1.87s/it][A
Epoch 14/32:  98%|█████████▊| 51/52 [01:35<00:01,  1.86s/it][A
Epoch 14/32:  98%|█████████▊| 51/52 [01:35<00:01,  1.86s/it][A
Epoch 14/32: 100%|██████████| 52/52 [01:36<00:00,  1.76s/it][AEpoch 14/32: 100%|██████████| 52/52 [01:36<00:00,  1.86s/it]

Epoch 14/32: 100%|██████████| 52/52 [01:36<00:00,  1.75s/it][AEpoch 14/32: 100%|██████████| 52/52 [01:36<00:00,  1.86s/it]

[β=2.0] Epoch 14 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=7.456e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=4.792e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=6.502e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.049e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.105e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.807e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.766e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.471e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.011e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.973e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.704e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.555e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.090e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.437e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.032e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.867e-01
  layer model_5__forward_module.module.input_conv act-std: mean=8.948e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.836e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=3.869e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=7.911e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.351e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=6.330e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.996e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.488e-01
  layer model_4__forward_module.module.input_conv act-std: mean=8.560e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=1.842e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.633e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=9.901e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.116e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.517e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.839e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.631e-01
  layer model_3__forward_module.module.input_conv act-std: mean=7.919e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=1.969e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.119e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.252e-01

  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.678e+00Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.675e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.994e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.908e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.040e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.426e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=4.880e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.105e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.483e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.062e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=4.854e-01
  layer model_2__forward_module.module.output_conv act-std: mean=2.023e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.043e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.565e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.439e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.732e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.260e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.865e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=8.948e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.672e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.405e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.778e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.084e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=9.992e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=3.018e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.970e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=5.502e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.859e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=2.342e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=5.451e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.338e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=5.717e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.316e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.299e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.288e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.945e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.239e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.260e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.717e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.370e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=5.686e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=6.372e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.201e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.656e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.205e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.249e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.072e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=3.255e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=7.007e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.678e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=8.904e-02
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.076e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.099e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.126e-03
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.561e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.410e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.347e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.022e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.602e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.825e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=3.870e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.449e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.613e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=6.214e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.009e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.094e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=3.276e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=7.057e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.371e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=8.379e-02
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=6.005e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.043e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=8.266e-03
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.194e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.897e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.346e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=9.717e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.891e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=4.974e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.388e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.538e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.432e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=6.759e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=1.761e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=9.712e-02
  param model_3__forward_module.module.output_scale grad-norm: mean=3.270e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.046e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=2.257e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.077e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=8.378e-02
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.296e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.478e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.906e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.689e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.093e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.520e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.882e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.390e-03
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.480e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.737e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.052e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=6.542e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.542e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.234e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=3.621e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.062e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.236e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.635e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.179e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.654e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.475e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.675e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.545e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.561e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.301e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=1.882e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.127e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.112e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=1.935e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=9.527e-03
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=5.819e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.381e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=1.449e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=3.439e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.083e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.258e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.863e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.373e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.504e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.351e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.650e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.034e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.992e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.498e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.575e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.021e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=2.155e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.864e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.146e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=8.492e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.644e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.432e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=3.184e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.614e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.482e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.952e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.726e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.781e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.478e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.977e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.330e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.035e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.400e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.799e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.084e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.261e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=4.279e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=9.762e-03
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.037e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=1.927e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=1.769e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=3.737e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=4.049e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=9.330e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=3.108e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.659e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.824e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.035e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.339e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.183e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.520e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.167e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.238e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.536e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.738e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.491e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.336e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.164e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.287e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.871e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.68it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.64it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.67it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.64it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.66it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.64it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.66it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.65it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.66it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.65it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.64it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.62it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.65it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.63it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.66it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.64it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.66it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.64it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.67it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.65it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.68it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.66it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.69it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.65it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.67it/s]
Training epochs:  44%|████▍     | 14/32 [23:05<30:47, 102.65s/it]
Epoch 15/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]
Epoch 14/32 - Train Loss: 17.920615 - Test Loss: 17.984311
Training epochs:  44%|████▍     | 14/32 [23:05<30:48, 102.69s/it]
Epoch 15/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 15/32:   2%|▏         | 1/52 [00:02<01:42,  2.01s/it][A
Epoch 15/32:   2%|▏         | 1/52 [00:01<01:35,  1.87s/it][A
Epoch 15/32:   4%|▍         | 2/52 [00:03<01:37,  1.94s/it][A
Epoch 15/32:   4%|▍         | 2/52 [00:03<01:34,  1.88s/it][A
Epoch 15/32:   6%|▌         | 3/52 [00:05<01:34,  1.93s/it][A
Epoch 15/32:   6%|▌         | 3/52 [00:05<01:32,  1.89s/it][A
Epoch 15/32:   8%|▊         | 4/52 [00:07<01:31,  1.91s/it][A
Epoch 15/32:   8%|▊         | 4/52 [00:07<01:30,  1.89s/it][A
Epoch 15/32:  10%|▉         | 5/52 [00:09<01:29,  1.90s/it][A
Epoch 15/32:  10%|▉         | 5/52 [00:09<01:28,  1.88s/it][A
Epoch 15/32:  12%|█▏        | 6/52 [00:11<01:26,  1.89s/it][A
Epoch 15/32:  12%|█▏        | 6/52 [00:11<01:26,  1.88s/it][A
Epoch 15/32:  13%|█▎        | 7/52 [00:13<01:24,  1.88s/it][A
Epoch 15/32:  13%|█▎        | 7/52 [00:13<01:24,  1.87s/it][A
Epoch 15/32:  15%|█▌        | 8/52 [00:15<01:22,  1.87s/it][A
Epoch 15/32:  15%|█▌        | 8/52 [00:15<01:22,  1.87s/it][A
Epoch 15/32:  17%|█▋        | 9/52 [00:17<01:20,  1.87s/it][A
Epoch 15/32:  17%|█▋        | 9/52 [00:16<01:20,  1.87s/it][A
Epoch 15/32:  19%|█▉        | 10/52 [00:18<01:18,  1.88s/it][A
Epoch 15/32:  19%|█▉        | 10/52 [00:18<01:18,  1.87s/it][A
Epoch 15/32:  21%|██        | 11/52 [00:20<01:16,  1.88s/it][A
Epoch 15/32:  21%|██        | 11/52 [00:20<01:16,  1.87s/it][A
Epoch 15/32:  23%|██▎       | 12/52 [00:22<01:14,  1.87s/it][A
Epoch 15/32:  23%|██▎       | 12/52 [00:22<01:14,  1.87s/it][A
Epoch 15/32:  25%|██▌       | 13/52 [00:24<01:12,  1.87s/it][A
Epoch 15/32:  25%|██▌       | 13/52 [00:24<01:12,  1.87s/it][A
Epoch 15/32:  27%|██▋       | 14/52 [00:26<01:11,  1.87s/it][A
Epoch 15/32:  27%|██▋       | 14/52 [00:26<01:11,  1.87s/it][A
Epoch 15/32:  29%|██▉       | 15/52 [00:28<01:09,  1.88s/it][A
Epoch 15/32:  29%|██▉       | 15/52 [00:28<01:09,  1.88s/it][A
Epoch 15/32:  31%|███       | 16/52 [00:30<01:07,  1.88s/it][A
Epoch 15/32:  31%|███       | 16/52 [00:30<01:07,  1.88s/it][A
Epoch 15/32:  33%|███▎      | 17/52 [00:32<01:05,  1.88s/it][A
Epoch 15/32:  33%|███▎      | 17/52 [00:31<01:05,  1.88s/it][A
Epoch 15/32:  35%|███▍      | 18/52 [00:33<01:03,  1.88s/it][A
Epoch 15/32:  35%|███▍      | 18/52 [00:33<01:03,  1.88s/it][A
Epoch 15/32:  37%|███▋      | 19/52 [00:35<01:01,  1.86s/it][A
Epoch 15/32:  37%|███▋      | 19/52 [00:35<01:01,  1.86s/it][A
Epoch 15/32:  38%|███▊      | 20/52 [00:37<00:59,  1.87s/it][A
Epoch 15/32:  38%|███▊      | 20/52 [00:37<00:59,  1.87s/it][A
Epoch 15/32:  40%|████      | 21/52 [00:39<00:58,  1.87s/it][A
Epoch 15/32:  40%|████      | 21/52 [00:39<00:58,  1.87s/it][A
Epoch 15/32:  42%|████▏     | 22/52 [00:41<00:56,  1.88s/it][A
Epoch 15/32:  42%|████▏     | 22/52 [00:41<00:56,  1.88s/it][A

Epoch 15/32:  44%|████▍     | 23/52 [00:43<00:54,  1.88s/it][AEpoch 15/32:  44%|████▍     | 23/52 [00:43<00:54,  1.88s/it][A
Epoch 15/32:  46%|████▌     | 24/52 [00:45<00:52,  1.88s/it][A
Epoch 15/32:  46%|████▌     | 24/52 [00:45<00:52,  1.88s/it][A
Epoch 15/32:  48%|████▊     | 25/52 [00:47<00:50,  1.88s/it][A
Epoch 15/32:  48%|████▊     | 25/52 [00:46<00:50,  1.88s/it][A
Epoch 15/32:  50%|█████     | 26/52 [00:48<00:48,  1.88s/it][A
Epoch 15/32:  50%|█████     | 26/52 [00:48<00:48,  1.88s/it][A
Epoch 15/32:  52%|█████▏    | 27/52 [00:50<00:47,  1.89s/it][A
Epoch 15/32:  52%|█████▏    | 27/52 [00:50<00:47,  1.89s/it][A
Epoch 15/32:  54%|█████▍    | 28/52 [00:52<00:45,  1.88s/it][A
Epoch 15/32:  54%|█████▍    | 28/52 [00:52<00:45,  1.88s/it][A
Epoch 15/32:  56%|█████▌    | 29/52 [00:54<00:43,  1.88s/it][A
Epoch 15/32:  56%|█████▌    | 29/52 [00:54<00:43,  1.88s/it][A
Epoch 15/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.88s/it][A
Epoch 15/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.88s/it][A
Epoch 15/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.88s/it][A
Epoch 15/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.88s/it][A
Epoch 15/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 15/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 15/32:  63%|██████▎   | 33/52 [01:02<00:35,  1.88s/it][A
Epoch 15/32:  63%|██████▎   | 33/52 [01:01<00:35,  1.88s/it][A
Epoch 15/32:  65%|██████▌   | 34/52 [01:03<00:33,  1.88s/it][A
Epoch 15/32:  65%|██████▌   | 34/52 [01:03<00:33,  1.89s/it][A
Epoch 15/32:  67%|██████▋   | 35/52 [01:05<00:31,  1.88s/it][A
Epoch 15/32:  67%|██████▋   | 35/52 [01:05<00:32,  1.88s/it][A
Epoch 15/32:  69%|██████▉   | 36/52 [01:07<00:30,  1.88s/it][A
Epoch 15/32:  69%|██████▉   | 36/52 [01:07<00:30,  1.88s/it][A
Epoch 15/32:  71%|███████   | 37/52 [01:09<00:28,  1.89s/it][A
Epoch 15/32:  71%|███████   | 37/52 [01:09<00:28,  1.89s/it][A
Epoch 15/32:  73%|███████▎  | 38/52 [01:11<00:26,  1.89s/it][A
Epoch 15/32:  73%|███████▎  | 38/52 [01:11<00:26,  1.89s/it][A
Epoch 15/32:  75%|███████▌  | 39/52 [01:13<00:24,  1.89s/it][A
Epoch 15/32:  75%|███████▌  | 39/52 [01:13<00:24,  1.89s/it][A
Epoch 15/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.90s/it][A
Epoch 15/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.90s/it][A
Epoch 15/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.89s/it][A
Epoch 15/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.89s/it][A
Epoch 15/32:  81%|████████  | 42/52 [01:19<00:18,  1.89s/it][A
Epoch 15/32:  81%|████████  | 42/52 [01:18<00:18,  1.89s/it][A
Epoch 15/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.89s/it][A
Epoch 15/32:  83%|████████▎ | 43/52 [01:20<00:17,  1.89s/it][A
Epoch 15/32:  85%|████████▍ | 44/52 [01:22<00:15,  1.89s/it][A
Epoch 15/32:  85%|████████▍ | 44/52 [01:22<00:15,  1.89s/it][A
Epoch 15/32:  87%|████████▋ | 45/52 [01:24<00:13,  1.88s/it][A
Epoch 15/32:  87%|████████▋ | 45/52 [01:24<00:13,  1.88s/it][A
Epoch 15/32:  88%|████████▊ | 46/52 [01:26<00:11,  1.88s/it][A
Epoch 15/32:  88%|████████▊ | 46/52 [01:26<00:11,  1.88s/it][A

Epoch 15/32:  90%|█████████ | 47/52 [01:28<00:09,  1.94s/it][AEpoch 15/32:  90%|█████████ | 47/52 [01:28<00:09,  1.95s/it][A
Epoch 15/32:  92%|█████████▏| 48/52 [01:30<00:07,  1.93s/it][A
Epoch 15/32:  92%|█████████▏| 48/52 [01:30<00:07,  1.93s/it][A
Epoch 15/32:  94%|█████████▍| 49/52 [01:32<00:05,  1.92s/it][A
Epoch 15/32:  94%|█████████▍| 49/52 [01:32<00:05,  1.92s/it][A
Epoch 15/32:  96%|█████████▌| 50/52 [01:34<00:03,  1.91s/it][A
Epoch 15/32:  96%|█████████▌| 50/52 [01:34<00:03,  1.91s/it][A

Epoch 15/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.90s/it][AEpoch 15/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.90s/it][A
Epoch 15/32: 100%|██████████| 52/52 [01:37<00:00,  1.78s/it][AEpoch 15/32: 100%|██████████| 52/52 [01:37<00:00,  1.88s/it]

Epoch 15/32: 100%|██████████| 52/52 [01:37<00:00,  1.78s/it][AEpoch 15/32: 100%|██████████| 52/52 [01:37<00:00,  1.88s/it]

[β=2.0] Epoch 15 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=7.882e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=4.925e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=6.869e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.096e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.096e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.860e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.747e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.470e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.041e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.926e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.611e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.634e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.084e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.485e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.128e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.903e-01
  layer model_5__forward_module.module.input_conv act-std: mean=9.425e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=1.912e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=3.811e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=7.258e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.283e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=6.015e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.002e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.419e-01
  layer model_4__forward_module.module.input_conv act-std: mean=9.226e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=1.828e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.572e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=9.624e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.126e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.428e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.789e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.589e-01
  layer model_3__forward_module.module.input_conv act-std: mean=8.377e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.021e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.049e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.257e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.707e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.634e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=4.025e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.888e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.092e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.379e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=4.792e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.083e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.395e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.067e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.093e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.840e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.115e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.505e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.320e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.663e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.194e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.823e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.159e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.605e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.450e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.829e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.452e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=9.816e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.918e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=9.002e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=5.852e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.764e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=2.260e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=5.913e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.434e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=5.960e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.795e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.784e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.907e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.868e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.991e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.769e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.019e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.723e-02

  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.966e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=7.203e-03Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.596e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.024e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.641e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.533e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.267e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=3.005e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=7.669e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.849e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=8.988e-02
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.242e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.234e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.845e-03
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.960e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.430e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.301e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.525e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.213e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.633e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=4.463e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.747e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.919e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.455e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.077e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.266e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=2.875e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=8.815e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.803e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.022e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=7.593e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.297e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.267e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.487e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.465e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.599e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.327e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.526e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.799e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.303e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.591e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.809e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=8.535e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.191e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.380e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=3.097e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.138e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=2.439e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.271e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=9.126e-02
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.716e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.880e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.079e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.447e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.175e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.807e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.277e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.145e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.701e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=2.000e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.150e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=7.158e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=1.870e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.587e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=3.617e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.435e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.121e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.167e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.705e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.075e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.060e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.026e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.455e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.184e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.943e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.754e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.761e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.806e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=2.765e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.258e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=7.870e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.615e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.060e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=3.256e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.010e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.090e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.010e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.481e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.454e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.235e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.077e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.821e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.654e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.568e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.554e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.169e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.692e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.658e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.064e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=8.398e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.607e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.630e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.860e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.696e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.693e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.999e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.814e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.998e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.721e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.895e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.527e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.362e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.478e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.674e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.383e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.241e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=4.588e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.109e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.160e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.030e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.153e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=3.086e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.190e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=7.272e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.576e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.093e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.205e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.975e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.230e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.959e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.551e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.831e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.544e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.518e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.997e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=4.969e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.371e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.196e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.351e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=1.986e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.70it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.68it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.69it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.67it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.70it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.68it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.70it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.69it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.70it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.69it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.70it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.69it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.70it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.69it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.70it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:02,  1.68it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.69it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.67it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.69it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:05<00:01,  1.67it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.69it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.67it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.69it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.68it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.68it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s]
Training epochs:  47%|████▋     | 15/32 [24:50<29:19, 103.52s/it]
Epoch 16/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.68it/s]
Epoch 15/32 - Train Loss: 17.821549 - Test Loss: 17.898148
Training epochs:  47%|████▋     | 15/32 [24:50<29:19, 103.53s/it]
Epoch 16/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 16/32:   2%|▏         | 1/52 [00:01<01:36,  1.90s/it][A
Epoch 16/32:   2%|▏         | 1/52 [00:01<01:34,  1.86s/it][A
Epoch 16/32:   4%|▍         | 2/52 [00:03<01:33,  1.88s/it][A
Epoch 16/32:   4%|▍         | 2/52 [00:03<01:32,  1.86s/it][A
Epoch 16/32:   6%|▌         | 3/52 [00:05<01:31,  1.86s/it][A
Epoch 16/32:   6%|▌         | 3/52 [00:05<01:32,  1.88s/it][A
Epoch 16/32:   8%|▊         | 4/52 [00:07<01:30,  1.88s/it][A
Epoch 16/32:   8%|▊         | 4/52 [00:07<01:30,  1.89s/it][A
Epoch 16/32:  10%|▉         | 5/52 [00:09<01:28,  1.88s/it][A
Epoch 16/32:  10%|▉         | 5/52 [00:09<01:28,  1.88s/it][A
Epoch 16/32:  12%|█▏        | 6/52 [00:11<01:26,  1.88s/it][A
Epoch 16/32:  12%|█▏        | 6/52 [00:11<01:26,  1.88s/it][A
Epoch 16/32:  13%|█▎        | 7/52 [00:13<01:24,  1.88s/it][A
Epoch 16/32:  13%|█▎        | 7/52 [00:13<01:24,  1.88s/it][A
Epoch 16/32:  15%|█▌        | 8/52 [00:15<01:22,  1.88s/it][A
Epoch 16/32:  15%|█▌        | 8/52 [00:14<01:22,  1.88s/it][A
Epoch 16/32:  17%|█▋        | 9/52 [00:16<01:20,  1.88s/it][A
Epoch 16/32:  17%|█▋        | 9/52 [00:16<01:20,  1.88s/it][A
Epoch 16/32:  19%|█▉        | 10/52 [00:18<01:21,  1.94s/it][A
Epoch 16/32:  19%|█▉        | 10/52 [00:18<01:21,  1.94s/it][A
Epoch 16/32:  21%|██        | 11/52 [00:20<01:19,  1.93s/it][A
Epoch 16/32:  21%|██        | 11/52 [00:20<01:19,  1.93s/it][A
Epoch 16/32:  23%|██▎       | 12/52 [00:22<01:16,  1.91s/it][A
Epoch 16/32:  23%|██▎       | 12/52 [00:22<01:16,  1.91s/it][A
Epoch 16/32:  25%|██▌       | 13/52 [00:24<01:14,  1.90s/it][A
Epoch 16/32:  25%|██▌       | 13/52 [00:24<01:14,  1.90s/it][A
Epoch 16/32:  27%|██▋       | 14/52 [00:26<01:11,  1.89s/it][A
Epoch 16/32:  27%|██▋       | 14/52 [00:26<01:11,  1.89s/it][A
Epoch 16/32:  29%|██▉       | 15/52 [00:28<01:09,  1.89s/it][A
Epoch 16/32:  29%|██▉       | 15/52 [00:28<01:09,  1.89s/it][A
Epoch 16/32:  31%|███       | 16/52 [00:30<01:07,  1.89s/it][A
Epoch 16/32:  31%|███       | 16/52 [00:30<01:07,  1.89s/it][A
Epoch 16/32:  33%|███▎      | 17/52 [00:32<01:06,  1.90s/it][A
Epoch 16/32:  33%|███▎      | 17/52 [00:32<01:06,  1.90s/it][A
Epoch 16/32:  35%|███▍      | 18/52 [00:34<01:04,  1.90s/it][A
Epoch 16/32:  35%|███▍      | 18/52 [00:34<01:04,  1.90s/it][A
Epoch 16/32:  37%|███▋      | 19/52 [00:36<01:02,  1.90s/it][A
Epoch 16/32:  37%|███▋      | 19/52 [00:35<01:02,  1.90s/it][A
Epoch 16/32:  38%|███▊      | 20/52 [00:37<01:00,  1.89s/it][A
Epoch 16/32:  38%|███▊      | 20/52 [00:37<01:00,  1.89s/it][A
Epoch 16/32:  40%|████      | 21/52 [00:39<00:58,  1.88s/it][A
Epoch 16/32:  40%|████      | 21/52 [00:39<00:58,  1.88s/it][A
Epoch 16/32:  42%|████▏     | 22/52 [00:41<00:56,  1.88s/it][A
Epoch 16/32:  42%|████▏     | 22/52 [00:41<00:56,  1.88s/it][A

Epoch 16/32:  44%|████▍     | 23/52 [00:43<00:54,  1.88s/it][AEpoch 16/32:  44%|████▍     | 23/52 [00:43<00:54,  1.88s/it][A
Epoch 16/32:  46%|████▌     | 24/52 [00:45<00:52,  1.88s/it][A
Epoch 16/32:  46%|████▌     | 24/52 [00:45<00:52,  1.88s/it][A
Epoch 16/32:  48%|████▊     | 25/52 [00:47<00:50,  1.89s/it][A
Epoch 16/32:  48%|████▊     | 25/52 [00:47<00:50,  1.89s/it][A
Epoch 16/32:  50%|█████     | 26/52 [00:49<00:48,  1.88s/it][A
Epoch 16/32:  50%|█████     | 26/52 [00:49<00:49,  1.89s/it][A
Epoch 16/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.88s/it][A
Epoch 16/32:  52%|█████▏    | 27/52 [00:50<00:47,  1.88s/it][A
Epoch 16/32:  54%|█████▍    | 28/52 [00:52<00:45,  1.88s/it][A
Epoch 16/32:  54%|█████▍    | 28/52 [00:52<00:45,  1.88s/it][A
Epoch 16/32:  56%|█████▌    | 29/52 [00:54<00:43,  1.89s/it][A
Epoch 16/32:  56%|█████▌    | 29/52 [00:54<00:43,  1.89s/it][A
Epoch 16/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.89s/it][A
Epoch 16/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.89s/it][A
Epoch 16/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.89s/it][A
Epoch 16/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.89s/it][A
Epoch 16/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 16/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 16/32:  63%|██████▎   | 33/52 [01:02<00:35,  1.89s/it][A
Epoch 16/32:  63%|██████▎   | 33/52 [01:02<00:35,  1.89s/it][A
Epoch 16/32:  65%|██████▌   | 34/52 [01:04<00:33,  1.88s/it][A
Epoch 16/32:  65%|██████▌   | 34/52 [01:04<00:33,  1.88s/it][A
Epoch 16/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.88s/it][A
Epoch 16/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.88s/it][A
Epoch 16/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.89s/it][A
Epoch 16/32:  69%|██████▉   | 36/52 [01:07<00:30,  1.89s/it][A
Epoch 16/32:  71%|███████   | 37/52 [01:09<00:28,  1.89s/it][A
Epoch 16/32:  71%|███████   | 37/52 [01:09<00:28,  1.89s/it][A
Epoch 16/32:  73%|███████▎  | 38/52 [01:11<00:26,  1.89s/it][A
Epoch 16/32:  73%|███████▎  | 38/52 [01:11<00:26,  1.89s/it][A
Epoch 16/32:  75%|███████▌  | 39/52 [01:13<00:24,  1.89s/it][A
Epoch 16/32:  75%|███████▌  | 39/52 [01:13<00:24,  1.89s/it][A
Epoch 16/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.89s/it][A
Epoch 16/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.89s/it][A
Epoch 16/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.89s/it][A
Epoch 16/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.89s/it][A
Epoch 16/32:  81%|████████  | 42/52 [01:19<00:18,  1.89s/it][A
Epoch 16/32:  81%|████████  | 42/52 [01:19<00:18,  1.89s/it][A
Epoch 16/32:  83%|████████▎ | 43/52 [01:21<00:16,  1.89s/it][A
Epoch 16/32:  83%|████████▎ | 43/52 [01:21<00:16,  1.89s/it][A
Epoch 16/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.88s/it][A
Epoch 16/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.88s/it][A
Epoch 16/32:  87%|████████▋ | 45/52 [01:24<00:13,  1.87s/it][A
Epoch 16/32:  87%|████████▋ | 45/52 [01:24<00:13,  1.87s/it][A
Epoch 16/32:  88%|████████▊ | 46/52 [01:26<00:11,  1.88s/it][A
Epoch 16/32:  88%|████████▊ | 46/52 [01:26<00:11,  1.88s/it][A
Epoch 16/32:  90%|█████████ | 47/52 [01:28<00:09,  1.88s/it][A
Epoch 16/32:  90%|█████████ | 47/52 [01:28<00:09,  1.88s/it][A
Epoch 16/32:  92%|█████████▏| 48/52 [01:30<00:07,  1.88s/it][A
Epoch 16/32:  92%|█████████▏| 48/52 [01:30<00:07,  1.88s/it][A
Epoch 16/32:  94%|█████████▍| 49/52 [01:32<00:05,  1.88s/it][A
Epoch 16/32:  94%|█████████▍| 49/52 [01:32<00:05,  1.88s/it][A
Epoch 16/32:  96%|█████████▌| 50/52 [01:34<00:03,  1.89s/it][A
Epoch 16/32:  96%|█████████▌| 50/52 [01:34<00:03,  1.88s/it][A
Epoch 16/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.89s/it][A
Epoch 16/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.88s/it][A
Epoch 16/32: 100%|██████████| 52/52 [01:37<00:00,  1.78s/it][AEpoch 16/32: 100%|██████████| 52/52 [01:37<00:00,  1.88s/it]

Epoch 16/32: 100%|██████████| 52/52 [01:37<00:00,  1.78s/it][AEpoch 16/32: 100%|██████████| 52/52 [01:37<00:00,  1.88s/it]

[β=2.0] Epoch 16 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=8.323e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.012e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=7.183e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.090e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.072e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.874e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.706e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.444e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.086e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.979e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.714e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.630e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.074e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.542e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.226e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.898e-01
  layer model_5__forward_module.module.input_conv act-std: mean=9.828e-02
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=2.056e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=3.840e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=6.377e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.244e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.757e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.978e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.373e-01
  layer model_4__forward_module.module.input_conv act-std: mean=9.836e-02
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=1.863e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.493e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=8.931e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.155e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.250e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.669e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.548e-01
  layer model_3__forward_module.module.input_conv act-std: mean=8.949e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=2.455e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=3.151e-01

  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.225e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.668e+00Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.438e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.995e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.838e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.152e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.384e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=4.763e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.058e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.348e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.079e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.368e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.744e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.193e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.326e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.124e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.643e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.130e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.542e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.152e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.551e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.497e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.828e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.423e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=9.218e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.734e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.805e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=6.033e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.626e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=2.637e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=8.923e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.282e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=8.895e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.404e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.740e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.337e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.606e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.400e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.813e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.669e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.571e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.384e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=9.283e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=3.522e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.628e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=1.000e-02
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=3.391e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.754e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=3.181e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=8.330e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.026e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.075e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.444e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.383e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=8.705e-03
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.319e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=6.040e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.689e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.932e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.669e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.016e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=4.901e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.036e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=2.165e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=8.645e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.659e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.673e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=2.727e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.462e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.977e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.166e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.380e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.368e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.380e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.623e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.699e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.442e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.290e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.783e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.041e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.011e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.503e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.053e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=9.475e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.414e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.678e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.813e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.221e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=2.714e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.446e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=9.903e-02
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.259e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.190e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.891e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.210e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.987e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.748e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.408e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.257e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.325e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.916e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.229e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=7.877e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.131e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.865e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=3.439e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.392e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.070e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.402e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.724e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.164e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.149e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.924e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.256e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.974e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.839e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.950e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.702e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.778e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=2.988e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.377e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=8.918e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.748e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.109e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=3.145e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.086e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.236e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.147e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.600e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.416e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.149e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.720e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.701e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.313e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.724e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.551e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.408e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.710e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=1.961e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.052e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=8.720e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.604e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.886e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.932e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.198e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.861e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.687e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.505e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.730e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.274e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.241e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.846e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.986e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.824e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.678e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.616e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.732e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=5.912e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.231e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.277e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.144e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.448e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=2.679e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.623e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=5.807e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.188e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=1.752e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.748e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.176e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.369e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.859e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.755e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.872e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.132e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.473e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=1.713e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=4.429e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.436e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.248e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.492e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.118e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.67it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.64it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.67it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.64it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.66it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.63it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.66it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.64it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.67it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.64it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.67it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.65it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.67it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.66it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.67it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.65it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.65it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.64it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.65it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.60it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.63it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.61it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.65it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.62it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s]
Training epochs:  50%|█████     | 16/32 [26:36<27:46, 104.17s/it]
Epoch 17/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]
Epoch 16/32 - Train Loss: 17.725860 - Test Loss: 17.787082
Training epochs:  50%|█████     | 16/32 [26:36<27:47, 104.20s/it]
Epoch 17/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 17/32:   2%|▏         | 1/52 [00:02<01:42,  2.02s/it][A
Epoch 17/32:   2%|▏         | 1/52 [00:01<01:36,  1.89s/it][A
Epoch 17/32:   4%|▍         | 2/52 [00:03<01:36,  1.94s/it][A
Epoch 17/32:   4%|▍         | 2/52 [00:03<01:34,  1.88s/it][A
Epoch 17/32:   6%|▌         | 3/52 [00:05<01:34,  1.93s/it][A
Epoch 17/32:   6%|▌         | 3/52 [00:05<01:32,  1.90s/it][A
Epoch 17/32:   8%|▊         | 4/52 [00:07<01:32,  1.92s/it][A
Epoch 17/32:   8%|▊         | 4/52 [00:07<01:31,  1.91s/it][A
Epoch 17/32:  10%|▉         | 5/52 [00:09<01:30,  1.92s/it][A
Epoch 17/32:  10%|▉         | 5/52 [00:09<01:29,  1.90s/it][A
Epoch 17/32:  12%|█▏        | 6/52 [00:11<01:27,  1.91s/it][A
Epoch 17/32:  12%|█▏        | 6/52 [00:11<01:27,  1.90s/it][A
Epoch 17/32:  13%|█▎        | 7/52 [00:13<01:25,  1.90s/it][A
Epoch 17/32:  13%|█▎        | 7/52 [00:13<01:25,  1.89s/it][A
Epoch 17/32:  15%|█▌        | 8/52 [00:15<01:22,  1.88s/it][A
Epoch 17/32:  15%|█▌        | 8/52 [00:15<01:22,  1.88s/it][A
Epoch 17/32:  17%|█▋        | 9/52 [00:17<01:21,  1.88s/it][A
Epoch 17/32:  17%|█▋        | 9/52 [00:17<01:20,  1.88s/it][A
Epoch 17/32:  19%|█▉        | 10/52 [00:19<01:19,  1.88s/it][A
Epoch 17/32:  19%|█▉        | 10/52 [00:18<01:18,  1.88s/it][A
Epoch 17/32:  21%|██        | 11/52 [00:20<01:17,  1.89s/it][A
Epoch 17/32:  21%|██        | 11/52 [00:20<01:17,  1.88s/it][A
Epoch 17/32:  23%|██▎       | 12/52 [00:22<01:15,  1.89s/it][A
Epoch 17/32:  23%|██▎       | 12/52 [00:22<01:15,  1.89s/it][A
Epoch 17/32:  25%|██▌       | 13/52 [00:24<01:13,  1.90s/it][A
Epoch 17/32:  25%|██▌       | 13/52 [00:24<01:13,  1.89s/it][A
Epoch 17/32:  27%|██▋       | 14/52 [00:26<01:11,  1.89s/it][A
Epoch 17/32:  27%|██▋       | 14/52 [00:26<01:11,  1.89s/it][A
Epoch 17/32:  29%|██▉       | 15/52 [00:28<01:09,  1.89s/it][A
Epoch 17/32:  29%|██▉       | 15/52 [00:28<01:09,  1.89s/it][A
Epoch 17/32:  31%|███       | 16/52 [00:30<01:07,  1.88s/it][A
Epoch 17/32:  31%|███       | 16/52 [00:30<01:07,  1.89s/it][A
Epoch 17/32:  33%|███▎      | 17/52 [00:32<01:06,  1.90s/it][A
Epoch 17/32:  33%|███▎      | 17/52 [00:32<01:06,  1.91s/it][A
Epoch 17/32:  35%|███▍      | 18/52 [00:34<01:04,  1.91s/it][A
Epoch 17/32:  35%|███▍      | 18/52 [00:34<01:04,  1.91s/it][A
Epoch 17/32:  37%|███▋      | 19/52 [00:35<01:02,  1.91s/it][A
Epoch 17/32:  37%|███▋      | 19/52 [00:36<01:02,  1.91s/it][A
Epoch 17/32:  38%|███▊      | 20/52 [00:37<01:00,  1.89s/it][A
Epoch 17/32:  38%|███▊      | 20/52 [00:38<01:00,  1.89s/it][A
Epoch 17/32:  40%|████      | 21/52 [00:39<00:58,  1.89s/it][A
Epoch 17/32:  40%|████      | 21/52 [00:39<00:58,  1.89s/it][A
Epoch 17/32:  42%|████▏     | 22/52 [00:41<00:56,  1.88s/it][A
Epoch 17/32:  42%|████▏     | 22/52 [00:41<00:56,  1.88s/it][A
Epoch 17/32:  44%|████▍     | 23/52 [00:43<00:54,  1.88s/it][A
Epoch 17/32:  44%|████▍     | 23/52 [00:43<00:54,  1.88s/it][A
Epoch 17/32:  46%|████▌     | 24/52 [00:45<00:52,  1.89s/it][A
Epoch 17/32:  46%|████▌     | 24/52 [00:45<00:52,  1.89s/it][A
Epoch 17/32:  48%|████▊     | 25/52 [00:47<00:51,  1.89s/it][A
Epoch 17/32:  48%|████▊     | 25/52 [00:47<00:51,  1.89s/it][A
Epoch 17/32:  50%|█████     | 26/52 [00:49<00:49,  1.90s/it][A
Epoch 17/32:  50%|█████     | 26/52 [00:49<00:49,  1.90s/it][A
Epoch 17/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.90s/it][A
Epoch 17/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.90s/it][A
Epoch 17/32:  54%|█████▍    | 28/52 [00:52<00:45,  1.89s/it][A
Epoch 17/32:  54%|█████▍    | 28/52 [00:53<00:45,  1.89s/it][A
Epoch 17/32:  56%|█████▌    | 29/52 [00:54<00:43,  1.89s/it][A
Epoch 17/32:  56%|█████▌    | 29/52 [00:54<00:43,  1.89s/it][A
Epoch 17/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.90s/it][A
Epoch 17/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.90s/it][A
Epoch 17/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.89s/it][A
Epoch 17/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.89s/it][A
Epoch 17/32:  62%|██████▏   | 32/52 [01:00<00:38,  1.93s/it][A
Epoch 17/32:  62%|██████▏   | 32/52 [01:00<00:38,  1.93s/it][A
Epoch 17/32:  63%|██████▎   | 33/52 [01:02<00:36,  1.92s/it][A
Epoch 17/32:  63%|██████▎   | 33/52 [01:02<00:36,  1.92s/it][A
Epoch 17/32:  65%|██████▌   | 34/52 [01:04<00:34,  1.91s/it][A
Epoch 17/32:  65%|██████▌   | 34/52 [01:04<00:34,  1.91s/it][A
Epoch 17/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.90s/it][A
Epoch 17/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.90s/it][A
Epoch 17/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.90s/it][A
Epoch 17/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.90s/it][A
Epoch 17/32:  71%|███████   | 37/52 [01:10<00:28,  1.90s/it][A
Epoch 17/32:  71%|███████   | 37/52 [01:10<00:28,  1.90s/it][A
Epoch 17/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.90s/it][A
Epoch 17/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.90s/it][A
Epoch 17/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.90s/it][A
Epoch 17/32:  75%|███████▌  | 39/52 [01:13<00:24,  1.90s/it][A
Epoch 17/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.89s/it][A
Epoch 17/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.90s/it][A
Epoch 17/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.90s/it][A
Epoch 17/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.90s/it][A
Epoch 17/32:  81%|████████  | 42/52 [01:19<00:18,  1.90s/it][A
Epoch 17/32:  81%|████████  | 42/52 [01:19<00:18,  1.90s/it][A
Epoch 17/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.90s/it][A
Epoch 17/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.90s/it][A
Epoch 17/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.90s/it][A
Epoch 17/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.90s/it][A
Epoch 17/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.89s/it][A
Epoch 17/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.89s/it][A

Epoch 17/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.89s/it][AEpoch 17/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.89s/it][A
Epoch 17/32:  90%|█████████ | 47/52 [01:29<00:09,  1.89s/it][A
Epoch 17/32:  90%|█████████ | 47/52 [01:29<00:09,  1.89s/it][A
Epoch 17/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.89s/it][A
Epoch 17/32:  92%|█████████▏| 48/52 [01:30<00:07,  1.89s/it][A
Epoch 17/32:  94%|█████████▍| 49/52 [01:32<00:05,  1.88s/it][A
Epoch 17/32:  94%|█████████▍| 49/52 [01:32<00:05,  1.88s/it][A
Epoch 17/32:  96%|█████████▌| 50/52 [01:34<00:03,  1.88s/it][A
Epoch 17/32:  96%|█████████▌| 50/52 [01:34<00:03,  1.88s/it][A
Epoch 17/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.89s/it][A
Epoch 17/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.89s/it][A
Epoch 17/32: 100%|██████████| 52/52 [01:38<00:00,  1.78s/it][AEpoch 17/32: 100%|██████████| 52/52 [01:38<00:00,  1.89s/it]

Epoch 17/32: 100%|██████████| 52/52 [01:38<00:00,  1.78s/it][AEpoch 17/32: 100%|██████████| 52/52 [01:38<00:00,  1.89s/it]

[β=2.0] Epoch 17 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=8.706e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.136e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=7.430e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.093e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.055e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.922e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.693e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.422e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.135e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.999e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.723e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.644e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.055e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.593e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.328e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.912e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.034e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=2.324e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=3.954e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=5.638e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.195e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.520e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.932e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.324e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.033e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.013e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.450e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=8.160e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.166e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=5.078e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.546e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.503e-01
  layer model_3__forward_module.module.input_conv act-std: mean=9.455e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=3.460e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=4.035e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.210e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.647e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.451e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=4.004e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.822e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.223e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.432e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=4.791e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.023e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.300e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.101e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.678e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.664e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.273e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.156e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.987e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.608e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.072e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.286e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.184e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.498e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.537e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.886e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.491e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=9.061e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.612e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.719e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=6.292e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.524e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=2.531e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=6.697e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.623e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=6.644e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.032e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.895e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.098e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.434e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.659e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.318e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.492e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.270e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.335e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=7.613e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=3.036e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.393e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=9.223e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.646e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.485e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=3.386e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=9.599e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.343e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.143e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.742e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.560e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.075e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.761e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=7.252e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.189e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.647e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.537e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.382e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=5.810e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.552e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=2.527e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=1.035e-02
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.547e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.803e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=2.497e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.059e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.843e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.087e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=8.085e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.533e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.527e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.576e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.450e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.311e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.229e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.684e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.917e-03

  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.293e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.160e-03Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.956e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=8.963e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.416e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.659e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.536e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.359e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.033e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.477e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.074e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.780e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.199e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.648e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.578e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.026e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.721e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.412e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.305e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.168e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.898e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.195e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=7.681e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.025e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.841e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=3.404e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.474e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.335e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.481e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.897e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.586e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.528e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.935e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.284e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.132e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.937e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.276e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.909e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.989e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=3.417e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.363e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=9.192e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.928e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.355e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=3.039e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.229e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.661e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.435e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.936e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.516e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.231e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.000e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.860e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.582e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.094e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.078e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.840e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.777e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.290e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.218e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.050e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.834e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.318e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.626e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.126e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.746e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.729e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.721e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.068e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.649e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.234e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.910e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.651e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.811e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.063e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.607e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.354e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=5.293e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.111e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.153e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.044e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.681e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=2.574e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.343e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=7.610e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.656e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.330e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.724e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.336e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.155e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.167e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.435e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.368e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.837e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.879e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.181e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=5.863e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.522e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.309e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.438e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.728e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.66it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.67it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.66it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.65it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.66it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.64it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.67it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.64it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.67it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.64it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.67it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.64it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.66it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.62it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.65it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.63it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.65it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.64it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.65it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.64it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.65it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.63it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.65it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.51it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s]
Training epochs:  53%|█████▎    | 17/32 [28:22<26:11, 104.78s/it]
Epoch 18/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]
Epoch 17/32 - Train Loss: 17.619182 - Test Loss: 17.689108
Training epochs:  53%|█████▎    | 17/32 [28:22<26:12, 104.84s/it]
Epoch 18/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 18/32:   2%|▏         | 1/52 [00:02<01:50,  2.16s/it][A
Epoch 18/32:   2%|▏         | 1/52 [00:01<01:36,  1.88s/it][A
Epoch 18/32:   4%|▍         | 2/52 [00:04<01:39,  2.00s/it][A
Epoch 18/32:   4%|▍         | 2/52 [00:03<01:34,  1.88s/it][A
Epoch 18/32:   6%|▌         | 3/52 [00:05<01:35,  1.95s/it][A
Epoch 18/32:   6%|▌         | 3/52 [00:05<01:32,  1.89s/it][A
Epoch 18/32:   8%|▊         | 4/52 [00:07<01:30,  1.88s/it][A
Epoch 18/32:   8%|▊         | 4/52 [00:07<01:32,  1.92s/it][A
Epoch 18/32:  10%|▉         | 5/52 [00:09<01:28,  1.88s/it][A
Epoch 18/32:  10%|▉         | 5/52 [00:09<01:29,  1.90s/it][A
Epoch 18/32:  12%|█▏        | 6/52 [00:11<01:26,  1.88s/it][A
Epoch 18/32:  12%|█▏        | 6/52 [00:11<01:27,  1.90s/it][A
Epoch 18/32:  13%|█▎        | 7/52 [00:13<01:24,  1.88s/it][A
Epoch 18/32:  13%|█▎        | 7/52 [00:13<01:25,  1.90s/it][A
Epoch 18/32:  15%|█▌        | 8/52 [00:15<01:23,  1.89s/it][A
Epoch 18/32:  15%|█▌        | 8/52 [00:15<01:22,  1.88s/it][A
Epoch 18/32:  17%|█▋        | 9/52 [00:17<01:21,  1.88s/it][A
Epoch 18/32:  17%|█▋        | 9/52 [00:16<01:20,  1.88s/it][A
Epoch 18/32:  19%|█▉        | 10/52 [00:19<01:19,  1.88s/it][A
Epoch 18/32:  19%|█▉        | 10/52 [00:18<01:18,  1.88s/it][A
Epoch 18/32:  21%|██        | 11/52 [00:20<01:17,  1.88s/it][A
Epoch 18/32:  21%|██        | 11/52 [00:20<01:17,  1.88s/it][A
Epoch 18/32:  23%|██▎       | 12/52 [00:22<01:15,  1.89s/it][A
Epoch 18/32:  23%|██▎       | 12/52 [00:22<01:15,  1.89s/it][A
Epoch 18/32:  25%|██▌       | 13/52 [00:24<01:13,  1.89s/it][A
Epoch 18/32:  25%|██▌       | 13/52 [00:24<01:13,  1.89s/it][A
Epoch 18/32:  27%|██▋       | 14/52 [00:26<01:11,  1.89s/it][A
Epoch 18/32:  27%|██▋       | 14/52 [00:26<01:11,  1.89s/it][A
Epoch 18/32:  29%|██▉       | 15/52 [00:28<01:10,  1.89s/it][A
Epoch 18/32:  29%|██▉       | 15/52 [00:28<01:10,  1.89s/it][A
Epoch 18/32:  31%|███       | 16/52 [00:30<01:08,  1.89s/it][A
Epoch 18/32:  31%|███       | 16/52 [00:30<01:08,  1.89s/it][A
Epoch 18/32:  33%|███▎      | 17/52 [00:32<01:06,  1.90s/it][A
Epoch 18/32:  33%|███▎      | 17/52 [00:32<01:06,  1.90s/it][A
Epoch 18/32:  35%|███▍      | 18/52 [00:34<01:04,  1.89s/it][A
Epoch 18/32:  35%|███▍      | 18/52 [00:33<01:04,  1.89s/it][A
Epoch 18/32:  37%|███▋      | 19/52 [00:36<01:02,  1.89s/it][A
Epoch 18/32:  37%|███▋      | 19/52 [00:35<01:02,  1.89s/it][A
Epoch 18/32:  38%|███▊      | 20/52 [00:38<01:01,  1.91s/it][A
Epoch 18/32:  38%|███▊      | 20/52 [00:37<01:01,  1.91s/it][A
Epoch 18/32:  40%|████      | 21/52 [00:39<00:58,  1.90s/it][A
Epoch 18/32:  40%|████      | 21/52 [00:39<00:58,  1.89s/it][A
Epoch 18/32:  42%|████▏     | 22/52 [00:41<00:56,  1.89s/it][A
Epoch 18/32:  42%|████▏     | 22/52 [00:41<00:56,  1.89s/it][A
Epoch 18/32:  44%|████▍     | 23/52 [00:43<00:54,  1.89s/it][A
Epoch 18/32:  44%|████▍     | 23/52 [00:43<00:54,  1.89s/it][A
Epoch 18/32:  46%|████▌     | 24/52 [00:45<00:53,  1.90s/it][A
Epoch 18/32:  46%|████▌     | 24/52 [00:45<00:53,  1.90s/it][A
Epoch 18/32:  48%|████▊     | 25/52 [00:47<00:51,  1.89s/it][A
Epoch 18/32:  48%|████▊     | 25/52 [00:47<00:51,  1.90s/it][A
Epoch 18/32:  50%|█████     | 26/52 [00:49<00:49,  1.89s/it][A
Epoch 18/32:  50%|█████     | 26/52 [00:49<00:49,  1.89s/it][A
Epoch 18/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.89s/it][A
Epoch 18/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.89s/it][A
Epoch 18/32:  54%|█████▍    | 28/52 [00:52<00:45,  1.89s/it][A
Epoch 18/32:  54%|█████▍    | 28/52 [00:53<00:45,  1.89s/it][A
Epoch 18/32:  56%|█████▌    | 29/52 [00:54<00:43,  1.89s/it][A
Epoch 18/32:  56%|█████▌    | 29/52 [00:55<00:43,  1.89s/it][A
Epoch 18/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.88s/it][A
Epoch 18/32:  58%|█████▊    | 30/52 [00:56<00:41,  1.88s/it][A
Epoch 18/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.88s/it][A
Epoch 18/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.88s/it][A
Epoch 18/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 18/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 18/32:  63%|██████▎   | 33/52 [01:02<00:36,  1.90s/it][A
Epoch 18/32:  63%|██████▎   | 33/52 [01:02<00:36,  1.90s/it][A
Epoch 18/32:  65%|██████▌   | 34/52 [01:04<00:34,  1.90s/it][A
Epoch 18/32:  65%|██████▌   | 34/52 [01:04<00:34,  1.90s/it][A
Epoch 18/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.90s/it][A
Epoch 18/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.90s/it][A
Epoch 18/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.89s/it][A
Epoch 18/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.90s/it][A
Epoch 18/32:  71%|███████   | 37/52 [01:10<00:28,  1.89s/it][A
Epoch 18/32:  71%|███████   | 37/52 [01:09<00:28,  1.90s/it][A
Epoch 18/32:  73%|███████▎  | 38/52 [01:11<00:26,  1.89s/it][A
Epoch 18/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.89s/it][A
Epoch 18/32:  75%|███████▌  | 39/52 [01:13<00:24,  1.90s/it][A
Epoch 18/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.90s/it][A
Epoch 18/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.90s/it][A
Epoch 18/32:  77%|███████▋  | 40/52 [01:15<00:22,  1.90s/it][A
Epoch 18/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.89s/it][A
Epoch 18/32:  79%|███████▉  | 41/52 [01:17<00:20,  1.90s/it][A
Epoch 18/32:  81%|████████  | 42/52 [01:19<00:19,  1.90s/it][A
Epoch 18/32:  81%|████████  | 42/52 [01:19<00:19,  1.90s/it][A
Epoch 18/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.90s/it][A
Epoch 18/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.90s/it][A
Epoch 18/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.90s/it][A
Epoch 18/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.90s/it][A
Epoch 18/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.91s/it][A
Epoch 18/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.91s/it][A
Epoch 18/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.91s/it][A
Epoch 18/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.91s/it][A
Epoch 18/32:  90%|█████████ | 47/52 [01:29<00:09,  1.91s/it][A
Epoch 18/32:  90%|█████████ | 47/52 [01:29<00:09,  1.91s/it][A
Epoch 18/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.91s/it][A
Epoch 18/32:  92%|█████████▏| 48/52 [01:30<00:07,  1.91s/it][A
Epoch 18/32:  94%|█████████▍| 49/52 [01:33<00:05,  1.91s/it][A
Epoch 18/32:  94%|█████████▍| 49/52 [01:32<00:05,  1.91s/it][A
Epoch 18/32:  96%|█████████▌| 50/52 [01:35<00:03,  1.92s/it][A
Epoch 18/32:  96%|█████████▌| 50/52 [01:34<00:03,  1.92s/it][A
Epoch 18/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.91s/it][A
Epoch 18/32:  98%|█████████▊| 51/52 [01:36<00:01,  1.91s/it][A
Epoch 18/32: 100%|██████████| 52/52 [01:38<00:00,  1.80s/it][AEpoch 18/32: 100%|██████████| 52/52 [01:38<00:00,  1.89s/it]

[β=2.0] Epoch 18 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=9.227e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.121e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=7.490e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.971e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.004e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=3.923e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.673e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.345e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.147e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=6.106e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.940e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.644e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.037e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.684e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.448e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.902e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.076e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=2.661e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.198e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=5.191e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.182e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.451e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.946e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.302e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.076e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.371e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.511e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=7.634e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.154e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.912e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.415e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.469e-01
  layer model_3__forward_module.module.input_conv act-std: mean=9.785e-02
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=4.260e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=5.100e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.168e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.646e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.522e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=4.009e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.818e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.283e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.508e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=4.937e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=1.001e+00
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.276e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.130e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.956e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.620e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.351e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.007e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.914e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.613e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=2.046e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.190e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.333e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.469e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.562e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.919e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.443e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.944e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.543e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.637e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=6.584e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.405e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.793e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=6.631e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.589e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=6.679e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.828e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.350e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.618e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.208e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.486e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.875e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.636e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.264e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.691e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=4.894e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.069e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.587e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.186e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.354e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.247e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.428e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=5.852e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.328e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=8.645e-02
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.024e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.076e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=7.474e-03
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.396e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.882e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.276e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.067e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.624e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.207e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.958e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.371e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.393e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=5.800e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.049e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.320e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=2.334e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=8.728e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.770e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.168e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=7.316e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.512e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.108e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.559e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.239e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.222e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.066e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.556e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.688e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.940e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.520e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.000e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=8.762e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.260e-01

  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.519e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.347e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.735e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.029e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.008e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.327e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.555e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.361e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.826e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.623e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.069e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.659e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.353e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.254e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.086e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.844e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.372e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=8.622e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.443e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.772e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.819e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.248e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.687e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.083e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.442e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.290e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.764e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.772e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.279e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.606e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.447e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.053e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.555e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.453e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=2.958e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.176e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=8.171e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.823e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=1.949e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.595e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.004e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=1.920e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.702e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.010e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.550e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.067e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.745e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.498e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=1.960e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.087e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.627e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.101e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.714e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.353e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=9.380e-03
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=8.335e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.721e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.713e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.293e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.598e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.260e-01
Epoch 18/32: 100%|██████████| 52/52 [01:38<00:00,  1.80s/it]  param model_6__forward_module.module.input_norm.weight grad-norm: mean=1.921e-01[A
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.562e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.231e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.829e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.417e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.730e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.668e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.316e-02Epoch 18/32: 100%|██████████| 52/52 [01:38<00:00,  1.89s/it]

  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.823e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.472e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.913e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=4.454e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.201e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.207e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.174e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.305e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=2.986e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=4.514e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=1.047e+00
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=3.471e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=3.063e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.139e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.371e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.761e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.214e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.084e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.556e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.878e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.748e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=3.284e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=9.045e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.639e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.389e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.645e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.030e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.64it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.62it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.64it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.62it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.63it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.62it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.64it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.63it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.65it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.63it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.63it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.61it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.62it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.62it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.61it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.60it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.62it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.61it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.62it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.61it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.62it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.59it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.62it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.60it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]
Training epochs:  56%|█████▋    | 18/32 [30:08<24:34, 105.30s/it]
Epoch 19/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]
Epoch 18/32 - Train Loss: 17.538975 - Test Loss: 17.604139
Training epochs:  56%|█████▋    | 18/32 [30:09<24:33, 105.28s/it]
Epoch 19/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 19/32:   2%|▏         | 1/52 [00:01<01:41,  1.99s/it][A
Epoch 19/32:   2%|▏         | 1/52 [00:01<01:37,  1.92s/it][A
Epoch 19/32:   4%|▍         | 2/52 [00:03<01:37,  1.95s/it][A
Epoch 19/32:   4%|▍         | 2/52 [00:03<01:35,  1.91s/it][A
Epoch 19/32:   6%|▌         | 3/52 [00:05<01:34,  1.93s/it][A
Epoch 19/32:   6%|▌         | 3/52 [00:05<01:33,  1.91s/it][A
Epoch 19/32:   8%|▊         | 4/52 [00:07<01:32,  1.92s/it][A
Epoch 19/32:   8%|▊         | 4/52 [00:07<01:31,  1.91s/it][A
Epoch 19/32:  10%|▉         | 5/52 [00:09<01:30,  1.92s/it][A
Epoch 19/32:  10%|▉         | 5/52 [00:09<01:29,  1.91s/it][A
Epoch 19/32:  12%|█▏        | 6/52 [00:11<01:28,  1.92s/it][A
Epoch 19/32:  12%|█▏        | 6/52 [00:11<01:27,  1.91s/it][A
Epoch 19/32:  13%|█▎        | 7/52 [00:13<01:26,  1.91s/it][A
Epoch 19/32:  13%|█▎        | 7/52 [00:13<01:25,  1.91s/it][A
Epoch 19/32:  15%|█▌        | 8/52 [00:15<01:23,  1.90s/it][A
Epoch 19/32:  15%|█▌        | 8/52 [00:15<01:23,  1.91s/it][A
Epoch 19/32:  17%|█▋        | 9/52 [00:17<01:21,  1.90s/it][A
Epoch 19/32:  17%|█▋        | 9/52 [00:17<01:21,  1.90s/it][A
Epoch 19/32:  19%|█▉        | 10/52 [00:19<01:19,  1.89s/it][A
Epoch 19/32:  19%|█▉        | 10/52 [00:19<01:19,  1.90s/it][A
Epoch 19/32:  21%|██        | 11/52 [00:20<01:17,  1.89s/it][A
Epoch 19/32:  21%|██        | 11/52 [00:20<01:17,  1.89s/it][A
Epoch 19/32:  23%|██▎       | 12/52 [00:22<01:16,  1.91s/it][A
Epoch 19/32:  23%|██▎       | 12/52 [00:22<01:16,  1.91s/it][A
Epoch 19/32:  25%|██▌       | 13/52 [00:24<01:14,  1.92s/it][A
Epoch 19/32:  25%|██▌       | 13/52 [00:24<01:14,  1.92s/it][A
Epoch 19/32:  27%|██▋       | 14/52 [00:26<01:13,  1.93s/it][A
Epoch 19/32:  27%|██▋       | 14/52 [00:26<01:13,  1.93s/it][A
Epoch 19/32:  29%|██▉       | 15/52 [00:28<01:10,  1.92s/it][A
Epoch 19/32:  29%|██▉       | 15/52 [00:28<01:10,  1.92s/it][A
Epoch 19/32:  31%|███       | 16/52 [00:30<01:08,  1.90s/it][A
Epoch 19/32:  31%|███       | 16/52 [00:30<01:08,  1.90s/it][A
Epoch 19/32:  33%|███▎      | 17/52 [00:32<01:06,  1.89s/it][A
Epoch 19/32:  33%|███▎      | 17/52 [00:32<01:06,  1.89s/it][A
Epoch 19/32:  35%|███▍      | 18/52 [00:34<01:04,  1.89s/it][A
Epoch 19/32:  35%|███▍      | 18/52 [00:34<01:04,  1.89s/it][A
Epoch 19/32:  37%|███▋      | 19/52 [00:36<01:02,  1.90s/it][A
Epoch 19/32:  37%|███▋      | 19/52 [00:36<01:02,  1.90s/it][A
Epoch 19/32:  38%|███▊      | 20/52 [00:38<01:00,  1.90s/it][A
Epoch 19/32:  38%|███▊      | 20/52 [00:38<01:00,  1.90s/it][A
Epoch 19/32:  40%|████      | 21/52 [00:40<00:58,  1.90s/it][A
Epoch 19/32:  40%|████      | 21/52 [00:39<00:58,  1.90s/it][A
Epoch 19/32:  42%|████▏     | 22/52 [00:42<00:58,  1.94s/it][A
Epoch 19/32:  42%|████▏     | 22/52 [00:42<00:58,  1.94s/it][A
Epoch 19/32:  44%|████▍     | 23/52 [00:43<00:55,  1.91s/it][A
Epoch 19/32:  44%|████▍     | 23/52 [00:43<00:55,  1.92s/it][A
Epoch 19/32:  46%|████▌     | 24/52 [00:45<00:53,  1.90s/it][A
Epoch 19/32:  46%|████▌     | 24/52 [00:45<00:53,  1.90s/it][A
Epoch 19/32:  48%|████▊     | 25/52 [00:47<00:51,  1.90s/it][A
Epoch 19/32:  48%|████▊     | 25/52 [00:47<00:51,  1.90s/it][A
Epoch 19/32:  50%|█████     | 26/52 [00:49<00:49,  1.89s/it][A
Epoch 19/32:  50%|█████     | 26/52 [00:49<00:49,  1.89s/it][A
Epoch 19/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.89s/it][A
Epoch 19/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.89s/it][A
Epoch 19/32:  54%|█████▍    | 28/52 [00:53<00:45,  1.88s/it][A
Epoch 19/32:  54%|█████▍    | 28/52 [00:53<00:45,  1.88s/it][A
Epoch 19/32:  56%|█████▌    | 29/52 [00:55<00:43,  1.88s/it][A
Epoch 19/32:  56%|█████▌    | 29/52 [00:55<00:43,  1.88s/it][A
Epoch 19/32:  58%|█████▊    | 30/52 [00:57<00:41,  1.87s/it][A
Epoch 19/32:  58%|█████▊    | 30/52 [00:57<00:41,  1.87s/it][A
Epoch 19/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.88s/it][A
Epoch 19/32:  60%|█████▉    | 31/52 [00:58<00:39,  1.88s/it][A
Epoch 19/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 19/32:  62%|██████▏   | 32/52 [01:00<00:37,  1.88s/it][A
Epoch 19/32:  63%|██████▎   | 33/52 [01:02<00:35,  1.89s/it][A
Epoch 19/32:  63%|██████▎   | 33/52 [01:02<00:35,  1.89s/it][A
Epoch 19/32:  65%|██████▌   | 34/52 [01:04<00:33,  1.89s/it][A
Epoch 19/32:  65%|██████▌   | 34/52 [01:04<00:33,  1.89s/it][A
Epoch 19/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.89s/it][A
Epoch 19/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.89s/it][A
Epoch 19/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.89s/it][A
Epoch 19/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.89s/it][A
Epoch 19/32:  71%|███████   | 37/52 [01:10<00:29,  1.94s/it][A
Epoch 19/32:  71%|███████   | 37/52 [01:10<00:29,  1.94s/it][A
Epoch 19/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.93s/it][A
Epoch 19/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.93s/it][A
Epoch 19/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.92s/it][A
Epoch 19/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.92s/it][A
Epoch 19/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.91s/it][A
Epoch 19/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.91s/it][A
Epoch 19/32:  79%|███████▉  | 41/52 [01:18<00:20,  1.91s/it][A
Epoch 19/32:  79%|███████▉  | 41/52 [01:18<00:21,  1.91s/it][A
Epoch 19/32:  81%|████████  | 42/52 [01:19<00:19,  1.91s/it][A
Epoch 19/32:  81%|████████  | 42/52 [01:19<00:19,  1.91s/it][A
Epoch 19/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.91s/it][A
Epoch 19/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.91s/it][A
Epoch 19/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.90s/it][A
Epoch 19/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.90s/it][A
Epoch 19/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.90s/it][A
Epoch 19/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.90s/it][A
Epoch 19/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.91s/it][A
Epoch 19/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.91s/it][A
Epoch 19/32:  90%|█████████ | 47/52 [01:29<00:09,  1.90s/it][A
Epoch 19/32:  90%|█████████ | 47/52 [01:29<00:09,  1.90s/it][A
Epoch 19/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.90s/it][A
Epoch 19/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.90s/it][A
Epoch 19/32:  94%|█████████▍| 49/52 [01:33<00:05,  1.90s/it][A
Epoch 19/32:  94%|█████████▍| 49/52 [01:33<00:05,  1.90s/it][A
Epoch 19/32:  96%|█████████▌| 50/52 [01:35<00:03,  1.90s/it][A
Epoch 19/32:  96%|█████████▌| 50/52 [01:35<00:03,  1.90s/it][A
Epoch 19/32:  98%|█████████▊| 51/52 [01:37<00:01,  1.91s/it][A
Epoch 19/32:  98%|█████████▊| 51/52 [01:37<00:01,  1.91s/it][A
Epoch 19/32: 100%|██████████| 52/52 [01:38<00:00,  1.80s/it][AEpoch 19/32: 100%|██████████| 52/52 [01:38<00:00,  1.90s/it]

Epoch 19/32: 100%|██████████| 52/52 [01:38<00:00,  1.80s/it][AEpoch 19/32: 100%|██████████| 52/52 [01:38<00:00,  1.90s/it]

[β=2.0] Epoch 19 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=9.620e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.208e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=7.691e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.023e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.010e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.027e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.688e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.346e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.160e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=6.156e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=1.010e+00
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.581e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=1.004e+00
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.753e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.564e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.862e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.111e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=2.995e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.427e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=4.838e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.179e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.492e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.983e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.295e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.113e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=2.940e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.627e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=7.209e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.127e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.764e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.300e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.438e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.010e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=4.747e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=6.022e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=3.075e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.613e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.454e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.933e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.788e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.332e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.609e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=5.187e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=9.822e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.256e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.158e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.206e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.598e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.428e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.848e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.803e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.567e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.992e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.077e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.435e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.432e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.596e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=2.949e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.280e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.798e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.479e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.575e-01

  layer model_0__forward_module.module.channel_attention.3 act-std: mean=6.893e-01
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]  layer model_0__forward_module.module.output_conv act-std: mean=2.298e-01[A
  param model_0__forward_module.module.output_scale grad-norm: mean=1.787e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=6.835e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.583e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=7.053e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=8.296e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.100e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.091e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.998e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.190e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.660e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.510e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.166e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.664e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=4.639e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.048e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.599e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.301e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.343e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.320e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.411e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=6.165e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.395e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.085e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.054e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.156e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=9.404e-03
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.065e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.587e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.135e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.976e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.561e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.974e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=3.355e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.649e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.425e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=6.032e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.091e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.481e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.976e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=8.359e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.654e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.050e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=6.758e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.744e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.496e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.815e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.249e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.018e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=9.623e-03
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.381e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=7.574e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.731e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.421e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.814e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=7.826e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.229e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.504e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.082e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.489e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.312e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=1.828e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.137e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.221e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.917e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.907e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.425e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.283e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.738e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.188e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.143e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=1.919e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=1.827e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.396e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=8.919e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.323e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.733e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.733e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.171e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.439e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.984e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.193e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.423e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.617e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.899e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.767e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.484e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.051e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.272e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.269e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.362e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=3.133e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.185e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=8.412e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=1.961e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=1.748e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.752e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.167e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.411e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.862e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.286e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.760e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.256e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.839e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.637e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.231e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.257e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.794e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.156e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.779e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.523e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.045e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=9.078e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=1.889e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.794e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.211e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.754e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.633e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.330e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.044e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.450e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.882e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.899e-02
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.899e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.487e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.340e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.406e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.436e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.956e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=4.511e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.246e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.249e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.054e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.279e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=2.227e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.092e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.992e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.494e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.152e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.984e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.492e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.375e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.070e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.694e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.242e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.536e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.603e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.285e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.404e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.478e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.243e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.568e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.168e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.61it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.60it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.60it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.60it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.61it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.62it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.61it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.62it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.62it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.61it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.62it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.61it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.61it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.62it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.62it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.62it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.62it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.61it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.62it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.62it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.62it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.63it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.62it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.63it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]

Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]
Epoch 19/32 - Train Loss: 17.451375 - Test Loss: 17.516836
Training epochs:  59%|█████▉    | 19/32 [31:55<22:54, 105.74s/it]
Epoch 20/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  59%|█████▉    | 19/32 [31:55<22:54, 105.71s/it]
Epoch 20/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 20/32:   2%|▏         | 1/52 [00:01<01:37,  1.90s/it][A
Epoch 20/32:   2%|▏         | 1/52 [00:01<01:37,  1.91s/it][A
Epoch 20/32:   4%|▍         | 2/52 [00:03<01:34,  1.90s/it][A
Epoch 20/32:   4%|▍         | 2/52 [00:03<01:35,  1.90s/it][A
Epoch 20/32:   6%|▌         | 3/52 [00:05<01:32,  1.89s/it][A
Epoch 20/32:   6%|▌         | 3/52 [00:05<01:32,  1.90s/it][A
Epoch 20/32:   8%|▊         | 4/52 [00:07<01:31,  1.90s/it][A
Epoch 20/32:   8%|▊         | 4/52 [00:07<01:31,  1.90s/it][A
Epoch 20/32:  10%|▉         | 5/52 [00:09<01:29,  1.90s/it][A
Epoch 20/32:  10%|▉         | 5/52 [00:09<01:29,  1.90s/it][A
Epoch 20/32:  12%|█▏        | 6/52 [00:11<01:27,  1.90s/it][A
Epoch 20/32:  12%|█▏        | 6/52 [00:11<01:27,  1.90s/it][A
Epoch 20/32:  13%|█▎        | 7/52 [00:13<01:25,  1.90s/it][A
Epoch 20/32:  13%|█▎        | 7/52 [00:13<01:25,  1.90s/it][A
Epoch 20/32:  15%|█▌        | 8/52 [00:15<01:23,  1.90s/it][A
Epoch 20/32:  15%|█▌        | 8/52 [00:15<01:23,  1.90s/it][A
Epoch 20/32:  17%|█▋        | 9/52 [00:17<01:21,  1.89s/it][A
Epoch 20/32:  17%|█▋        | 9/52 [00:17<01:21,  1.90s/it][A
Epoch 20/32:  19%|█▉        | 10/52 [00:18<01:19,  1.90s/it][A
Epoch 20/32:  19%|█▉        | 10/52 [00:18<01:19,  1.90s/it][A
Epoch 20/32:  21%|██        | 11/52 [00:20<01:18,  1.91s/it][A
Epoch 20/32:  21%|██        | 11/52 [00:20<01:18,  1.91s/it][A
Epoch 20/32:  23%|██▎       | 12/52 [00:22<01:16,  1.91s/it][A
Epoch 20/32:  23%|██▎       | 12/52 [00:22<01:16,  1.91s/it][A
Epoch 20/32:  25%|██▌       | 13/52 [00:24<01:14,  1.91s/it][A
Epoch 20/32:  25%|██▌       | 13/52 [00:24<01:14,  1.91s/it][A
Epoch 20/32:  27%|██▋       | 14/52 [00:26<01:12,  1.91s/it][A
Epoch 20/32:  27%|██▋       | 14/52 [00:26<01:12,  1.91s/it][A
Epoch 20/32:  29%|██▉       | 15/52 [00:28<01:10,  1.91s/it][A
Epoch 20/32:  29%|██▉       | 15/52 [00:28<01:10,  1.91s/it][A
Epoch 20/32:  31%|███       | 16/52 [00:30<01:08,  1.90s/it][A
Epoch 20/32:  31%|███       | 16/52 [00:30<01:08,  1.91s/it][A
Epoch 20/32:  33%|███▎      | 17/52 [00:32<01:06,  1.91s/it][A
Epoch 20/32:  33%|███▎      | 17/52 [00:32<01:06,  1.91s/it][A
Epoch 20/32:  35%|███▍      | 18/52 [00:34<01:04,  1.91s/it][A
Epoch 20/32:  35%|███▍      | 18/52 [00:34<01:04,  1.91s/it][A
Epoch 20/32:  37%|███▋      | 19/52 [00:36<01:02,  1.91s/it][A
Epoch 20/32:  37%|███▋      | 19/52 [00:36<01:02,  1.91s/it][A
Epoch 20/32:  38%|███▊      | 20/52 [00:38<01:00,  1.90s/it][A
Epoch 20/32:  38%|███▊      | 20/52 [00:38<01:00,  1.90s/it][A
Epoch 20/32:  40%|████      | 21/52 [00:39<00:58,  1.90s/it][A
Epoch 20/32:  40%|████      | 21/52 [00:39<00:58,  1.90s/it][A
Epoch 20/32:  42%|████▏     | 22/52 [00:41<00:56,  1.90s/it][A
Epoch 20/32:  42%|████▏     | 22/52 [00:41<00:56,  1.90s/it][A
Epoch 20/32:  44%|████▍     | 23/52 [00:43<00:55,  1.90s/it][A
Epoch 20/32:  44%|████▍     | 23/52 [00:43<00:55,  1.91s/it][A
Epoch 20/32:  46%|████▌     | 24/52 [00:45<00:53,  1.91s/it][A
Epoch 20/32:  46%|████▌     | 24/52 [00:45<00:53,  1.91s/it][A
Epoch 20/32:  48%|████▊     | 25/52 [00:47<00:51,  1.92s/it][A
Epoch 20/32:  48%|████▊     | 25/52 [00:47<00:51,  1.92s/it][A
Epoch 20/32:  50%|█████     | 26/52 [00:49<00:49,  1.92s/it][A
Epoch 20/32:  50%|█████     | 26/52 [00:49<00:49,  1.92s/it][A
Epoch 20/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.92s/it][A
Epoch 20/32:  52%|█████▏    | 27/52 [00:51<00:47,  1.92s/it][A
Epoch 20/32:  54%|█████▍    | 28/52 [00:53<00:45,  1.92s/it][A
Epoch 20/32:  54%|█████▍    | 28/52 [00:53<00:45,  1.92s/it][A
Epoch 20/32:  56%|█████▌    | 29/52 [00:55<00:44,  1.91s/it][A
Epoch 20/32:  56%|█████▌    | 29/52 [00:55<00:43,  1.91s/it][A
Epoch 20/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.91s/it][A
Epoch 20/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.92s/it][A
Epoch 20/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.92s/it][A
Epoch 20/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.92s/it][A
Epoch 20/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.92s/it][A
Epoch 20/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.92s/it][A
Epoch 20/32:  63%|██████▎   | 33/52 [01:02<00:36,  1.91s/it][A
Epoch 20/32:  63%|██████▎   | 33/52 [01:02<00:36,  1.91s/it][A

Epoch 20/32:  65%|██████▌   | 34/52 [01:04<00:34,  1.91s/it][AEpoch 20/32:  65%|██████▌   | 34/52 [01:04<00:34,  1.91s/it][A

Epoch 20/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.91s/it][AEpoch 20/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.91s/it][A
Epoch 20/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.90s/it][A
Epoch 20/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.91s/it][A
Epoch 20/32:  71%|███████   | 37/52 [01:10<00:28,  1.91s/it][A
Epoch 20/32:  71%|███████   | 37/52 [01:10<00:28,  1.91s/it][A
Epoch 20/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.91s/it][A
Epoch 20/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.91s/it][A
Epoch 20/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.91s/it][A
Epoch 20/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.91s/it][A
Epoch 20/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.90s/it][A
Epoch 20/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.90s/it][A
Epoch 20/32:  79%|███████▉  | 41/52 [01:18<00:20,  1.90s/it][A
Epoch 20/32:  79%|███████▉  | 41/52 [01:18<00:20,  1.90s/it][A
Epoch 20/32:  81%|████████  | 42/52 [01:20<00:19,  1.90s/it][A
Epoch 20/32:  81%|████████  | 42/52 [01:20<00:18,  1.90s/it][A
Epoch 20/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.90s/it][A
Epoch 20/32:  83%|████████▎ | 43/52 [01:21<00:17,  1.90s/it][A
Epoch 20/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.91s/it][A
Epoch 20/32:  85%|████████▍ | 44/52 [01:23<00:15,  1.91s/it][A
Epoch 20/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.91s/it][A
Epoch 20/32:  87%|████████▋ | 45/52 [01:25<00:13,  1.91s/it][A
Epoch 20/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.90s/it][A
Epoch 20/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.90s/it][A
Epoch 20/32:  90%|█████████ | 47/52 [01:29<00:09,  1.89s/it][A
Epoch 20/32:  90%|█████████ | 47/52 [01:29<00:09,  1.89s/it][A
Epoch 20/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.89s/it][A
Epoch 20/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.89s/it][A
Epoch 20/32:  94%|█████████▍| 49/52 [01:33<00:05,  1.89s/it][A
Epoch 20/32:  94%|█████████▍| 49/52 [01:33<00:05,  1.89s/it][A
Epoch 20/32:  96%|█████████▌| 50/52 [01:35<00:03,  1.90s/it][A
Epoch 20/32:  96%|█████████▌| 50/52 [01:35<00:03,  1.90s/it][A
Epoch 20/32:  98%|█████████▊| 51/52 [01:37<00:01,  1.90s/it][A
Epoch 20/32:  98%|█████████▊| 51/52 [01:37<00:01,  1.91s/it][A
Epoch 20/32: 100%|██████████| 52/52 [01:38<00:00,  1.80s/it][AEpoch 20/32: 100%|██████████| 52/52 [01:38<00:00,  1.90s/it]

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Epoch 20/32: 100%|██████████| 52/52 [01:38<00:00,  1.80s/it][AEpoch 20/32: 100%|██████████| 52/52 [01:38<00:00,  1.90s/it]

[β=2.0] Epoch 20 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=9.892e-02
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.274e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=7.873e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.039e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=1.001e+00
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.103e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.706e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.306e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.162e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=6.205e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=1.015e+00
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.514e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=9.657e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.792e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.670e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.831e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.144e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.224e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.585e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=4.455e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.172e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.441e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.973e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.282e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.135e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=3.576e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.775e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=7.060e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.097e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.651e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.200e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.424e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.041e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=4.958e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=6.576e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.983e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.597e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.321e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.849e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.742e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.378e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.718e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=5.575e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=9.651e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.258e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.181e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.404e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.569e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.483e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.698e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.702e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.528e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.935e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=8.045e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.633e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.402e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.631e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.019e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.193e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.659e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.434e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.513e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=7.190e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.201e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.754e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=7.631e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=1.787e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=7.771e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=9.282e-02
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.640e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.885e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.499e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.648e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.661e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.544e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.141e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.764e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=4.318e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.038e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.542e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.120e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.379e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.397e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.054e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=6.039e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.346e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=9.409e-02
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.038e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.124e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=9.133e-03
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.706e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.530e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.013e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.986e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.682e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.444e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.853e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.443e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.401e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=5.875e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.003e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.482e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.932e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=8.537e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.737e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.052e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=7.465e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.669e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.267e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.810e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.019e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.357e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.168e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.805e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.532e-03
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.427e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.281e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=1.966e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=8.284e-03
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.250e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.494e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.157e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.806e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.165e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.092e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.550e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.358e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.782e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.014e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.620e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.895e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.196e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.729e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.483e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.958e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.009e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.451e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=9.457e-03
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.407e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=1.833e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.578e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.224e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.608e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.930e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.217e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.769e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.783e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.043e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.724e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.735e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.137e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.615e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.362e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.230e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=3.015e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.284e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=9.257e-03
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.045e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.057e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.400e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.244e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.576e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.822e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.162e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.976e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.400e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.203e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.614e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.336e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.094e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.345e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.316e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.895e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.945e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.049e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=9.353e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.060e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.989e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.971e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.589e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.173e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.043e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=1.701e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.345e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.690e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.071e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.037e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.634e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.436e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.284e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.563e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.760e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=4.269e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.231e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.217e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.122e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.586e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=2.013e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.465e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=7.894e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.780e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.431e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.784e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.990e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.018e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.967e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.868e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.238e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.579e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.752e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.465e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.794e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.561e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.277e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.657e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.285e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.66it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.65it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.66it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.65it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:05,  1.67it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.66it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.67it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.65it/s][A
Evaluating:  38%|███▊      | 5/13 [00:02<00:04,  1.68it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.64it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.66it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.62it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.60it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:04,  1.49it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.58it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.51it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.58it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.53it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.60it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.56it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.60it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.57it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.61it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.59it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]
Epoch 20/32 - Train Loss: 17.367940 - Test Loss: 17.461574
Training epochs:  62%|██████▎   | 20/32 [33:42<21:12, 106.04s/it]
Epoch 21/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]
Training epochs:  62%|██████▎   | 20/32 [33:42<21:13, 106.10s/it]
Epoch 21/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 21/32:   2%|▏         | 1/52 [00:01<01:39,  1.96s/it][A
Epoch 21/32:   2%|▏         | 1/52 [00:01<01:35,  1.87s/it][A
Epoch 21/32:   4%|▍         | 2/52 [00:03<01:35,  1.90s/it][A
Epoch 21/32:   4%|▍         | 2/52 [00:03<01:33,  1.87s/it][A
Epoch 21/32:   6%|▌         | 3/52 [00:05<01:32,  1.88s/it][A
Epoch 21/32:   6%|▌         | 3/52 [00:05<01:33,  1.90s/it][A
Epoch 21/32:   8%|▊         | 4/52 [00:07<01:31,  1.90s/it][A
Epoch 21/32:   8%|▊         | 4/52 [00:07<01:31,  1.91s/it][A
Epoch 21/32:  10%|▉         | 5/52 [00:09<01:29,  1.91s/it][A
Epoch 21/32:  10%|▉         | 5/52 [00:09<01:29,  1.91s/it][A
Epoch 21/32:  12%|█▏        | 6/52 [00:11<01:29,  1.96s/it][A
Epoch 21/32:  12%|█▏        | 6/52 [00:11<01:30,  1.96s/it][A
Epoch 21/32:  13%|█▎        | 7/52 [00:13<01:26,  1.93s/it][A
Epoch 21/32:  13%|█▎        | 7/52 [00:13<01:27,  1.93s/it][A
Epoch 21/32:  15%|█▌        | 8/52 [00:15<01:24,  1.92s/it][A
Epoch 21/32:  15%|█▌        | 8/52 [00:15<01:24,  1.92s/it][A
Epoch 21/32:  17%|█▋        | 9/52 [00:17<01:22,  1.92s/it][A
Epoch 21/32:  17%|█▋        | 9/52 [00:17<01:22,  1.92s/it][A
Epoch 21/32:  19%|█▉        | 10/52 [00:19<01:20,  1.92s/it][A
Epoch 21/32:  19%|█▉        | 10/52 [00:19<01:20,  1.93s/it][A
Epoch 21/32:  21%|██        | 11/52 [00:21<01:18,  1.92s/it][A
Epoch 21/32:  21%|██        | 11/52 [00:21<01:18,  1.92s/it][A
Epoch 21/32:  23%|██▎       | 12/52 [00:23<01:16,  1.92s/it][A
Epoch 21/32:  23%|██▎       | 12/52 [00:22<01:16,  1.92s/it][A
Epoch 21/32:  25%|██▌       | 13/52 [00:24<01:14,  1.91s/it][A
Epoch 21/32:  25%|██▌       | 13/52 [00:24<01:14,  1.91s/it][A
Epoch 21/32:  27%|██▋       | 14/52 [00:26<01:12,  1.91s/it][A
Epoch 21/32:  27%|██▋       | 14/52 [00:26<01:12,  1.91s/it][A
Epoch 21/32:  29%|██▉       | 15/52 [00:28<01:10,  1.91s/it][A
Epoch 21/32:  29%|██▉       | 15/52 [00:28<01:10,  1.91s/it][A
Epoch 21/32:  31%|███       | 16/52 [00:30<01:08,  1.90s/it][A
Epoch 21/32:  31%|███       | 16/52 [00:30<01:08,  1.90s/it][A
Epoch 21/32:  33%|███▎      | 17/52 [00:32<01:06,  1.91s/it][A
Epoch 21/32:  33%|███▎      | 17/52 [00:32<01:06,  1.91s/it][A
Epoch 21/32:  35%|███▍      | 18/52 [00:34<01:05,  1.91s/it][A
Epoch 21/32:  35%|███▍      | 18/52 [00:34<01:05,  1.92s/it][A
Epoch 21/32:  37%|███▋      | 19/52 [00:36<01:03,  1.92s/it][A
Epoch 21/32:  37%|███▋      | 19/52 [00:36<01:03,  1.92s/it][A
Epoch 21/32:  38%|███▊      | 20/52 [00:38<01:01,  1.92s/it][A
Epoch 21/32:  38%|███▊      | 20/52 [00:38<01:01,  1.92s/it][A
Epoch 21/32:  40%|████      | 21/52 [00:40<00:59,  1.91s/it][A
Epoch 21/32:  40%|████      | 21/52 [00:40<00:59,  1.91s/it][A
Epoch 21/32:  42%|████▏     | 22/52 [00:42<00:57,  1.91s/it][A
Epoch 21/32:  42%|████▏     | 22/52 [00:42<00:57,  1.91s/it][A
Epoch 21/32:  44%|████▍     | 23/52 [00:44<00:55,  1.91s/it][A
Epoch 21/32:  44%|████▍     | 23/52 [00:43<00:55,  1.91s/it][A

Epoch 21/32:  46%|████▌     | 24/52 [00:46<00:53,  1.92s/it][AEpoch 21/32:  46%|████▌     | 24/52 [00:45<00:53,  1.92s/it][A
Epoch 21/32:  48%|████▊     | 25/52 [00:47<00:51,  1.91s/it][A
Epoch 21/32:  48%|████▊     | 25/52 [00:47<00:51,  1.91s/it][A
Epoch 21/32:  50%|█████     | 26/52 [00:49<00:49,  1.92s/it][A
Epoch 21/32:  50%|█████     | 26/52 [00:49<00:49,  1.92s/it][A
Epoch 21/32:  52%|█████▏    | 27/52 [00:51<00:48,  1.93s/it][A
Epoch 21/32:  52%|█████▏    | 27/52 [00:51<00:48,  1.93s/it][A
Epoch 21/32:  54%|█████▍    | 28/52 [00:53<00:46,  1.92s/it][A
Epoch 21/32:  54%|█████▍    | 28/52 [00:53<00:46,  1.93s/it][A
Epoch 21/32:  56%|█████▌    | 29/52 [00:55<00:44,  1.91s/it][A
Epoch 21/32:  56%|█████▌    | 29/52 [00:55<00:44,  1.92s/it][A

Epoch 21/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.91s/it][AEpoch 21/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.91s/it][A
Epoch 21/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.90s/it][A
Epoch 21/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.91s/it][A
Epoch 21/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.90s/it][A
Epoch 21/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.90s/it][A
Epoch 21/32:  63%|██████▎   | 33/52 [01:03<00:36,  1.90s/it][A
Epoch 21/32:  63%|██████▎   | 33/52 [01:03<00:36,  1.90s/it][A
Epoch 21/32:  65%|██████▌   | 34/52 [01:05<00:34,  1.90s/it][A
Epoch 21/32:  65%|██████▌   | 34/52 [01:05<00:34,  1.90s/it][A
Epoch 21/32:  67%|██████▋   | 35/52 [01:06<00:32,  1.91s/it][A
Epoch 21/32:  67%|██████▋   | 35/52 [01:07<00:32,  1.91s/it][A
Epoch 21/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.91s/it][A
Epoch 21/32:  69%|██████▉   | 36/52 [01:08<00:30,  1.91s/it][A
Epoch 21/32:  71%|███████   | 37/52 [01:10<00:28,  1.91s/it][A
Epoch 21/32:  71%|███████   | 37/52 [01:10<00:28,  1.91s/it][A

Epoch 21/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.91s/it][AEpoch 21/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.91s/it][A
Epoch 21/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.90s/it][A
Epoch 21/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.90s/it][A
Epoch 21/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.90s/it][A
Epoch 21/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.90s/it][A
Epoch 21/32:  79%|███████▉  | 41/52 [01:18<00:20,  1.91s/it][A
Epoch 21/32:  79%|███████▉  | 41/52 [01:18<00:21,  1.91s/it][A
Epoch 21/32:  81%|████████  | 42/52 [01:20<00:19,  1.92s/it][A
Epoch 21/32:  81%|████████  | 42/52 [01:20<00:19,  1.92s/it][A
Epoch 21/32:  83%|████████▎ | 43/52 [01:22<00:17,  1.92s/it][A
Epoch 21/32:  83%|████████▎ | 43/52 [01:22<00:17,  1.92s/it][A
Epoch 21/32:  85%|████████▍ | 44/52 [01:24<00:15,  1.92s/it][A
Epoch 21/32:  85%|████████▍ | 44/52 [01:24<00:15,  1.92s/it][A
Epoch 21/32:  87%|████████▋ | 45/52 [01:26<00:13,  1.92s/it][A
Epoch 21/32:  87%|████████▋ | 45/52 [01:26<00:13,  1.92s/it][A
Epoch 21/32:  88%|████████▊ | 46/52 [01:28<00:11,  1.92s/it][A
Epoch 21/32:  88%|████████▊ | 46/52 [01:27<00:11,  1.92s/it][A
Epoch 21/32:  90%|█████████ | 47/52 [01:29<00:09,  1.91s/it][A
Epoch 21/32:  90%|█████████ | 47/52 [01:29<00:09,  1.91s/it][A
Epoch 21/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.91s/it][A
Epoch 21/32:  92%|█████████▏| 48/52 [01:31<00:07,  1.91s/it][A
Epoch 21/32:  94%|█████████▍| 49/52 [01:33<00:05,  1.91s/it][A
Epoch 21/32:  94%|█████████▍| 49/52 [01:33<00:05,  1.91s/it][A
Epoch 21/32:  96%|█████████▌| 50/52 [01:35<00:03,  1.92s/it][A
Epoch 21/32:  96%|█████████▌| 50/52 [01:35<00:03,  1.92s/it][A

Epoch 21/32:  98%|█████████▊| 51/52 [01:37<00:01,  1.92s/it][AEpoch 21/32:  98%|█████████▊| 51/52 [01:37<00:01,  1.92s/it][A
Epoch 21/32: 100%|██████████| 52/52 [01:39<00:00,  1.81s/it][AEpoch 21/32: 100%|██████████| 52/52 [01:39<00:00,  1.91s/it]

[β=2.0] Epoch 21 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.021e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.302e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=7.928e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.991e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.645e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.146e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.735e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.258e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.206e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=6.141e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.962e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.431e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=9.434e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.771e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.741e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.760e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.172e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.330e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.744e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=4.293e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.152e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.415e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.983e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.262e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.170e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.074e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.893e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.819e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.067e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.497e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.057e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.401e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.069e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.073e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=6.970e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.886e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.577e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=7.079e-01

  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.684e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.690e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.406e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.813e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=5.990e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=9.430e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.214e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.194e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.524e-01
Epoch 21/32: 100%|██████████| 52/52 [01:39<00:00,  1.81s/it]  layer model_2__forward_module.module.output_conv act-std: mean=1.562e-01[A
  layer model_1__forward_module.module.input_conv act-std: mean=1.533e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.535e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.572e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.474e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.881e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=7.897e-01
Epoch 21/32: 100%|██████████| 52/52 [01:39<00:00,  1.91s/it]  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.697e-01

  layer model_1__forward_module.module.output_conv act-std: mean=2.365e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.667e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.065e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.039e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.432e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.404e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.444e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=7.436e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.127e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.806e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=9.015e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.143e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=9.102e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.073e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.549e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.437e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.721e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.883e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.307e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.080e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.771e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.675e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=5.180e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.518e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.939e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.777e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.457e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.388e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.281e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=7.448e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.705e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.264e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.381e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.328e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.342e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.885e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.126e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.637e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.771e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.241e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.117e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=4.076e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.220e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.682e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.316e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.252e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.648e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=2.034e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.534e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.984e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.409e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=9.799e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.052e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.741e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.190e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.154e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.120e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.564e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.659e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.231e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.783e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.486e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.835e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.172e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.824e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.973e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.058e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.692e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.810e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.149e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.478e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.333e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.315e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.916e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.689e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.066e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.282e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.038e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.713e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.323e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=2.518e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.492e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.002e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.454e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.159e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.913e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.962e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.526e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.893e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.011e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.237e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.922e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.808e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.422e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.529e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.645e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.771e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.092e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.273e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=5.115e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.681e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.264e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.498e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.756e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.459e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.275e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.614e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.082e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.458e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.939e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.286e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.019e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.694e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.335e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.383e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.391e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.468e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.718e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.835e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.211e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.070e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.057e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=1.928e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.221e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.483e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=5.485e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=3.333e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=3.178e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.324e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.553e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.270e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.833e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.132e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.956e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.432e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.136e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.677e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=6.822e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.532e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.522e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.462e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=3.508e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.801e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.876e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.451e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.395e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.113e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.154e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.033e-02

  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.155e-02
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.930e-02[A
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.817e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.276e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.434e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.740e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.007e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=5.385e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.574e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.271e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.673e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.374e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.70it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.66it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.65it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.64it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.66it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.64it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.63it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.61it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.62it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.60it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.61it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.60it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.63it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.62it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.64it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.63it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.65it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.64it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.65it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.65it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.66it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.66it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.66it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.67it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]

Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]
Epoch 21/32 - Train Loss: 17.299256 - Test Loss: 17.389570
Training epochs:  66%|██████▌   | 21/32 [35:29<19:30, 106.39s/it]
Epoch 22/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  66%|██████▌   | 21/32 [35:29<19:30, 106.39s/it]
Epoch 22/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 22/32:   2%|▏         | 1/52 [00:01<01:36,  1.90s/it][A
Epoch 22/32:   2%|▏         | 1/52 [00:01<01:35,  1.88s/it][A
Epoch 22/32:   4%|▍         | 2/52 [00:03<01:34,  1.90s/it][A
Epoch 22/32:   4%|▍         | 2/52 [00:03<01:34,  1.89s/it][A
Epoch 22/32:   6%|▌         | 3/52 [00:05<01:32,  1.89s/it][A
Epoch 22/32:   6%|▌         | 3/52 [00:05<01:33,  1.90s/it][A
Epoch 22/32:   8%|▊         | 4/52 [00:07<01:31,  1.90s/it][A
Epoch 22/32:   8%|▊         | 4/52 [00:07<01:31,  1.91s/it][A
Epoch 22/32:  10%|▉         | 5/52 [00:09<01:30,  1.92s/it][A
Epoch 22/32:  10%|▉         | 5/52 [00:09<01:30,  1.92s/it][A
Epoch 22/32:  12%|█▏        | 6/52 [00:11<01:27,  1.91s/it][A
Epoch 22/32:  12%|█▏        | 6/52 [00:11<01:27,  1.91s/it][A
Epoch 22/32:  13%|█▎        | 7/52 [00:13<01:25,  1.91s/it][A
Epoch 22/32:  13%|█▎        | 7/52 [00:13<01:25,  1.91s/it][A
Epoch 22/32:  15%|█▌        | 8/52 [00:15<01:23,  1.90s/it][A
Epoch 22/32:  15%|█▌        | 8/52 [00:15<01:23,  1.90s/it][A
Epoch 22/32:  17%|█▋        | 9/52 [00:17<01:21,  1.91s/it][A
Epoch 22/32:  17%|█▋        | 9/52 [00:17<01:22,  1.91s/it][A
Epoch 22/32:  19%|█▉        | 10/52 [00:19<01:20,  1.91s/it][A
Epoch 22/32:  19%|█▉        | 10/52 [00:19<01:20,  1.91s/it][A
Epoch 22/32:  21%|██        | 11/52 [00:21<01:18,  1.92s/it][A
Epoch 22/32:  21%|██        | 11/52 [00:20<01:18,  1.92s/it][A
Epoch 22/32:  23%|██▎       | 12/52 [00:22<01:16,  1.92s/it][A
Epoch 22/32:  23%|██▎       | 12/52 [00:22<01:16,  1.92s/it][A

Epoch 22/32:  25%|██▌       | 13/52 [00:24<01:14,  1.91s/it][AEpoch 22/32:  25%|██▌       | 13/52 [00:24<01:14,  1.91s/it][A
Epoch 22/32:  27%|██▋       | 14/52 [00:26<01:12,  1.91s/it][A
Epoch 22/32:  27%|██▋       | 14/52 [00:26<01:12,  1.91s/it][A
Epoch 22/32:  29%|██▉       | 15/52 [00:28<01:10,  1.91s/it][A
Epoch 22/32:  29%|██▉       | 15/52 [00:28<01:10,  1.91s/it][A
Epoch 22/32:  31%|███       | 16/52 [00:30<01:08,  1.90s/it][A
Epoch 22/32:  31%|███       | 16/52 [00:30<01:08,  1.91s/it][A
Epoch 22/32:  33%|███▎      | 17/52 [00:32<01:06,  1.91s/it][A
Epoch 22/32:  33%|███▎      | 17/52 [00:32<01:06,  1.91s/it][A
Epoch 22/32:  35%|███▍      | 18/52 [00:34<01:05,  1.93s/it][A
Epoch 22/32:  35%|███▍      | 18/52 [00:34<01:05,  1.93s/it][A
Epoch 22/32:  37%|███▋      | 19/52 [00:36<01:03,  1.92s/it][A
Epoch 22/32:  37%|███▋      | 19/52 [00:36<01:03,  1.92s/it][A
Epoch 22/32:  38%|███▊      | 20/52 [00:38<01:01,  1.92s/it][A
Epoch 22/32:  38%|███▊      | 20/52 [00:38<01:01,  1.92s/it][A
Epoch 22/32:  40%|████      | 21/52 [00:40<01:00,  1.96s/it][A
Epoch 22/32:  40%|████      | 21/52 [00:40<01:00,  1.96s/it][A
Epoch 22/32:  42%|████▏     | 22/52 [00:42<00:58,  1.95s/it][A
Epoch 22/32:  42%|████▏     | 22/52 [00:42<00:58,  1.95s/it][A
Epoch 22/32:  44%|████▍     | 23/52 [00:44<00:56,  1.94s/it][A
Epoch 22/32:  44%|████▍     | 23/52 [00:44<00:56,  1.94s/it][A
Epoch 22/32:  46%|████▌     | 24/52 [00:46<00:54,  1.93s/it][A
Epoch 22/32:  46%|████▌     | 24/52 [00:46<00:54,  1.93s/it][A
Epoch 22/32:  48%|████▊     | 25/52 [00:47<00:51,  1.92s/it][A
Epoch 22/32:  48%|████▊     | 25/52 [00:47<00:51,  1.92s/it][A
Epoch 22/32:  50%|█████     | 26/52 [00:49<00:49,  1.91s/it][A
Epoch 22/32:  50%|█████     | 26/52 [00:49<00:49,  1.92s/it][A
Epoch 22/32:  52%|█████▏    | 27/52 [00:51<00:48,  1.92s/it][A
Epoch 22/32:  52%|█████▏    | 27/52 [00:51<00:48,  1.92s/it][A
Epoch 22/32:  54%|█████▍    | 28/52 [00:53<00:46,  1.93s/it][A
Epoch 22/32:  54%|█████▍    | 28/52 [00:53<00:46,  1.93s/it][A
Epoch 22/32:  56%|█████▌    | 29/52 [00:55<00:44,  1.93s/it][A
Epoch 22/32:  56%|█████▌    | 29/52 [00:55<00:44,  1.93s/it][A
Epoch 22/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.93s/it][A
Epoch 22/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.93s/it][A
Epoch 22/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.92s/it][A
Epoch 22/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.92s/it][A
Epoch 22/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.92s/it][A
Epoch 22/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.92s/it][A
Epoch 22/32:  63%|██████▎   | 33/52 [01:03<00:36,  1.91s/it][A
Epoch 22/32:  63%|██████▎   | 33/52 [01:03<00:36,  1.92s/it][A
Epoch 22/32:  65%|██████▌   | 34/52 [01:05<00:34,  1.91s/it][A
Epoch 22/32:  65%|██████▌   | 34/52 [01:05<00:34,  1.92s/it][A
Epoch 22/32:  67%|██████▋   | 35/52 [01:07<00:32,  1.92s/it][A
Epoch 22/32:  67%|██████▋   | 35/52 [01:07<00:32,  1.92s/it][A
Epoch 22/32:  69%|██████▉   | 36/52 [01:09<00:30,  1.92s/it][A
Epoch 22/32:  69%|██████▉   | 36/52 [01:09<00:30,  1.92s/it][A
Epoch 22/32:  71%|███████   | 37/52 [01:11<00:28,  1.92s/it][A
Epoch 22/32:  71%|███████   | 37/52 [01:10<00:28,  1.92s/it][A

Epoch 22/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.92s/it][AEpoch 22/32:  73%|███████▎  | 38/52 [01:12<00:26,  1.92s/it][A
Epoch 22/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.92s/it][A
Epoch 22/32:  75%|███████▌  | 39/52 [01:14<00:24,  1.92s/it][A
Epoch 22/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.92s/it][A
Epoch 22/32:  77%|███████▋  | 40/52 [01:16<00:22,  1.92s/it][A
Epoch 22/32:  79%|███████▉  | 41/52 [01:18<00:21,  1.92s/it][A
Epoch 22/32:  79%|███████▉  | 41/52 [01:18<00:21,  1.92s/it][A
Epoch 22/32:  81%|████████  | 42/52 [01:20<00:19,  1.92s/it][A
Epoch 22/32:  81%|████████  | 42/52 [01:20<00:19,  1.92s/it][A
Epoch 22/32:  83%|████████▎ | 43/52 [01:22<00:17,  1.93s/it][A
Epoch 22/32:  83%|████████▎ | 43/52 [01:22<00:17,  1.93s/it][A
Epoch 22/32:  85%|████████▍ | 44/52 [01:24<00:15,  1.93s/it][A
Epoch 22/32:  85%|████████▍ | 44/52 [01:24<00:15,  1.93s/it][A
Epoch 22/32:  87%|████████▋ | 45/52 [01:26<00:13,  1.92s/it][A
Epoch 22/32:  87%|████████▋ | 45/52 [01:26<00:13,  1.92s/it][A
Epoch 22/32:  88%|████████▊ | 46/52 [01:28<00:11,  1.92s/it][A
Epoch 22/32:  88%|████████▊ | 46/52 [01:28<00:11,  1.92s/it][A

Epoch 22/32:  90%|█████████ | 47/52 [01:30<00:09,  1.97s/it][AEpoch 22/32:  90%|█████████ | 47/52 [01:30<00:09,  1.97s/it][A
Epoch 22/32:  92%|█████████▏| 48/52 [01:32<00:07,  1.95s/it][A
Epoch 22/32:  92%|█████████▏| 48/52 [01:32<00:07,  1.95s/it][A
Epoch 22/32:  94%|█████████▍| 49/52 [01:34<00:05,  1.95s/it][A
Epoch 22/32:  94%|█████████▍| 49/52 [01:34<00:05,  1.95s/it][A

Epoch 22/32:  96%|█████████▌| 50/52 [01:36<00:03,  1.94s/it][AEpoch 22/32:  96%|█████████▌| 50/52 [01:36<00:03,  1.94s/it][A
Epoch 22/32:  98%|█████████▊| 51/52 [01:38<00:01,  1.94s/it][A
Epoch 22/32:  98%|█████████▊| 51/52 [01:38<00:01,  1.95s/it][A
Epoch 22/32: 100%|██████████| 52/52 [01:39<00:00,  1.84s/it][AEpoch 22/32: 100%|██████████| 52/52 [01:39<00:00,  1.92s/it]

[β=2.0] Epoch 22 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.043e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.327e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.123e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.021e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.700e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.238e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.771e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.243e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.257e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=6.049e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.715e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.241e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=8.835e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.712e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.797e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.650e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.213e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.359e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.748e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=4.118e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.128e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.356e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.970e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.236e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.221e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.288e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=3.990e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.716e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.053e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.449e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.009e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.396e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.101e-01

  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.209e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=7.443e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.792e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.541e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.914e-01
Epoch 22/32: 100%|██████████| 52/52 [01:39<00:00,  1.84s/it]  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.567e-01[A
  layer model_3__forward_module.module.output_conv act-std: mean=1.652e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.416e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.843e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=6.370e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=9.105e-01
Epoch 22/32: 100%|██████████| 52/52 [01:39<00:00,  1.92s/it]  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.175e+00

  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.190e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.549e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.537e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.569e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.435e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.493e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.457e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.840e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=7.719e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.726e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.329e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.715e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.168e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.126e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.423e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.365e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.361e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=7.636e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.054e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=2.036e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=9.482e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.259e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=9.654e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.179e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.003e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.338e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.663e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.716e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.584e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.291e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.884e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.825e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=6.068e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=3.043e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.095e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=8.432e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.655e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.691e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.288e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=7.574e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.736e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.262e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.470e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.182e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.030e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.243e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.058e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.230e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.840e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.421e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.321e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=3.864e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.270e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.669e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.441e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.315e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.895e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=2.028e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.865e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.063e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.238e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=9.863e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.227e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.695e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.020e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.647e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.343e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.674e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.652e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.077e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.791e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.524e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.749e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.138e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.782e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.890e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.977e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.862e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.296e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.500e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.673e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.845e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.754e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.666e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.631e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.333e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.309e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.109e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.732e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.169e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=2.492e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.642e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.128e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.613e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.163e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.775e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.939e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.408e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.769e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.933e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.419e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.055e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.741e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.126e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.490e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.581e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.720e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.740e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.622e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=5.941e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.550e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.189e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.448e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.298e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.385e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.441e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.044e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.374e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.853e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.104e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.326e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.678e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.025e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.671e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.640e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.282e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.099e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.838e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=3.048e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.286e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.157e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.262e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.735e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.142e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.286e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.983e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=3.147e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.834e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.444e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.469e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.283e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.656e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.433e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.930e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.339e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.808e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.424e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=6.341e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.502e-02

  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.507e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.377e-01
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.910e-01[A
  param model_7__forward_module.module.output_scale grad-norm: mean=1.805e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.890e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.430e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.377e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.162e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.455e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.107e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.941e-02
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.155e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.883e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.257e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.789e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.916e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.088e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=5.378e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.738e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.374e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.834e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.070e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.61it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.59it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.61it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.61it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.62it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.62it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.62it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.62it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.62it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.62it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.64it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.64it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.64it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.63it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.64it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.63it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.64it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.64it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.63it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.63it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.65it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.64it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.65it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.66it/s][A
Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]
Epoch 22/32 - Train Loss: 17.240807 - Test Loss: 17.321792

Evaluating: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s][AEvaluating: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]
Training epochs:  69%|██████▉   | 22/32 [37:17<17:47, 106.78s/it]
Epoch 23/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  69%|██████▉   | 22/32 [37:17<17:47, 106.79s/it]
Epoch 23/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 23/32:   2%|▏         | 1/52 [00:01<01:37,  1.91s/it][A
Epoch 23/32:   2%|▏         | 1/52 [00:01<01:37,  1.90s/it][A
Epoch 23/32:   4%|▍         | 2/52 [00:03<01:34,  1.90s/it][A
Epoch 23/32:   4%|▍         | 2/52 [00:03<01:35,  1.91s/it][A
Epoch 23/32:   6%|▌         | 3/52 [00:05<01:33,  1.91s/it][A
Epoch 23/32:   6%|▌         | 3/52 [00:05<01:33,  1.92s/it][A
Epoch 23/32:   8%|▊         | 4/52 [00:07<01:32,  1.92s/it][A
Epoch 23/32:   8%|▊         | 4/52 [00:07<01:32,  1.92s/it][A
Epoch 23/32:  10%|▉         | 5/52 [00:09<01:29,  1.91s/it][A
Epoch 23/32:  10%|▉         | 5/52 [00:09<01:30,  1.92s/it][A
Epoch 23/32:  12%|█▏        | 6/52 [00:11<01:27,  1.91s/it][A
Epoch 23/32:  12%|█▏        | 6/52 [00:11<01:27,  1.91s/it][A
Epoch 23/32:  13%|█▎        | 7/52 [00:13<01:25,  1.91s/it][A
Epoch 23/32:  13%|█▎        | 7/52 [00:13<01:25,  1.91s/it][A
Epoch 23/32:  15%|█▌        | 8/52 [00:15<01:24,  1.91s/it][A
Epoch 23/32:  15%|█▌        | 8/52 [00:15<01:24,  1.92s/it][A
Epoch 23/32:  17%|█▋        | 9/52 [00:17<01:23,  1.93s/it][A
Epoch 23/32:  17%|█▋        | 9/52 [00:17<01:23,  1.93s/it][A
Epoch 23/32:  19%|█▉        | 10/52 [00:19<01:21,  1.94s/it][A
Epoch 23/32:  19%|█▉        | 10/52 [00:19<01:21,  1.94s/it][A
Epoch 23/32:  21%|██        | 11/52 [00:21<01:19,  1.93s/it][A
Epoch 23/32:  21%|██        | 11/52 [00:21<01:19,  1.94s/it][A
Epoch 23/32:  23%|██▎       | 12/52 [00:23<01:17,  1.93s/it][A
Epoch 23/32:  23%|██▎       | 12/52 [00:23<01:17,  1.93s/it][A
Epoch 23/32:  25%|██▌       | 13/52 [00:24<01:15,  1.92s/it][A
Epoch 23/32:  25%|██▌       | 13/52 [00:24<01:15,  1.93s/it][A

Epoch 23/32:  27%|██▋       | 14/52 [00:26<01:13,  1.92s/it][AEpoch 23/32:  27%|██▋       | 14/52 [00:26<01:13,  1.93s/it][A
Epoch 23/32:  29%|██▉       | 15/52 [00:28<01:11,  1.93s/it][A
Epoch 23/32:  29%|██▉       | 15/52 [00:28<01:11,  1.93s/it][A
Epoch 23/32:  31%|███       | 16/52 [00:30<01:09,  1.92s/it][A
Epoch 23/32:  31%|███       | 16/52 [00:30<01:09,  1.92s/it][A
Epoch 23/32:  33%|███▎      | 17/52 [00:32<01:07,  1.92s/it][A
Epoch 23/32:  33%|███▎      | 17/52 [00:32<01:07,  1.92s/it][A
Epoch 23/32:  35%|███▍      | 18/52 [00:34<01:05,  1.92s/it][A
Epoch 23/32:  35%|███▍      | 18/52 [00:34<01:05,  1.92s/it][A
Epoch 23/32:  37%|███▋      | 19/52 [00:36<01:03,  1.92s/it][A
Epoch 23/32:  37%|███▋      | 19/52 [00:36<01:03,  1.92s/it][A
Epoch 23/32:  38%|███▊      | 20/52 [00:38<01:01,  1.93s/it][A
Epoch 23/32:  38%|███▊      | 20/52 [00:38<01:01,  1.93s/it][A
Epoch 23/32:  40%|████      | 21/52 [00:40<00:59,  1.93s/it][A
Epoch 23/32:  40%|████      | 21/52 [00:40<00:59,  1.93s/it][A
Epoch 23/32:  42%|████▏     | 22/52 [00:42<00:57,  1.93s/it][A
Epoch 23/32:  42%|████▏     | 22/52 [00:42<00:57,  1.93s/it][A
Epoch 23/32:  44%|████▍     | 23/52 [00:44<00:55,  1.93s/it][A
Epoch 23/32:  44%|████▍     | 23/52 [00:44<00:55,  1.93s/it][A
Epoch 23/32:  46%|████▌     | 24/52 [00:46<00:53,  1.92s/it][A
Epoch 23/32:  46%|████▌     | 24/52 [00:46<00:53,  1.92s/it][A
Epoch 23/32:  48%|████▊     | 25/52 [00:48<00:51,  1.92s/it][A
Epoch 23/32:  48%|████▊     | 25/52 [00:48<00:51,  1.92s/it][A
Epoch 23/32:  50%|█████     | 26/52 [00:50<00:50,  1.93s/it][A
Epoch 23/32:  50%|█████     | 26/52 [00:50<00:50,  1.93s/it][A
Epoch 23/32:  52%|█████▏    | 27/52 [00:51<00:48,  1.93s/it][A
Epoch 23/32:  52%|█████▏    | 27/52 [00:51<00:48,  1.93s/it][A
Epoch 23/32:  54%|█████▍    | 28/52 [00:53<00:46,  1.93s/it][A
Epoch 23/32:  54%|█████▍    | 28/52 [00:53<00:46,  1.93s/it][A
Epoch 23/32:  56%|█████▌    | 29/52 [00:55<00:44,  1.92s/it][A
Epoch 23/32:  56%|█████▌    | 29/52 [00:55<00:44,  1.92s/it][A
Epoch 23/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.92s/it][A
Epoch 23/32:  58%|█████▊    | 30/52 [00:57<00:42,  1.92s/it][A
Epoch 23/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.92s/it][A
Epoch 23/32:  60%|█████▉    | 31/52 [00:59<00:40,  1.92s/it][A
Epoch 23/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.93s/it][A
Epoch 23/32:  62%|██████▏   | 32/52 [01:01<00:38,  1.93s/it][A
Epoch 23/32:  63%|██████▎   | 33/52 [01:03<00:36,  1.93s/it][A
Epoch 23/32:  63%|██████▎   | 33/52 [01:03<00:36,  1.94s/it][A
Epoch 23/32:  65%|██████▌   | 34/52 [01:05<00:34,  1.93s/it][A
Epoch 23/32:  65%|██████▌   | 34/52 [01:05<00:34,  1.93s/it][A
Epoch 23/32:  67%|██████▋   | 35/52 [01:07<00:32,  1.93s/it][A
Epoch 23/32:  67%|██████▋   | 35/52 [01:07<00:32,  1.93s/it][A

Epoch 23/32:  69%|██████▉   | 36/52 [01:09<00:30,  1.93s/it][AEpoch 23/32:  69%|██████▉   | 36/52 [01:09<00:30,  1.93s/it][A
Epoch 23/32:  71%|███████   | 37/52 [01:11<00:28,  1.93s/it][A
Epoch 23/32:  71%|███████   | 37/52 [01:11<00:28,  1.93s/it][A
Epoch 23/32:  73%|███████▎  | 38/52 [01:13<00:27,  1.93s/it][A
Epoch 23/32:  73%|███████▎  | 38/52 [01:13<00:27,  1.93s/it][A
Epoch 23/32:  75%|███████▌  | 39/52 [01:15<00:25,  1.93s/it][A
Epoch 23/32:  75%|███████▌  | 39/52 [01:15<00:25,  1.93s/it][A
Epoch 23/32:  77%|███████▋  | 40/52 [01:17<00:23,  1.92s/it][A
Epoch 23/32:  77%|███████▋  | 40/52 [01:17<00:23,  1.93s/it][A
Epoch 23/32:  79%|███████▉  | 41/52 [01:18<00:21,  1.93s/it][A
Epoch 23/32:  79%|███████▉  | 41/52 [01:18<00:21,  1.93s/it][A
Epoch 23/32:  81%|████████  | 42/52 [01:20<00:19,  1.93s/it][A
Epoch 23/32:  81%|████████  | 42/52 [01:20<00:19,  1.94s/it][A
Epoch 23/32:  83%|████████▎ | 43/52 [01:22<00:17,  1.94s/it][A
Epoch 23/32:  83%|████████▎ | 43/52 [01:22<00:17,  1.94s/it][A
Epoch 23/32:  85%|████████▍ | 44/52 [01:24<00:15,  1.93s/it][A
Epoch 23/32:  85%|████████▍ | 44/52 [01:24<00:15,  1.93s/it][A
Epoch 23/32:  87%|████████▋ | 45/52 [01:26<00:13,  1.93s/it][A
Epoch 23/32:  87%|████████▋ | 45/52 [01:26<00:13,  1.93s/it][A
Epoch 23/32:  88%|████████▊ | 46/52 [01:28<00:11,  1.93s/it][A
Epoch 23/32:  88%|████████▊ | 46/52 [01:28<00:11,  1.93s/it][A
Epoch 23/32:  90%|█████████ | 47/52 [01:30<00:09,  1.93s/it][A
Epoch 23/32:  90%|█████████ | 47/52 [01:30<00:09,  1.93s/it][A
Epoch 23/32:  92%|█████████▏| 48/52 [01:32<00:07,  1.93s/it][A
Epoch 23/32:  92%|█████████▏| 48/52 [01:32<00:07,  1.93s/it][A
Epoch 23/32:  94%|█████████▍| 49/52 [01:34<00:05,  1.93s/it][A
Epoch 23/32:  94%|█████████▍| 49/52 [01:34<00:05,  1.93s/it][A
Epoch 23/32:  96%|█████████▌| 50/52 [01:36<00:03,  1.93s/it][A
Epoch 23/32:  96%|█████████▌| 50/52 [01:36<00:03,  1.93s/it][A
Epoch 23/32:  98%|█████████▊| 51/52 [01:38<00:01,  1.92s/it][A
Epoch 23/32:  98%|█████████▊| 51/52 [01:38<00:01,  1.93s/it][A
Epoch 23/32: 100%|██████████| 52/52 [01:39<00:00,  1.81s/it][AEpoch 23/32: 100%|██████████| 52/52 [01:39<00:00,  1.92s/it]

[β=2.0] Epoch 23 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.071e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.324e-01

  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.223e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.997e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.475e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.283e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.805e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.213e-01
Epoch 23/32: 100%|██████████| 52/52 [01:39<00:00,  1.81s/it]  layer model_6__forward_module.module.input_conv act-std: mean=1.289e-01[A
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=6.032e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.656e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.190e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=8.689e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.737e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.842e-01
Epoch 23/32: 100%|██████████| 52/52 [01:39<00:00,  1.92s/it]  layer model_6__forward_module.module.output_conv act-std: mean=1.606e-01

  layer model_5__forward_module.module.input_conv act-std: mean=1.239e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.360e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.766e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.936e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.094e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.294e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.960e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.216e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.247e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.461e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.137e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.644e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.058e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.423e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=3.994e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.403e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.130e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.307e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=7.945e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.721e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.519e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.727e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.450e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.606e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.434e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.878e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=6.682e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=9.093e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.152e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.184e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.548e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.517e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.606e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.402e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.485e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.416e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.800e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=7.596e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.784e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.312e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.777e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.236e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.112e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.233e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.393e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.323e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=7.783e-01
  layer model_0__forward_module.module.output_conv act-std: mean=2.019e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.483e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=9.401e-01
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.205e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=9.792e-02
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.021e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.921e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.826e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.895e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.708e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.644e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.608e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.323e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=6.951e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=3.722e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=1.866e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.694e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.747e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.585e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.477e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.738e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=6.525e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.409e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.263e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.022e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.186e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.098e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=2.809e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.455e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.395e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.710e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.390e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.261e-03
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.565e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.595e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.264e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=5.661e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.099e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.656e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.857e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.004e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.131e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.326e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=9.881e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.341e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.857e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.057e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.729e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.751e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.385e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.489e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.161e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.681e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.491e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.727e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.130e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.767e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.915e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=2.133e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.871e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.261e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.595e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.670e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.958e-02

  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.787e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.366e-02Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.725e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.391e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.221e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.924e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.578e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.636e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.159e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.874e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.321e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.622e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.052e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.412e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.724e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.842e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.335e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.611e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.151e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.776e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.716e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.898e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.225e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.317e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.554e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.566e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.172e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=5.429e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.447e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.109e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.463e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.384e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.093e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.246e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.466e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=1.929e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.303e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.048e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.255e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.072e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.537e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.328e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.036e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.601e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.341e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.321e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.405e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.094e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=9.898e-03
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.118e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.144e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.971e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.208e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.807e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=3.163e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.829e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.416e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.484e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.246e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.559e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.747e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.027e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.411e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.807e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.509e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=6.805e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.463e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.465e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.376e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=2.968e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.733e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.354e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=7.517e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.816e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.488e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.592e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.921e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.057e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.318e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.425e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.513e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.712e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.855e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.293e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=5.974e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.733e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.362e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.963e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.650e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.64it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.61it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.63it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.61it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.65it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.61it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.63it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.61it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.63it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.61it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.63it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.62it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.63it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.62it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.61it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.61it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.59it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.59it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.61it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.60it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.62it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.62it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.61it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.61it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.63it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]
Training epochs:  72%|███████▏  | 23/32 [39:05<16:04, 107.11s/it]
Epoch 24/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]
Epoch 23/32 - Train Loss: 17.197504 - Test Loss: 17.276614
Training epochs:  72%|███████▏  | 23/32 [39:05<16:04, 107.13s/it]
Epoch 24/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 24/32:   2%|▏         | 1/52 [00:01<01:40,  1.97s/it][A
Epoch 24/32:   2%|▏         | 1/52 [00:01<01:37,  1.91s/it][A
Epoch 24/32:   4%|▍         | 2/52 [00:03<01:34,  1.90s/it][A
Epoch 24/32:   4%|▍         | 2/52 [00:03<01:36,  1.93s/it][A
Epoch 24/32:   6%|▌         | 3/52 [00:05<01:32,  1.90s/it][A
Epoch 24/32:   6%|▌         | 3/52 [00:05<01:33,  1.91s/it][A
Epoch 24/32:   8%|▊         | 4/52 [00:07<01:31,  1.92s/it][A
Epoch 24/32:   8%|▊         | 4/52 [00:07<01:31,  1.91s/it][A

Epoch 24/32:  10%|▉         | 5/52 [00:09<01:30,  1.92s/it][AEpoch 24/32:  10%|▉         | 5/52 [00:09<01:30,  1.92s/it][A
Epoch 24/32:  12%|█▏        | 6/52 [00:11<01:28,  1.92s/it][A
Epoch 24/32:  12%|█▏        | 6/52 [00:11<01:28,  1.92s/it][A
Epoch 24/32:  13%|█▎        | 7/52 [00:13<01:26,  1.93s/it][A
Epoch 24/32:  13%|█▎        | 7/52 [00:13<01:26,  1.92s/it][A
Epoch 24/32:  15%|█▌        | 8/52 [00:15<01:24,  1.93s/it][A
Epoch 24/32:  15%|█▌        | 8/52 [00:15<01:24,  1.93s/it][A
Epoch 24/32:  17%|█▋        | 9/52 [00:17<01:23,  1.94s/it][A
Epoch 24/32:  17%|█▋        | 9/52 [00:17<01:23,  1.94s/it][A
Epoch 24/32:  19%|█▉        | 10/52 [00:19<01:21,  1.94s/it][A
Epoch 24/32:  19%|█▉        | 10/52 [00:19<01:21,  1.95s/it][A
Epoch 24/32:  21%|██        | 11/52 [00:21<01:19,  1.94s/it][A
Epoch 24/32:  21%|██        | 11/52 [00:21<01:19,  1.94s/it][A
Epoch 24/32:  23%|██▎       | 12/52 [00:23<01:19,  1.99s/it][A
Epoch 24/32:  23%|██▎       | 12/52 [00:23<01:19,  1.99s/it][A
Epoch 24/32:  25%|██▌       | 13/52 [00:25<01:17,  1.98s/it][A
Epoch 24/32:  25%|██▌       | 13/52 [00:25<01:17,  1.98s/it][A
Epoch 24/32:  27%|██▋       | 14/52 [00:27<01:15,  1.99s/it][A
Epoch 24/32:  27%|██▋       | 14/52 [00:27<01:15,  1.99s/it][A
Epoch 24/32:  29%|██▉       | 15/52 [00:29<01:13,  1.97s/it][A
Epoch 24/32:  29%|██▉       | 15/52 [00:29<01:13,  1.97s/it][A
Epoch 24/32:  31%|███       | 16/52 [00:31<01:10,  1.97s/it][A
Epoch 24/32:  31%|███       | 16/52 [00:31<01:10,  1.97s/it][A
Epoch 24/32:  33%|███▎      | 17/52 [00:33<01:08,  1.96s/it][A
Epoch 24/32:  33%|███▎      | 17/52 [00:33<01:08,  1.95s/it][A

Epoch 24/32:  35%|███▍      | 18/52 [00:35<01:06,  1.94s/it][AEpoch 24/32:  35%|███▍      | 18/52 [00:34<01:06,  1.94s/it][A
Epoch 24/32:  37%|███▋      | 19/52 [00:37<01:04,  1.94s/it][A
Epoch 24/32:  37%|███▋      | 19/52 [00:36<01:04,  1.95s/it][A
Epoch 24/32:  38%|███▊      | 20/52 [00:38<01:02,  1.94s/it][A
Epoch 24/32:  38%|███▊      | 20/52 [00:38<01:02,  1.94s/it][A
Epoch 24/32:  40%|████      | 21/52 [00:40<01:00,  1.94s/it][A
Epoch 24/32:  40%|████      | 21/52 [00:40<01:00,  1.94s/it][A
Epoch 24/32:  42%|████▏     | 22/52 [00:42<00:57,  1.93s/it][A
Epoch 24/32:  42%|████▏     | 22/52 [00:42<00:57,  1.93s/it][A
Epoch 24/32:  44%|████▍     | 23/52 [00:44<00:55,  1.93s/it][A
Epoch 24/32:  44%|████▍     | 23/52 [00:44<00:55,  1.93s/it][A
Epoch 24/32:  46%|████▌     | 24/52 [00:46<00:53,  1.93s/it][A
Epoch 24/32:  46%|████▌     | 24/52 [00:46<00:53,  1.93s/it][A
Epoch 24/32:  48%|████▊     | 25/52 [00:48<00:52,  1.93s/it][A
Epoch 24/32:  48%|████▊     | 25/52 [00:48<00:52,  1.93s/it][A
Epoch 24/32:  50%|█████     | 26/52 [00:50<00:50,  1.94s/it][A
Epoch 24/32:  50%|█████     | 26/52 [00:50<00:50,  1.94s/it][A
Epoch 24/32:  52%|█████▏    | 27/52 [00:52<00:48,  1.94s/it][A
Epoch 24/32:  52%|█████▏    | 27/52 [00:52<00:48,  1.94s/it][A
Epoch 24/32:  54%|█████▍    | 28/52 [00:54<00:46,  1.94s/it][A
Epoch 24/32:  54%|█████▍    | 28/52 [00:54<00:46,  1.94s/it][A
Epoch 24/32:  56%|█████▌    | 29/52 [00:56<00:44,  1.94s/it][A
Epoch 24/32:  56%|█████▌    | 29/52 [00:56<00:44,  1.94s/it][A
Epoch 24/32:  58%|█████▊    | 30/52 [00:58<00:42,  1.94s/it][A
Epoch 24/32:  58%|█████▊    | 30/52 [00:58<00:42,  1.94s/it][A
Epoch 24/32:  60%|█████▉    | 31/52 [01:00<00:40,  1.94s/it][A
Epoch 24/32:  60%|█████▉    | 31/52 [01:00<00:40,  1.94s/it][A
Epoch 24/32:  62%|██████▏   | 32/52 [01:02<00:39,  1.96s/it][A
Epoch 24/32:  62%|██████▏   | 32/52 [01:02<00:39,  1.96s/it][A
Epoch 24/32:  63%|██████▎   | 33/52 [01:04<00:37,  1.95s/it][A
Epoch 24/32:  63%|██████▎   | 33/52 [01:04<00:37,  1.95s/it][A
Epoch 24/32:  65%|██████▌   | 34/52 [01:06<00:34,  1.94s/it][A
Epoch 24/32:  65%|██████▌   | 34/52 [01:06<00:34,  1.94s/it][A
Epoch 24/32:  67%|██████▋   | 35/52 [01:07<00:33,  1.94s/it][A
Epoch 24/32:  67%|██████▋   | 35/52 [01:08<00:33,  1.94s/it][A
Epoch 24/32:  69%|██████▉   | 36/52 [01:09<00:31,  1.94s/it][A
Epoch 24/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.94s/it][A
Epoch 24/32:  71%|███████   | 37/52 [01:11<00:29,  1.94s/it][A
Epoch 24/32:  71%|███████   | 37/52 [01:11<00:29,  1.94s/it][A
Epoch 24/32:  73%|███████▎  | 38/52 [01:13<00:27,  1.94s/it][A
Epoch 24/32:  73%|███████▎  | 38/52 [01:13<00:27,  1.94s/it][A
Epoch 24/32:  75%|███████▌  | 39/52 [01:15<00:25,  1.95s/it][A
Epoch 24/32:  75%|███████▌  | 39/52 [01:15<00:25,  1.95s/it][A

Epoch 24/32:  77%|███████▋  | 40/52 [01:17<00:23,  1.95s/it][AEpoch 24/32:  77%|███████▋  | 40/52 [01:17<00:23,  1.95s/it][A

Epoch 24/32:  79%|███████▉  | 41/52 [01:19<00:21,  1.95s/it][AEpoch 24/32:  79%|███████▉  | 41/52 [01:19<00:21,  1.95s/it][A
Epoch 24/32:  81%|████████  | 42/52 [01:21<00:19,  1.95s/it][A
Epoch 24/32:  81%|████████  | 42/52 [01:21<00:19,  1.96s/it][A
Epoch 24/32:  83%|████████▎ | 43/52 [01:23<00:17,  1.95s/it][A
Epoch 24/32:  83%|████████▎ | 43/52 [01:23<00:17,  1.96s/it][A
Epoch 24/32:  85%|████████▍ | 44/52 [01:25<00:15,  1.95s/it][A
Epoch 24/32:  85%|████████▍ | 44/52 [01:25<00:15,  1.95s/it][A
Epoch 24/32:  87%|████████▋ | 45/52 [01:27<00:13,  1.96s/it][A
Epoch 24/32:  87%|████████▋ | 45/52 [01:27<00:13,  1.96s/it][A
Epoch 24/32:  88%|████████▊ | 46/52 [01:29<00:11,  1.95s/it][A
Epoch 24/32:  88%|████████▊ | 46/52 [01:29<00:11,  1.95s/it][A
Epoch 24/32:  90%|█████████ | 47/52 [01:31<00:09,  2.00s/it][A
Epoch 24/32:  90%|█████████ | 47/52 [01:31<00:09,  2.00s/it][A
Epoch 24/32:  92%|█████████▏| 48/52 [01:33<00:07,  1.98s/it][A
Epoch 24/32:  92%|█████████▏| 48/52 [01:33<00:07,  1.98s/it][A
Epoch 24/32:  94%|█████████▍| 49/52 [01:35<00:05,  1.97s/it][A
Epoch 24/32:  94%|█████████▍| 49/52 [01:35<00:05,  1.97s/it][A
Epoch 24/32:  96%|█████████▌| 50/52 [01:37<00:03,  1.96s/it][A
Epoch 24/32:  96%|█████████▌| 50/52 [01:37<00:03,  1.96s/it][A
Epoch 24/32:  98%|█████████▊| 51/52 [01:39<00:01,  1.95s/it][A
Epoch 24/32:  98%|█████████▊| 51/52 [01:39<00:01,  1.95s/it][A
Epoch 24/32: 100%|██████████| 52/52 [01:40<00:00,  1.83s/it][AEpoch 24/32: 100%|██████████| 52/52 [01:40<00:00,  1.94s/it]

[β=2.0] Epoch 24 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.099e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.361e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.317e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.066e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.634e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.401e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.845e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.210e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.318e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=6.007e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.630e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.142e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=8.499e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.713e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.872e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.564e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.277e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.323e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.684e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.795e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.077e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.299e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.974e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.185e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.283e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.511e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.273e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.609e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.068e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.421e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.007e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.423e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.157e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.305e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=8.143e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.654e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.494e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.493e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.306e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.561e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.453e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.874e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=6.905e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=8.870e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.069e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.172e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.511e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.495e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.641e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.431e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.500e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.378e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.753e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=7.362e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.685e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.283e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.820e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.322e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.235e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.239e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.377e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.290e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=7.914e-01
  layer model_0__forward_module.module.output_conv act-std: mean=1.987e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.863e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.222e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.992e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.270e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.435e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.241e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.883e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.843e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.448e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.675e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.458e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.396e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.119e-02
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=5.414e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.816e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.357e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=9.591e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.939e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.773e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.372e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=8.241e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.962e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.335e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.687e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.383e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.421e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.721e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.904e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.539e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.550e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.816e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.522e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=4.557e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.970e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.810e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=8.323e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.300e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.973e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.838e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.098e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.347e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.509e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.068e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.658e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.207e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.397e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.602e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.886e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.380e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.468e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.193e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.591e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.490e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.719e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.141e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.887e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.989e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.840e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.643e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.584e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.370e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.413e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.488e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.020e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.061e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.984e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.880e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.502e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.234e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.667e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.539e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.279e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.905e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.392e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.899e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.254e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.202e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.734e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.909e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.488e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.791e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.818e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.412e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.144e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.099e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.258e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.337e-02

  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.255e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.045e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.824e-03
Epoch 24/32: 100%|██████████| 52/52 [01:40<00:00,  1.83s/it]  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=4.945e-03[A
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.640e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.254e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.637e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.763e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.215e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.536e+00
Epoch 24/32: 100%|██████████| 52/52 [01:40<00:00,  1.94s/it]  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.299e-01

  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.613e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=2.143e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.291e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.456e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.898e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.223e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.622e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.626e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.462e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.385e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.465e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.869e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.408e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.272e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.435e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=3.186e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.866e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.033e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.207e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.883e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.604e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.249e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.324e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.191e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.374e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.557e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.015e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.165e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.874e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.038e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=5.358e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.324e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.338e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.340e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=3.180e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.441e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.843e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.237e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.366e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.144e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.168e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.402e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.045e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.244e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.296e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.207e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.258e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.925e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.451e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.114e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.827e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.393e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=2.944e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.193e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.63it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.62it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.61it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.60it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.62it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.61it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.61it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.60it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.60it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.60it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.61it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.60it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.60it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.60it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.60it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.60it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.62it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.59it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.61it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.58it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.58it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.56it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.56it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.54it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]

Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]
Epoch 24/32 - Train Loss: 17.147334 - Test Loss: 17.256675
Training epochs:  75%|███████▌  | 24/32 [40:54<14:21, 107.74s/it]
Epoch 25/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  75%|███████▌  | 24/32 [40:54<14:21, 107.74s/it]
Epoch 25/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 25/32:   2%|▏         | 1/52 [00:01<01:40,  1.97s/it][A
Epoch 25/32:   2%|▏         | 1/52 [00:01<01:38,  1.93s/it][A
Epoch 25/32:   4%|▍         | 2/52 [00:03<01:37,  1.95s/it][A
Epoch 25/32:   4%|▍         | 2/52 [00:03<01:36,  1.94s/it][A
Epoch 25/32:   6%|▌         | 3/52 [00:05<01:34,  1.93s/it][A
Epoch 25/32:   6%|▌         | 3/52 [00:05<01:35,  1.95s/it][A
Epoch 25/32:   8%|▊         | 4/52 [00:07<01:32,  1.93s/it][A
Epoch 25/32:   8%|▊         | 4/52 [00:07<01:32,  1.94s/it][A
Epoch 25/32:  10%|▉         | 5/52 [00:09<01:30,  1.93s/it][A
Epoch 25/32:  10%|▉         | 5/52 [00:09<01:30,  1.93s/it][A
Epoch 25/32:  12%|█▏        | 6/52 [00:11<01:29,  1.94s/it][A
Epoch 25/32:  12%|█▏        | 6/52 [00:11<01:29,  1.94s/it][A
Epoch 25/32:  13%|█▎        | 7/52 [00:13<01:27,  1.94s/it][A
Epoch 25/32:  13%|█▎        | 7/52 [00:13<01:27,  1.94s/it][A
Epoch 25/32:  15%|█▌        | 8/52 [00:15<01:25,  1.95s/it][A
Epoch 25/32:  15%|█▌        | 8/52 [00:15<01:25,  1.95s/it][A
Epoch 25/32:  17%|█▋        | 9/52 [00:17<01:24,  1.96s/it][A
Epoch 25/32:  17%|█▋        | 9/52 [00:17<01:24,  1.96s/it][A
Epoch 25/32:  19%|█▉        | 10/52 [00:19<01:21,  1.95s/it][A
Epoch 25/32:  19%|█▉        | 10/52 [00:19<01:21,  1.95s/it][A
Epoch 25/32:  21%|██        | 11/52 [00:21<01:20,  1.95s/it][A
Epoch 25/32:  21%|██        | 11/52 [00:21<01:20,  1.95s/it][A
Epoch 25/32:  23%|██▎       | 12/52 [00:23<01:18,  1.95s/it][A
Epoch 25/32:  23%|██▎       | 12/52 [00:23<01:18,  1.95s/it][A
Epoch 25/32:  25%|██▌       | 13/52 [00:25<01:16,  1.96s/it][A
Epoch 25/32:  25%|██▌       | 13/52 [00:25<01:16,  1.96s/it][A
Epoch 25/32:  27%|██▋       | 14/52 [00:27<01:14,  1.96s/it][A
Epoch 25/32:  27%|██▋       | 14/52 [00:27<01:14,  1.96s/it][A
Epoch 25/32:  29%|██▉       | 15/52 [00:29<01:12,  1.96s/it][A
Epoch 25/32:  29%|██▉       | 15/52 [00:29<01:12,  1.96s/it][A
Epoch 25/32:  31%|███       | 16/52 [00:31<01:10,  1.95s/it][A
Epoch 25/32:  31%|███       | 16/52 [00:31<01:10,  1.95s/it][A
Epoch 25/32:  33%|███▎      | 17/52 [00:33<01:08,  1.95s/it][A
Epoch 25/32:  33%|███▎      | 17/52 [00:33<01:08,  1.95s/it][A
Epoch 25/32:  35%|███▍      | 18/52 [00:35<01:06,  1.96s/it][A
Epoch 25/32:  35%|███▍      | 18/52 [00:35<01:06,  1.96s/it][A
Epoch 25/32:  37%|███▋      | 19/52 [00:37<01:04,  1.96s/it][A
Epoch 25/32:  37%|███▋      | 19/52 [00:37<01:04,  1.96s/it][A
Epoch 25/32:  38%|███▊      | 20/52 [00:39<01:02,  1.96s/it][A
Epoch 25/32:  38%|███▊      | 20/52 [00:39<01:02,  1.96s/it][A
Epoch 25/32:  40%|████      | 21/52 [00:40<01:00,  1.95s/it][A
Epoch 25/32:  40%|████      | 21/52 [00:40<01:00,  1.95s/it][A
Epoch 25/32:  42%|████▏     | 22/52 [00:42<00:58,  1.94s/it][A
Epoch 25/32:  42%|████▏     | 22/52 [00:42<00:58,  1.94s/it][A
Epoch 25/32:  44%|████▍     | 23/52 [00:44<00:56,  1.95s/it][A
Epoch 25/32:  44%|████▍     | 23/52 [00:44<00:56,  1.95s/it][A
Epoch 25/32:  46%|████▌     | 24/52 [00:46<00:54,  1.95s/it][A
Epoch 25/32:  46%|████▌     | 24/52 [00:46<00:54,  1.95s/it][A
Epoch 25/32:  48%|████▊     | 25/52 [00:48<00:52,  1.95s/it][A
Epoch 25/32:  48%|████▊     | 25/52 [00:48<00:52,  1.95s/it][A
Epoch 25/32:  50%|█████     | 26/52 [00:50<00:50,  1.94s/it][A
Epoch 25/32:  50%|█████     | 26/52 [00:50<00:50,  1.94s/it][A
Epoch 25/32:  52%|█████▏    | 27/52 [00:52<00:48,  1.95s/it][A
Epoch 25/32:  52%|█████▏    | 27/52 [00:52<00:48,  1.95s/it][A
Epoch 25/32:  54%|█████▍    | 28/52 [00:54<00:46,  1.94s/it][A
Epoch 25/32:  54%|█████▍    | 28/52 [00:54<00:46,  1.94s/it][A
Epoch 25/32:  56%|█████▌    | 29/52 [00:56<00:44,  1.94s/it][A
Epoch 25/32:  56%|█████▌    | 29/52 [00:56<00:44,  1.94s/it][A
Epoch 25/32:  58%|█████▊    | 30/52 [00:58<00:42,  1.95s/it][A
Epoch 25/32:  58%|█████▊    | 30/52 [00:58<00:42,  1.95s/it][A
Epoch 25/32:  60%|█████▉    | 31/52 [01:00<00:40,  1.94s/it][A
Epoch 25/32:  60%|█████▉    | 31/52 [01:00<00:40,  1.94s/it][A
Epoch 25/32:  62%|██████▏   | 32/52 [01:02<00:39,  1.95s/it][A
Epoch 25/32:  62%|██████▏   | 32/52 [01:02<00:39,  1.95s/it][A
Epoch 25/32:  63%|██████▎   | 33/52 [01:04<00:37,  1.95s/it][A
Epoch 25/32:  63%|██████▎   | 33/52 [01:04<00:36,  1.95s/it][A
Epoch 25/32:  65%|██████▌   | 34/52 [01:06<00:35,  1.95s/it][A
Epoch 25/32:  65%|██████▌   | 34/52 [01:06<00:35,  1.95s/it][A
Epoch 25/32:  67%|██████▋   | 35/52 [01:08<00:33,  1.95s/it][A
Epoch 25/32:  67%|██████▋   | 35/52 [01:08<00:33,  1.95s/it][A
Epoch 25/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.95s/it][A
Epoch 25/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.95s/it][A
Epoch 25/32:  71%|███████   | 37/52 [01:12<00:29,  1.95s/it][A
Epoch 25/32:  71%|███████   | 37/52 [01:12<00:29,  1.95s/it][A
Epoch 25/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.96s/it][A
Epoch 25/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.96s/it][A
Epoch 25/32:  75%|███████▌  | 39/52 [01:16<00:25,  1.95s/it][A
Epoch 25/32:  75%|███████▌  | 39/52 [01:15<00:25,  1.95s/it][A
Epoch 25/32:  77%|███████▋  | 40/52 [01:17<00:23,  1.94s/it][A
Epoch 25/32:  77%|███████▋  | 40/52 [01:17<00:23,  1.94s/it][A
Epoch 25/32:  79%|███████▉  | 41/52 [01:19<00:21,  1.93s/it][A
Epoch 25/32:  79%|███████▉  | 41/52 [01:19<00:21,  1.93s/it][A
Epoch 25/32:  81%|████████  | 42/52 [01:21<00:19,  1.94s/it][A
Epoch 25/32:  81%|████████  | 42/52 [01:21<00:19,  1.94s/it][A
Epoch 25/32:  83%|████████▎ | 43/52 [01:23<00:17,  1.95s/it][A
Epoch 25/32:  83%|████████▎ | 43/52 [01:23<00:17,  1.95s/it][A
Epoch 25/32:  85%|████████▍ | 44/52 [01:25<00:15,  1.95s/it][A
Epoch 25/32:  85%|████████▍ | 44/52 [01:25<00:15,  1.95s/it][A
Epoch 25/32:  87%|████████▋ | 45/52 [01:27<00:13,  1.95s/it][A
Epoch 25/32:  87%|████████▋ | 45/52 [01:27<00:13,  1.95s/it][A
Epoch 25/32:  88%|████████▊ | 46/52 [01:29<00:11,  1.95s/it][A
Epoch 25/32:  88%|████████▊ | 46/52 [01:29<00:11,  1.95s/it][A
Epoch 25/32:  90%|█████████ | 47/52 [01:31<00:09,  1.95s/it][A
Epoch 25/32:  90%|█████████ | 47/52 [01:31<00:09,  1.94s/it][A
Epoch 25/32:  92%|█████████▏| 48/52 [01:33<00:07,  1.95s/it][A
Epoch 25/32:  92%|█████████▏| 48/52 [01:33<00:07,  1.95s/it][A
Epoch 25/32:  94%|█████████▍| 49/52 [01:35<00:05,  1.95s/it][A
Epoch 25/32:  94%|█████████▍| 49/52 [01:35<00:05,  1.95s/it][A
Epoch 25/32:  96%|█████████▌| 50/52 [01:37<00:03,  1.95s/it][A
Epoch 25/32:  96%|█████████▌| 50/52 [01:37<00:03,  1.95s/it][A
Epoch 25/32:  98%|█████████▊| 51/52 [01:39<00:01,  1.95s/it][A
Epoch 25/32:  98%|█████████▊| 51/52 [01:39<00:01,  1.96s/it][A
Epoch 25/32: 100%|██████████| 52/52 [01:40<00:00,  1.84s/it][AEpoch 25/32: 100%|██████████| 52/52 [01:40<00:00,  1.94s/it]

Epoch 25/32: 100%|██████████| 52/52 [01:40<00:00,  1.84s/it][AEpoch 25/32: 100%|██████████| 52/52 [01:40<00:00,  1.94s/it]

[β=2.0] Epoch 25 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.137e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.349e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.303e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=2.986e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.351e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.456e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.891e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.162e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.349e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.952e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.463e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.052e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=8.165e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.666e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.876e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.505e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.320e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.295e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.646e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.767e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.080e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.318e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.987e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.165e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.316e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.494e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.341e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.418e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.080e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.377e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=3.991e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.413e-01

  layer model_3__forward_module.module.input_conv act-std: mean=1.187e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.375e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=8.468e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.655e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.489e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.359e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.219e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.531e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.481e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.907e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.132e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=8.817e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=2.023e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.160e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.458e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.473e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.662e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.556e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.577e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.341e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.708e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=7.116e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.553e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.248e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.887e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.381e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.314e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.211e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.371e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.262e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.013e-01
  layer model_0__forward_module.module.output_conv act-std: mean=1.953e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.499e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.097e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.640e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.146e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.293e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.925e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.471e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.589e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.260e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.600e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.241e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.196e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.004e-02
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=4.304e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.270e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.002e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=8.066e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.959e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.798e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.827e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=7.796e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.795e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.363e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.426e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.437e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.514e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.551e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.897e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.996e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.742e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.364e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.389e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=3.226e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.257e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.446e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=6.869e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.321e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.123e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.819e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.121e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.472e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.304e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.060e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.654e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.941e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.531e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.861e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.485e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.834e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.131e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.447e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.782e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=2.197e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.807e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.184e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=3.114e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=2.285e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.975e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.811e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.081e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.556e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.664e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.886e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.667e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=9.546e-02
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.788e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.514e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.253e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.716e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.997e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.651e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.558e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.935e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.442e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.734e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.554e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.470e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=2.061e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.761e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.817e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.146e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.773e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.211e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.544e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.330e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.556e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.567e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.081e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.482e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.044e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=5.536e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.900e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.462e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.666e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=3.156e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=2.213e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.733e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.796e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.498e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.912e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.973e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.109e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.760e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.054e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.876e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.458e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.506e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.936e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=2.115e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=4.143e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.416e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.267e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.429e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.749e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.628e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.841e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.704e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.679e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.280e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.597e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.556e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.306e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.818e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.740e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.044e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.936e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.998e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.118e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=6.021e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.547e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.572e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.532e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=3.442e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.953e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=4.147e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=9.508e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=3.457e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=3.242e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.418e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.740e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.201e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.689e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.227e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.304e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.340e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.304e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=3.546e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=8.874e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=2.044e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.536e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.125e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.146e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.62it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.58it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.61it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.59it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.62it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.60it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.63it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.59it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.63it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.59it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.63it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.59it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.62it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.60it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.62it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.60it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.62it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.59it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.62it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.59it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.61it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.59it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.58it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.56it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]
Training epochs:  78%|███████▊  | 25/32 [42:43<12:37, 108.17s/it]
Epoch 26/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]
Epoch 25/32 - Train Loss: 17.115772 - Test Loss: 17.201832
Training epochs:  78%|███████▊  | 25/32 [42:43<12:37, 108.19s/it]
Epoch 26/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 26/32:   2%|▏         | 1/52 [00:02<01:46,  2.09s/it][A
Epoch 26/32:   2%|▏         | 1/52 [00:01<01:39,  1.96s/it][A
Epoch 26/32:   4%|▍         | 2/52 [00:04<01:44,  2.09s/it][A
Epoch 26/32:   4%|▍         | 2/52 [00:04<01:41,  2.04s/it][A
Epoch 26/32:   6%|▌         | 3/52 [00:06<01:37,  2.00s/it][A
Epoch 26/32:   6%|▌         | 3/52 [00:06<01:39,  2.03s/it][A
Epoch 26/32:   8%|▊         | 4/52 [00:08<01:36,  2.01s/it][A
Epoch 26/32:   8%|▊         | 4/52 [00:08<01:37,  2.03s/it][A
Epoch 26/32:  10%|▉         | 5/52 [00:10<01:34,  2.01s/it][A
Epoch 26/32:  10%|▉         | 5/52 [00:10<01:34,  2.00s/it][A
Epoch 26/32:  12%|█▏        | 6/52 [00:11<01:31,  1.99s/it][A
Epoch 26/32:  12%|█▏        | 6/52 [00:12<01:31,  1.99s/it][A
Epoch 26/32:  13%|█▎        | 7/52 [00:13<01:28,  1.97s/it][A
Epoch 26/32:  13%|█▎        | 7/52 [00:14<01:28,  1.98s/it][A
Epoch 26/32:  15%|█▌        | 8/52 [00:15<01:26,  1.97s/it][A
Epoch 26/32:  15%|█▌        | 8/52 [00:16<01:26,  1.97s/it][A
Epoch 26/32:  17%|█▋        | 9/52 [00:17<01:24,  1.96s/it][A
Epoch 26/32:  17%|█▋        | 9/52 [00:17<01:24,  1.96s/it][A
Epoch 26/32:  19%|█▉        | 10/52 [00:19<01:22,  1.96s/it][A
Epoch 26/32:  19%|█▉        | 10/52 [00:19<01:22,  1.96s/it][A
Epoch 26/32:  21%|██        | 11/52 [00:21<01:20,  1.95s/it][A
Epoch 26/32:  21%|██        | 11/52 [00:21<01:20,  1.95s/it][A
Epoch 26/32:  23%|██▎       | 12/52 [00:23<01:18,  1.96s/it][A
Epoch 26/32:  23%|██▎       | 12/52 [00:23<01:18,  1.96s/it][A
Epoch 26/32:  25%|██▌       | 13/52 [00:25<01:16,  1.96s/it][A
Epoch 26/32:  25%|██▌       | 13/52 [00:25<01:16,  1.96s/it][A
Epoch 26/32:  27%|██▋       | 14/52 [00:27<01:14,  1.96s/it][A
Epoch 26/32:  27%|██▋       | 14/52 [00:27<01:14,  1.96s/it][A
Epoch 26/32:  29%|██▉       | 15/52 [00:29<01:12,  1.96s/it][A
Epoch 26/32:  29%|██▉       | 15/52 [00:29<01:12,  1.96s/it][A
Epoch 26/32:  31%|███       | 16/52 [00:31<01:10,  1.96s/it][A
Epoch 26/32:  31%|███       | 16/52 [00:31<01:10,  1.96s/it][A
Epoch 26/32:  33%|███▎      | 17/52 [00:33<01:08,  1.96s/it][A
Epoch 26/32:  33%|███▎      | 17/52 [00:33<01:08,  1.96s/it][A
Epoch 26/32:  35%|███▍      | 18/52 [00:35<01:06,  1.96s/it][A
Epoch 26/32:  35%|███▍      | 18/52 [00:35<01:06,  1.96s/it][A
Epoch 26/32:  37%|███▋      | 19/52 [00:37<01:04,  1.96s/it][A
Epoch 26/32:  37%|███▋      | 19/52 [00:37<01:04,  1.96s/it][A
Epoch 26/32:  38%|███▊      | 20/52 [00:39<01:02,  1.96s/it][A
Epoch 26/32:  38%|███▊      | 20/52 [00:39<01:02,  1.96s/it][A
Epoch 26/32:  40%|████      | 21/52 [00:41<01:00,  1.96s/it][A
Epoch 26/32:  40%|████      | 21/52 [00:41<01:00,  1.96s/it][A
Epoch 26/32:  42%|████▏     | 22/52 [00:43<00:58,  1.96s/it][A
Epoch 26/32:  42%|████▏     | 22/52 [00:43<00:58,  1.96s/it][A
Epoch 26/32:  44%|████▍     | 23/52 [00:45<00:56,  1.96s/it][A
Epoch 26/32:  44%|████▍     | 23/52 [00:45<00:56,  1.96s/it][A
Epoch 26/32:  46%|████▌     | 24/52 [00:47<00:55,  1.96s/it][A
Epoch 26/32:  46%|████▌     | 24/52 [00:47<00:55,  1.97s/it][A
Epoch 26/32:  48%|████▊     | 25/52 [00:49<00:53,  1.96s/it][A
Epoch 26/32:  48%|████▊     | 25/52 [00:49<00:53,  1.96s/it][A
Epoch 26/32:  50%|█████     | 26/52 [00:51<00:51,  1.97s/it][A
Epoch 26/32:  50%|█████     | 26/52 [00:51<00:51,  1.97s/it][A
Epoch 26/32:  52%|█████▏    | 27/52 [00:53<00:48,  1.96s/it][A
Epoch 26/32:  52%|█████▏    | 27/52 [00:53<00:49,  1.96s/it][A
Epoch 26/32:  54%|█████▍    | 28/52 [00:55<00:46,  1.96s/it][A
Epoch 26/32:  54%|█████▍    | 28/52 [00:55<00:47,  1.96s/it][A
Epoch 26/32:  56%|█████▌    | 29/52 [00:57<00:44,  1.95s/it][A
Epoch 26/32:  56%|█████▌    | 29/52 [00:57<00:44,  1.95s/it][A
Epoch 26/32:  58%|█████▊    | 30/52 [00:58<00:43,  1.96s/it][A
Epoch 26/32:  58%|█████▊    | 30/52 [00:59<00:43,  1.96s/it][A
Epoch 26/32:  60%|█████▉    | 31/52 [01:00<00:41,  1.96s/it][A
Epoch 26/32:  60%|█████▉    | 31/52 [01:01<00:41,  1.96s/it][A
Epoch 26/32:  62%|██████▏   | 32/52 [01:02<00:39,  1.96s/it][A
Epoch 26/32:  62%|██████▏   | 32/52 [01:03<00:39,  1.96s/it][A
Epoch 26/32:  63%|██████▎   | 33/52 [01:05<00:38,  2.01s/it][A
Epoch 26/32:  63%|██████▎   | 33/52 [01:05<00:38,  2.01s/it][A
Epoch 26/32:  65%|██████▌   | 34/52 [01:06<00:35,  1.99s/it][A
Epoch 26/32:  65%|██████▌   | 34/52 [01:07<00:35,  1.99s/it][A
Epoch 26/32:  67%|██████▋   | 35/52 [01:08<00:33,  1.98s/it][A
Epoch 26/32:  67%|██████▋   | 35/52 [01:09<00:33,  1.98s/it][A
Epoch 26/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.97s/it][A
Epoch 26/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.97s/it][A
Epoch 26/32:  71%|███████   | 37/52 [01:12<00:29,  1.97s/it][A
Epoch 26/32:  71%|███████   | 37/52 [01:12<00:29,  1.97s/it][A
Epoch 26/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.97s/it][A
Epoch 26/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.97s/it][A
Epoch 26/32:  75%|███████▌  | 39/52 [01:16<00:25,  1.97s/it][A
Epoch 26/32:  75%|███████▌  | 39/52 [01:16<00:25,  1.97s/it][A
Epoch 26/32:  77%|███████▋  | 40/52 [01:18<00:23,  1.96s/it][A
Epoch 26/32:  77%|███████▋  | 40/52 [01:18<00:23,  1.97s/it][A
Epoch 26/32:  79%|███████▉  | 41/52 [01:20<00:21,  1.96s/it][A
Epoch 26/32:  79%|███████▉  | 41/52 [01:20<00:21,  1.96s/it][A
Epoch 26/32:  81%|████████  | 42/52 [01:22<00:19,  1.96s/it][A
Epoch 26/32:  81%|████████  | 42/52 [01:22<00:19,  1.97s/it][A
Epoch 26/32:  83%|████████▎ | 43/52 [01:24<00:17,  1.96s/it][A
Epoch 26/32:  83%|████████▎ | 43/52 [01:24<00:17,  1.96s/it][A
Epoch 26/32:  85%|████████▍ | 44/52 [01:26<00:15,  1.96s/it][A
Epoch 26/32:  85%|████████▍ | 44/52 [01:26<00:15,  1.96s/it][A
Epoch 26/32:  87%|████████▋ | 45/52 [01:28<00:13,  1.97s/it][A
Epoch 26/32:  87%|████████▋ | 45/52 [01:28<00:13,  1.97s/it][A
Epoch 26/32:  88%|████████▊ | 46/52 [01:30<00:11,  1.96s/it][A
Epoch 26/32:  88%|████████▊ | 46/52 [01:30<00:11,  1.96s/it][A
Epoch 26/32:  90%|█████████ | 47/52 [01:32<00:09,  1.95s/it][A
Epoch 26/32:  90%|█████████ | 47/52 [01:32<00:09,  1.96s/it][A
Epoch 26/32:  92%|█████████▏| 48/52 [01:34<00:07,  1.96s/it][A
Epoch 26/32:  92%|█████████▏| 48/52 [01:34<00:07,  1.96s/it][A
Epoch 26/32:  94%|█████████▍| 49/52 [01:36<00:05,  1.95s/it][A
Epoch 26/32:  94%|█████████▍| 49/52 [01:36<00:05,  1.96s/it][A
Epoch 26/32:  96%|█████████▌| 50/52 [01:38<00:03,  1.95s/it][A
Epoch 26/32:  96%|█████████▌| 50/52 [01:38<00:03,  1.95s/it][A
Epoch 26/32:  98%|█████████▊| 51/52 [01:40<00:01,  1.95s/it][A
Epoch 26/32:  98%|█████████▊| 51/52 [01:40<00:01,  1.95s/it][A
Epoch 26/32: 100%|██████████| 52/52 [01:41<00:00,  1.84s/it][AEpoch 26/32: 100%|██████████| 52/52 [01:41<00:00,  1.96s/it]

Epoch 26/32: 100%|██████████| 52/52 [01:41<00:00,  1.84s/it][AEpoch 26/32: 100%|██████████| 52/52 [01:41<00:00,  1.96s/it]

[β=2.0] Epoch 26 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.157e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.367e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.349e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.074e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.611e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.596e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.954e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.168e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.382e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.898e-01

  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.336e-01
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=3.022e-01[A
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=8.040e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.646e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.874e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.478e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.338e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.264e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.657e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.560e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.042e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.247e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.971e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.143e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.356e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.567e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.499e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.456e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.082e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.392e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.014e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.432e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.225e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.387e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=8.733e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.643e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.456e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.228e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.148e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.498e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.503e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.913e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.292e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=8.596e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=1.914e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.133e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.330e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.444e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.686e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.765e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.777e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.275e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.665e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.866e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.354e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.219e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.927e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.481e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.438e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.111e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.396e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.238e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.091e-01
  layer model_0__forward_module.module.output_conv act-std: mean=1.931e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.188e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.093e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.602e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.153e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.156e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.122e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.544e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.449e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.496e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.901e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.749e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.747e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.943e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=3.919e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.117e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.684e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=6.871e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.954e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.797e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.455e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=6.996e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.535e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.327e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.175e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.342e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.210e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.421e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.923e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.653e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.126e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.807e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.198e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.470e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=1.876e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.134e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=5.655e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.171e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.080e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.752e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.111e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.380e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.381e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.038e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.050e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.208e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.829e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.730e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.291e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.527e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.829e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.406e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.071e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.884e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.771e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.209e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=3.204e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=2.359e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.806e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.976e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.451e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.829e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.887e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.066e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.479e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.127e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.116e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.694e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.092e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.807e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.094e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.410e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.386e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.942e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.478e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=3.001e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.727e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.195e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.871e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.184e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.698e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.930e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.823e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.111e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.678e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.111e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.423e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.322e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.842e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.042e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.731e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=5.222e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.805e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.387e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.729e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.851e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=1.727e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.344e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.698e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.026e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.444e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.369e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.550e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.490e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.833e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.582e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.085e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.326e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.620e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.715e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=3.562e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.215e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.111e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.147e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.574e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.792e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.290e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.868e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=3.203e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.912e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.063e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.836e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.326e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.784e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.830e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.282e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.267e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.943e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.479e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=7.000e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.397e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.426e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.511e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=3.171e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.518e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.894e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.412e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.558e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.291e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.535e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.522e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.083e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.331e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.823e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.560e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.370e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.993e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.571e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.076e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.832e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.340e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.188e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.034e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.61it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.61it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.60it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.60it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.60it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.60it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.59it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.60it/s][A

Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.58it/s][AEvaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.59it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.60it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.60it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.60it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.60it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.61it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.59it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.61it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.59it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.60it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.59it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.60it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.59it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.59it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.59it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]

Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]
Epoch 26/32 - Train Loss: 17.066504 - Test Loss: 17.173975
Training epochs:  81%|████████▏ | 26/32 [44:33<10:52, 108.76s/it]
Epoch 27/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  81%|████████▏ | 26/32 [44:33<10:52, 108.75s/it]
Epoch 27/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 27/32:   2%|▏         | 1/52 [00:01<01:38,  1.94s/it][A
Epoch 27/32:   2%|▏         | 1/52 [00:01<01:38,  1.93s/it][A
Epoch 27/32:   4%|▍         | 2/52 [00:03<01:36,  1.93s/it][A
Epoch 27/32:   4%|▍         | 2/52 [00:03<01:36,  1.92s/it][A
Epoch 27/32:   6%|▌         | 3/52 [00:05<01:34,  1.93s/it][A
Epoch 27/32:   6%|▌         | 3/52 [00:05<01:34,  1.92s/it][A
Epoch 27/32:   8%|▊         | 4/52 [00:07<01:32,  1.92s/it][A
Epoch 27/32:   8%|▊         | 4/52 [00:07<01:32,  1.93s/it][A
Epoch 27/32:  10%|▉         | 5/52 [00:09<01:30,  1.93s/it][A
Epoch 27/32:  10%|▉         | 5/52 [00:09<01:30,  1.93s/it][A
Epoch 27/32:  12%|█▏        | 6/52 [00:11<01:30,  1.96s/it][A
Epoch 27/32:  12%|█▏        | 6/52 [00:11<01:30,  1.96s/it][A
Epoch 27/32:  13%|█▎        | 7/52 [00:13<01:27,  1.96s/it][A
Epoch 27/32:  13%|█▎        | 7/52 [00:13<01:27,  1.96s/it][A
Epoch 27/32:  15%|█▌        | 8/52 [00:15<01:25,  1.95s/it][A
Epoch 27/32:  15%|█▌        | 8/52 [00:15<01:25,  1.95s/it][A
Epoch 27/32:  17%|█▋        | 9/52 [00:17<01:25,  1.99s/it][A
Epoch 27/32:  17%|█▋        | 9/52 [00:17<01:25,  1.99s/it][A
Epoch 27/32:  19%|█▉        | 10/52 [00:19<01:22,  1.97s/it][A
Epoch 27/32:  19%|█▉        | 10/52 [00:19<01:22,  1.97s/it][A
Epoch 27/32:  21%|██        | 11/52 [00:21<01:20,  1.96s/it][A
Epoch 27/32:  21%|██        | 11/52 [00:21<01:20,  1.97s/it][A
Epoch 27/32:  23%|██▎       | 12/52 [00:23<01:18,  1.96s/it][A
Epoch 27/32:  23%|██▎       | 12/52 [00:23<01:18,  1.96s/it][A
Epoch 27/32:  25%|██▌       | 13/52 [00:25<01:16,  1.95s/it][A
Epoch 27/32:  25%|██▌       | 13/52 [00:25<01:16,  1.95s/it][A
Epoch 27/32:  27%|██▋       | 14/52 [00:27<01:14,  1.95s/it][A
Epoch 27/32:  27%|██▋       | 14/52 [00:27<01:14,  1.95s/it][A
Epoch 27/32:  29%|██▉       | 15/52 [00:29<01:12,  1.95s/it][A
Epoch 27/32:  29%|██▉       | 15/52 [00:29<01:12,  1.95s/it][A
Epoch 27/32:  31%|███       | 16/52 [00:31<01:10,  1.95s/it][A
Epoch 27/32:  31%|███       | 16/52 [00:31<01:10,  1.95s/it][A
Epoch 27/32:  33%|███▎      | 17/52 [00:33<01:08,  1.94s/it][A
Epoch 27/32:  33%|███▎      | 17/52 [00:33<01:08,  1.94s/it][A
Epoch 27/32:  35%|███▍      | 18/52 [00:35<01:06,  1.95s/it][A
Epoch 27/32:  35%|███▍      | 18/52 [00:35<01:06,  1.95s/it][A
Epoch 27/32:  37%|███▋      | 19/52 [00:37<01:04,  1.96s/it][A
Epoch 27/32:  37%|███▋      | 19/52 [00:37<01:04,  1.96s/it][A
Epoch 27/32:  38%|███▊      | 20/52 [00:39<01:02,  1.97s/it][A
Epoch 27/32:  38%|███▊      | 20/52 [00:39<01:02,  1.97s/it][A
Epoch 27/32:  40%|████      | 21/52 [00:41<01:00,  1.97s/it][A
Epoch 27/32:  40%|████      | 21/52 [00:41<01:00,  1.96s/it][A
Epoch 27/32:  42%|████▏     | 22/52 [00:43<00:58,  1.96s/it][A
Epoch 27/32:  42%|████▏     | 22/52 [00:43<00:58,  1.96s/it][A
Epoch 27/32:  44%|████▍     | 23/52 [00:44<00:56,  1.95s/it][A
Epoch 27/32:  44%|████▍     | 23/52 [00:44<00:56,  1.95s/it][A
Epoch 27/32:  46%|████▌     | 24/52 [00:46<00:54,  1.95s/it][A
Epoch 27/32:  46%|████▌     | 24/52 [00:46<00:54,  1.95s/it][A
Epoch 27/32:  48%|████▊     | 25/52 [00:48<00:52,  1.95s/it][A
Epoch 27/32:  48%|████▊     | 25/52 [00:48<00:52,  1.95s/it][A
Epoch 27/32:  50%|█████     | 26/52 [00:50<00:50,  1.95s/it][A
Epoch 27/32:  50%|█████     | 26/52 [00:50<00:50,  1.95s/it][A
Epoch 27/32:  52%|█████▏    | 27/52 [00:52<00:48,  1.95s/it][A
Epoch 27/32:  52%|█████▏    | 27/52 [00:52<00:48,  1.95s/it][A
Epoch 27/32:  54%|█████▍    | 28/52 [00:54<00:47,  1.97s/it][A
Epoch 27/32:  54%|█████▍    | 28/52 [00:54<00:47,  1.97s/it][A
Epoch 27/32:  56%|█████▌    | 29/52 [00:56<00:45,  1.99s/it][A
Epoch 27/32:  56%|█████▌    | 29/52 [00:56<00:45,  1.99s/it][A
Epoch 27/32:  58%|█████▊    | 30/52 [00:58<00:44,  2.01s/it][A
Epoch 27/32:  58%|█████▊    | 30/52 [00:58<00:44,  2.01s/it][A
Epoch 27/32:  60%|█████▉    | 31/52 [01:00<00:42,  2.02s/it][A
Epoch 27/32:  60%|█████▉    | 31/52 [01:00<00:42,  2.02s/it][A
Epoch 27/32:  62%|██████▏   | 32/52 [01:02<00:40,  2.01s/it][A
Epoch 27/32:  62%|██████▏   | 32/52 [01:02<00:40,  2.01s/it][A
Epoch 27/32:  63%|██████▎   | 33/52 [01:04<00:38,  2.00s/it][A
Epoch 27/32:  63%|██████▎   | 33/52 [01:04<00:38,  2.00s/it][A
Epoch 27/32:  65%|██████▌   | 34/52 [01:06<00:35,  1.99s/it][A
Epoch 27/32:  65%|██████▌   | 34/52 [01:06<00:35,  1.99s/it][A
Epoch 27/32:  67%|██████▋   | 35/52 [01:08<00:33,  1.97s/it][A
Epoch 27/32:  67%|██████▋   | 35/52 [01:08<00:33,  1.97s/it][A
Epoch 27/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.96s/it][A
Epoch 27/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.96s/it][A
Epoch 27/32:  71%|███████   | 37/52 [01:12<00:29,  1.96s/it][A
Epoch 27/32:  71%|███████   | 37/52 [01:12<00:29,  1.96s/it][A
Epoch 27/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.96s/it][A
Epoch 27/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.96s/it][A
Epoch 27/32:  75%|███████▌  | 39/52 [01:16<00:25,  1.96s/it][A
Epoch 27/32:  75%|███████▌  | 39/52 [01:16<00:25,  1.96s/it][A
Epoch 27/32:  77%|███████▋  | 40/52 [01:18<00:23,  1.96s/it][A
Epoch 27/32:  77%|███████▋  | 40/52 [01:18<00:23,  1.96s/it][A
Epoch 27/32:  79%|███████▉  | 41/52 [01:20<00:21,  1.95s/it][A
Epoch 27/32:  79%|███████▉  | 41/52 [01:20<00:21,  1.95s/it][A
Epoch 27/32:  81%|████████  | 42/52 [01:22<00:19,  1.95s/it][A
Epoch 27/32:  81%|████████  | 42/52 [01:22<00:19,  1.95s/it][A
Epoch 27/32:  83%|████████▎ | 43/52 [01:24<00:17,  1.94s/it][A
Epoch 27/32:  83%|████████▎ | 43/52 [01:24<00:17,  1.94s/it][A

Epoch 27/32:  85%|████████▍ | 44/52 [01:26<00:15,  1.94s/it][AEpoch 27/32:  85%|████████▍ | 44/52 [01:26<00:15,  1.94s/it][A
Epoch 27/32:  87%|████████▋ | 45/52 [01:28<00:13,  1.94s/it][A
Epoch 27/32:  87%|████████▋ | 45/52 [01:28<00:13,  1.94s/it][A
Epoch 27/32:  88%|████████▊ | 46/52 [01:30<00:11,  1.94s/it][A
Epoch 27/32:  88%|████████▊ | 46/52 [01:30<00:11,  1.94s/it][A
Epoch 27/32:  90%|█████████ | 47/52 [01:32<00:09,  1.94s/it][A
Epoch 27/32:  90%|█████████ | 47/52 [01:32<00:09,  1.95s/it][A
Epoch 27/32:  92%|█████████▏| 48/52 [01:33<00:07,  1.94s/it][A
Epoch 27/32:  92%|█████████▏| 48/52 [01:34<00:07,  1.94s/it][A
Epoch 27/32:  94%|█████████▍| 49/52 [01:35<00:05,  1.93s/it][A
Epoch 27/32:  94%|█████████▍| 49/52 [01:35<00:05,  1.94s/it][A
Epoch 27/32:  96%|█████████▌| 50/52 [01:37<00:03,  1.93s/it][A
Epoch 27/32:  96%|█████████▌| 50/52 [01:37<00:03,  1.94s/it][A
Epoch 27/32:  98%|█████████▊| 51/52 [01:39<00:01,  1.94s/it][A
Epoch 27/32:  98%|█████████▊| 51/52 [01:39<00:01,  1.94s/it][A
Epoch 27/32: 100%|██████████| 52/52 [01:41<00:00,  1.84s/it][AEpoch 27/32: 100%|██████████| 52/52 [01:41<00:00,  1.95s/it]

[β=2.0] Epoch 27 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.183e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.332e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.306e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.074e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.563e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.637e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=1.974e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.157e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.443e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.832e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.073e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.924e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=7.712e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.585e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.861e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.413e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.373e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.211e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.695e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.546e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.034e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.264e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.973e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.127e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.385e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.558e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.634e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.213e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.086e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.347e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.004e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.418e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.256e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.441e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=8.937e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.670e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.448e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=6.130e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=3.077e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.473e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.529e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.943e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.435e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=8.399e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=1.859e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.109e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.206e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.423e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.724e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=1.912e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=2.992e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.209e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.621e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.640e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.166e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.176e-01
  layer model_0__forward_module.module.input_conv act-std: mean=1.981e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.602e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.591e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.192e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.395e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.210e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.160e-01
  layer model_0__forward_module.module.output_conv act-std: mean=1.904e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.460e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.062e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.478e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.153e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.214e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.947e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.353e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.458e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.767e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.210e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.991e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.028e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.144e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=4.227e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.300e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.848e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.576e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.901e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.655e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.718e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=9.421e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.265e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.532e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.504e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.115e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.243e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.426e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.299e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.416e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.981e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.498e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.406e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=3.360e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.753e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.548e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.915e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.338e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.939e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.478e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.023e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.117e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.379e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=9.880e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.997e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.028e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.472e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.465e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.910e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.299e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.475e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.227e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.225e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=2.034e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.375e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.056e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.958e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.982e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.886e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=2.050e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=4.688e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.984e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.923e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.243e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.785e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.137e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.101e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.717e-02

  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.056e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.852e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.045e-02
Epoch 27/32: 100%|██████████| 52/52 [01:41<00:00,  1.84s/it]  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.791e-03[A
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=4.030e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.789e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.385e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.911e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.536e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.625e-01
Epoch 27/32: 100%|██████████| 52/52 [01:41<00:00,  1.95s/it]  param model_4__forward_module.module.input_conv.weight grad-norm: mean=2.102e+00

  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.869e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.903e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.308e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.841e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.231e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=4.884e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.302e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.811e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.688e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.654e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.569e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=3.208e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=6.212e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.928e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.494e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.859e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=3.456e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=1.703e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.582e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.379e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.320e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.606e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.952e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.067e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.493e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.836e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.705e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.004e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.546e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.593e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.651e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=3.472e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.330e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.198e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.329e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.455e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=2.196e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.623e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=5.816e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=3.980e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=3.819e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.881e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.623e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.467e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.545e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.480e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.776e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.361e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.465e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.909e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=8.369e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.607e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.662e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.776e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=4.168e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.588e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.159e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.964e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.661e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.430e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=8.106e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.209e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.077e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.392e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.835e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.782e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.022e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.068e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.873e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.849e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.874e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.353e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.179e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.028e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.57it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:09,  1.21it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:07,  1.56it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:08,  1.33it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.49it/s][A
Evaluating:  23%|██▎       | 3/13 [00:02<00:07,  1.40it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:06,  1.48it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:06,  1.46it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.50it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.49it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.52it/s][A
Evaluating:  46%|████▌     | 6/13 [00:04<00:04,  1.51it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.53it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.53it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.55it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.54it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.52it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:06<00:02,  1.53it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.54it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.54it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.54it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.55it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.54it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.55it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s]
Epoch 27/32 - Train Loss: 17.039031 - Test Loss: 17.141866
Training epochs:  84%|████████▍ | 27/32 [46:23<09:05, 109.11s/it]
Epoch 28/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.50it/s]
Training epochs:  84%|████████▍ | 27/32 [46:23<09:05, 109.18s/it]
Epoch 28/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 28/32:   2%|▏         | 1/52 [00:02<01:47,  2.11s/it][A
Epoch 28/32:   2%|▏         | 1/52 [00:01<01:38,  1.94s/it][A
Epoch 28/32:   4%|▍         | 2/52 [00:03<01:36,  1.94s/it][A
Epoch 28/32:   4%|▍         | 2/52 [00:04<01:40,  2.02s/it][A
Epoch 28/32:   6%|▌         | 3/52 [00:05<01:35,  1.94s/it][A
Epoch 28/32:   6%|▌         | 3/52 [00:06<01:37,  1.98s/it][A
Epoch 28/32:   8%|▊         | 4/52 [00:07<01:33,  1.96s/it][A
Epoch 28/32:   8%|▊         | 4/52 [00:07<01:35,  1.98s/it][A
Epoch 28/32:  10%|▉         | 5/52 [00:09<01:32,  1.96s/it][A
Epoch 28/32:  10%|▉         | 5/52 [00:09<01:32,  1.97s/it][A
Epoch 28/32:  12%|█▏        | 6/52 [00:11<01:30,  1.96s/it][A
Epoch 28/32:  12%|█▏        | 6/52 [00:11<01:30,  1.97s/it][A
Epoch 28/32:  13%|█▎        | 7/52 [00:13<01:28,  1.96s/it][A
Epoch 28/32:  13%|█▎        | 7/52 [00:13<01:28,  1.96s/it][A
Epoch 28/32:  15%|█▌        | 8/52 [00:15<01:25,  1.95s/it][A
Epoch 28/32:  15%|█▌        | 8/52 [00:15<01:25,  1.95s/it][A
Epoch 28/32:  17%|█▋        | 9/52 [00:17<01:23,  1.94s/it][A
Epoch 28/32:  17%|█▋        | 9/52 [00:17<01:23,  1.95s/it][A
Epoch 28/32:  19%|█▉        | 10/52 [00:19<01:21,  1.94s/it][A
Epoch 28/32:  19%|█▉        | 10/52 [00:19<01:21,  1.95s/it][A
Epoch 28/32:  21%|██        | 11/52 [00:21<01:20,  1.95s/it][A
Epoch 28/32:  21%|██        | 11/52 [00:21<01:20,  1.96s/it][A
Epoch 28/32:  23%|██▎       | 12/52 [00:23<01:18,  1.97s/it][A
Epoch 28/32:  23%|██▎       | 12/52 [00:23<01:18,  1.97s/it][A
Epoch 28/32:  25%|██▌       | 13/52 [00:25<01:17,  1.97s/it][A
Epoch 28/32:  25%|██▌       | 13/52 [00:25<01:17,  1.98s/it][A
Epoch 28/32:  27%|██▋       | 14/52 [00:27<01:15,  1.99s/it][A
Epoch 28/32:  27%|██▋       | 14/52 [00:27<01:15,  2.00s/it][A
Epoch 28/32:  29%|██▉       | 15/52 [00:29<01:13,  2.00s/it][A
Epoch 28/32:  29%|██▉       | 15/52 [00:29<01:13,  2.00s/it][A
Epoch 28/32:  31%|███       | 16/52 [00:31<01:11,  2.00s/it][A
Epoch 28/32:  31%|███       | 16/52 [00:31<01:11,  2.00s/it][A
Epoch 28/32:  33%|███▎      | 17/52 [00:33<01:09,  1.99s/it][A
Epoch 28/32:  33%|███▎      | 17/52 [00:33<01:09,  1.99s/it][A
Epoch 28/32:  35%|███▍      | 18/52 [00:35<01:07,  1.98s/it][A
Epoch 28/32:  35%|███▍      | 18/52 [00:35<01:07,  1.98s/it][A
Epoch 28/32:  37%|███▋      | 19/52 [00:37<01:06,  2.00s/it][A
Epoch 28/32:  37%|███▋      | 19/52 [00:37<01:06,  2.00s/it][A
Epoch 28/32:  38%|███▊      | 20/52 [00:39<01:04,  2.00s/it][A
Epoch 28/32:  38%|███▊      | 20/52 [00:39<01:04,  2.01s/it][A
Epoch 28/32:  40%|████      | 21/52 [00:41<01:02,  2.00s/it][A
Epoch 28/32:  40%|████      | 21/52 [00:41<01:02,  2.00s/it][A
Epoch 28/32:  42%|████▏     | 22/52 [00:43<00:59,  1.99s/it][A
Epoch 28/32:  42%|████▏     | 22/52 [00:43<00:59,  1.99s/it][A
Epoch 28/32:  44%|████▍     | 23/52 [00:45<00:57,  1.98s/it][A
Epoch 28/32:  44%|████▍     | 23/52 [00:45<00:57,  1.98s/it][A
Epoch 28/32:  46%|████▌     | 24/52 [00:47<00:55,  1.97s/it][A
Epoch 28/32:  46%|████▌     | 24/52 [00:47<00:55,  1.98s/it][A
Epoch 28/32:  48%|████▊     | 25/52 [00:49<00:53,  1.98s/it][A
Epoch 28/32:  48%|████▊     | 25/52 [00:49<00:53,  1.98s/it][A
Epoch 28/32:  50%|█████     | 26/52 [00:51<00:51,  1.97s/it][A
Epoch 28/32:  50%|█████     | 26/52 [00:51<00:51,  1.98s/it][A
Epoch 28/32:  52%|█████▏    | 27/52 [00:53<00:49,  1.98s/it][A
Epoch 28/32:  52%|█████▏    | 27/52 [00:53<00:49,  1.98s/it][A
Epoch 28/32:  54%|█████▍    | 28/52 [00:55<00:47,  1.96s/it][A
Epoch 28/32:  54%|█████▍    | 28/52 [00:55<00:47,  1.97s/it][A
Epoch 28/32:  56%|█████▌    | 29/52 [00:57<00:45,  1.96s/it][A
Epoch 28/32:  56%|█████▌    | 29/52 [00:57<00:45,  1.96s/it][A
Epoch 28/32:  58%|█████▊    | 30/52 [00:59<00:43,  1.96s/it][A
Epoch 28/32:  58%|█████▊    | 30/52 [00:59<00:43,  1.96s/it][A
Epoch 28/32:  60%|█████▉    | 31/52 [01:01<00:41,  1.96s/it][A
Epoch 28/32:  60%|█████▉    | 31/52 [01:01<00:41,  1.97s/it][A
Epoch 28/32:  62%|██████▏   | 32/52 [01:03<00:39,  1.96s/it][A
Epoch 28/32:  62%|██████▏   | 32/52 [01:03<00:39,  1.96s/it][A
Epoch 28/32:  63%|██████▎   | 33/52 [01:05<00:37,  1.96s/it][A
Epoch 28/32:  63%|██████▎   | 33/52 [01:05<00:37,  1.96s/it][A
Epoch 28/32:  65%|██████▌   | 34/52 [01:07<00:35,  1.97s/it][A
Epoch 28/32:  65%|██████▌   | 34/52 [01:07<00:35,  1.97s/it][A
Epoch 28/32:  67%|██████▋   | 35/52 [01:08<00:33,  1.97s/it][A
Epoch 28/32:  67%|██████▋   | 35/52 [01:09<00:33,  1.96s/it][A
Epoch 28/32:  69%|██████▉   | 36/52 [01:10<00:31,  1.97s/it][A
Epoch 28/32:  69%|██████▉   | 36/52 [01:11<00:31,  1.97s/it][A
Epoch 28/32:  71%|███████   | 37/52 [01:13<00:29,  1.99s/it][A
Epoch 28/32:  71%|███████   | 37/52 [01:13<00:29,  1.99s/it][A
Epoch 28/32:  73%|███████▎  | 38/52 [01:15<00:27,  1.98s/it][A
Epoch 28/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.99s/it][A
Epoch 28/32:  75%|███████▌  | 39/52 [01:17<00:26,  2.05s/it][A
Epoch 28/32:  75%|███████▌  | 39/52 [01:17<00:26,  2.05s/it][A
Epoch 28/32:  77%|███████▋  | 40/52 [01:19<00:24,  2.04s/it][A
Epoch 28/32:  77%|███████▋  | 40/52 [01:19<00:24,  2.04s/it][A
Epoch 28/32:  79%|███████▉  | 41/52 [01:21<00:22,  2.03s/it][A
Epoch 28/32:  79%|███████▉  | 41/52 [01:21<00:22,  2.03s/it][A
Epoch 28/32:  81%|████████  | 42/52 [01:23<00:20,  2.01s/it][A
Epoch 28/32:  81%|████████  | 42/52 [01:23<00:20,  2.01s/it][A
Epoch 28/32:  83%|████████▎ | 43/52 [01:25<00:17,  1.99s/it][A
Epoch 28/32:  83%|████████▎ | 43/52 [01:25<00:17,  1.99s/it][A
Epoch 28/32:  85%|████████▍ | 44/52 [01:27<00:15,  1.99s/it][A
Epoch 28/32:  85%|████████▍ | 44/52 [01:27<00:15,  1.99s/it][A

Epoch 28/32:  87%|████████▋ | 45/52 [01:29<00:13,  1.98s/it][AEpoch 28/32:  87%|████████▋ | 45/52 [01:29<00:13,  1.98s/it][A
Epoch 28/32:  88%|████████▊ | 46/52 [01:31<00:11,  1.98s/it][A
Epoch 28/32:  88%|████████▊ | 46/52 [01:31<00:11,  1.98s/it][A
Epoch 28/32:  90%|█████████ | 47/52 [01:33<00:09,  1.98s/it][A
Epoch 28/32:  90%|█████████ | 47/52 [01:33<00:09,  1.99s/it][A
Epoch 28/32:  92%|█████████▏| 48/52 [01:35<00:07,  2.00s/it][A
Epoch 28/32:  92%|█████████▏| 48/52 [01:35<00:07,  2.00s/it][A
Epoch 28/32:  94%|█████████▍| 49/52 [01:37<00:05,  1.99s/it][A
Epoch 28/32:  94%|█████████▍| 49/52 [01:37<00:05,  1.99s/it][A
Epoch 28/32:  96%|█████████▌| 50/52 [01:39<00:03,  2.00s/it][A
Epoch 28/32:  96%|█████████▌| 50/52 [01:39<00:03,  2.00s/it][A
Epoch 28/32:  98%|█████████▊| 51/52 [01:41<00:01,  1.99s/it][A
Epoch 28/32:  98%|█████████▊| 51/52 [01:41<00:01,  1.99s/it][A
Epoch 28/32: 100%|██████████| 52/52 [01:42<00:00,  1.87s/it][AEpoch 28/32: 100%|██████████| 52/52 [01:42<00:00,  1.97s/it]

Epoch 28/32: 100%|██████████| 52/52 [01:42<00:00,  1.87s/it][AEpoch 28/32: 100%|██████████| 52/52 [01:42<00:00,  1.98s/it]

[β=2.0] Epoch 28 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.205e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.370e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.499e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.107e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.680e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.765e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=2.037e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.150e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.469e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.859e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.069e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.957e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=7.903e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.620e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.871e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.418e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.403e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.159e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.750e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.473e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.033e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.293e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.001e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.112e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.413e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.530e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.794e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.243e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.074e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.315e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=3.975e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.417e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.288e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.404e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=9.130e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.686e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.429e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=5.929e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=2.968e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.430e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.545e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.951e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.502e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=8.260e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=1.821e+00

  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.090e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=6.098e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  layer model_2__forward_module.module.output_conv act-std: mean=1.409e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.741e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.105e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.341e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.142e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.582e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.449e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=9.004e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.141e-01
  layer model_0__forward_module.module.input_conv act-std: mean=2.017e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.688e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.596e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.104e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.399e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.183e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.227e-01
  layer model_0__forward_module.module.output_conv act-std: mean=1.885e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.253e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.086e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.546e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.145e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.262e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.286e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.891e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.023e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.212e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.869e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.618e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.833e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=8.331e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=3.560e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=1.926e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.821e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.436e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.784e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.764e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.520e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=8.318e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=1.939e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.442e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.155e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.872e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.914e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.236e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.465e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.237e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.826e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=2.695e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.088e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.513e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.168e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.192e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=6.259e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.106e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=1.862e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.297e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.854e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.012e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.300e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=9.175e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.103e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.872e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.820e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.366e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.236e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.415e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.738e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.203e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.485e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.562e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.428e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.095e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.811e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.920e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.464e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.772e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.914e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.458e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.578e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.425e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.879e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.015e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.878e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.509e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.455e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.443e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.723e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.097e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.250e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.718e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.376e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.778e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.186e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=1.761e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.677e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=3.731e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.242e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.727e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.536e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.900e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.020e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.072e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.520e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.177e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.805e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.853e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.461e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=4.952e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.615e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.259e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.646e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.787e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=1.784e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.446e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.980e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.269e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.808e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.655e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.652e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.475e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.803e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.665e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.198e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.708e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.915e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.249e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=2.632e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.232e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.112e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.278e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.834e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.191e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.768e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.481e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.577e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.082e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.989e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.875e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.250e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.716e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.298e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.112e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.485e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.948e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.513e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=7.259e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.475e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.512e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.395e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=3.335e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.279e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.777e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.015e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.518e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.271e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.909e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.885e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.115e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.374e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.278e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.631e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.485e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.053e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.325e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=5.324e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=1.946e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.375e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.138e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=2.989e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.58it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:09,  1.24it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.60it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:07,  1.43it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.60it/s][A
Evaluating:  23%|██▎       | 3/13 [00:02<00:06,  1.51it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.57it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.53it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.56it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.54it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.57it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.56it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.57it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.57it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.56it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.58it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.56it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.59it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.57it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.60it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.57it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.60it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.57it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.59it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s]
Epoch 28/32 - Train Loss: 16.987386 - Test Loss: 17.101159
Training epochs:  88%|████████▊ | 28/32 [48:14<07:18, 109.72s/it]
Epoch 29/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s]
Training epochs:  88%|████████▊ | 28/32 [48:14<07:18, 109.74s/it]
Epoch 29/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 29/32:   2%|▏         | 1/52 [00:02<01:43,  2.02s/it][A
Epoch 29/32:   2%|▏         | 1/52 [00:01<01:39,  1.95s/it][A
Epoch 29/32:   4%|▍         | 2/52 [00:03<01:36,  1.94s/it][A
Epoch 29/32:   4%|▍         | 2/52 [00:03<01:38,  1.98s/it][A
Epoch 29/32:   6%|▌         | 3/52 [00:05<01:35,  1.95s/it][A
Epoch 29/32:   6%|▌         | 3/52 [00:05<01:36,  1.97s/it][A
Epoch 29/32:   8%|▊         | 4/52 [00:07<01:34,  1.96s/it][A
Epoch 29/32:   8%|▊         | 4/52 [00:07<01:34,  1.98s/it][A
Epoch 29/32:  10%|▉         | 5/52 [00:09<01:32,  1.96s/it][A
Epoch 29/32:  10%|▉         | 5/52 [00:09<01:32,  1.97s/it][A
Epoch 29/32:  12%|█▏        | 6/52 [00:11<01:30,  1.96s/it][A
Epoch 29/32:  12%|█▏        | 6/52 [00:11<01:30,  1.97s/it][A

Epoch 29/32:  13%|█▎        | 7/52 [00:13<01:28,  1.97s/it][AEpoch 29/32:  13%|█▎        | 7/52 [00:13<01:28,  1.97s/it][A
Epoch 29/32:  15%|█▌        | 8/52 [00:15<01:26,  1.97s/it][A
Epoch 29/32:  15%|█▌        | 8/52 [00:15<01:26,  1.98s/it][A
Epoch 29/32:  17%|█▋        | 9/52 [00:17<01:25,  1.98s/it][A
Epoch 29/32:  17%|█▋        | 9/52 [00:17<01:25,  1.98s/it][A
Epoch 29/32:  19%|█▉        | 10/52 [00:19<01:23,  1.98s/it][A
Epoch 29/32:  19%|█▉        | 10/52 [00:19<01:23,  1.98s/it][A
Epoch 29/32:  21%|██        | 11/52 [00:21<01:21,  1.99s/it][A
Epoch 29/32:  21%|██        | 11/52 [00:21<01:21,  1.99s/it][A
Epoch 29/32:  23%|██▎       | 12/52 [00:23<01:21,  2.03s/it][A
Epoch 29/32:  23%|██▎       | 12/52 [00:23<01:21,  2.04s/it][A
Epoch 29/32:  25%|██▌       | 13/52 [00:25<01:18,  2.00s/it][A
Epoch 29/32:  25%|██▌       | 13/52 [00:25<01:18,  2.00s/it][A
Epoch 29/32:  27%|██▋       | 14/52 [00:27<01:15,  1.98s/it][A
Epoch 29/32:  27%|██▋       | 14/52 [00:27<01:15,  1.98s/it][A
Epoch 29/32:  29%|██▉       | 15/52 [00:29<01:13,  1.97s/it][A
Epoch 29/32:  29%|██▉       | 15/52 [00:29<01:13,  1.97s/it][A
Epoch 29/32:  31%|███       | 16/52 [00:31<01:10,  1.97s/it][A
Epoch 29/32:  31%|███       | 16/52 [00:31<01:10,  1.97s/it][A
Epoch 29/32:  33%|███▎      | 17/52 [00:33<01:08,  1.97s/it][A
Epoch 29/32:  33%|███▎      | 17/52 [00:33<01:08,  1.97s/it][A
Epoch 29/32:  35%|███▍      | 18/52 [00:35<01:06,  1.96s/it][A
Epoch 29/32:  35%|███▍      | 18/52 [00:35<01:06,  1.96s/it][A
Epoch 29/32:  37%|███▋      | 19/52 [00:37<01:04,  1.95s/it][A
Epoch 29/32:  37%|███▋      | 19/52 [00:37<01:04,  1.95s/it][A

Epoch 29/32:  38%|███▊      | 20/52 [00:39<01:02,  1.94s/it][AEpoch 29/32:  38%|███▊      | 20/52 [00:39<01:02,  1.94s/it][A
Epoch 29/32:  40%|████      | 21/52 [00:41<01:00,  1.94s/it][A
Epoch 29/32:  40%|████      | 21/52 [00:41<01:00,  1.95s/it][A
Epoch 29/32:  42%|████▏     | 22/52 [00:43<00:58,  1.96s/it][A
Epoch 29/32:  42%|████▏     | 22/52 [00:43<00:58,  1.96s/it][A
Epoch 29/32:  44%|████▍     | 23/52 [00:45<00:58,  2.01s/it][A
Epoch 29/32:  44%|████▍     | 23/52 [00:45<00:58,  2.01s/it][A
Epoch 29/32:  46%|████▌     | 24/52 [00:47<00:55,  1.99s/it][A
Epoch 29/32:  46%|████▌     | 24/52 [00:47<00:55,  1.99s/it][A
Epoch 29/32:  48%|████▊     | 25/52 [00:49<00:53,  1.98s/it][A
Epoch 29/32:  48%|████▊     | 25/52 [00:49<00:53,  1.98s/it][A
Epoch 29/32:  50%|█████     | 26/52 [00:51<00:51,  1.98s/it][A
Epoch 29/32:  50%|█████     | 26/52 [00:51<00:51,  1.98s/it][A
Epoch 29/32:  52%|█████▏    | 27/52 [00:53<00:49,  1.97s/it][A
Epoch 29/32:  52%|█████▏    | 27/52 [00:53<00:49,  1.98s/it][A
Epoch 29/32:  54%|█████▍    | 28/52 [00:55<00:47,  1.97s/it][A
Epoch 29/32:  54%|█████▍    | 28/52 [00:55<00:47,  1.98s/it][A
Epoch 29/32:  56%|█████▌    | 29/52 [00:57<00:45,  1.98s/it][A
Epoch 29/32:  56%|█████▌    | 29/52 [00:57<00:45,  1.98s/it][A
Epoch 29/32:  58%|█████▊    | 30/52 [00:59<00:43,  1.98s/it][A
Epoch 29/32:  58%|█████▊    | 30/52 [00:59<00:43,  1.98s/it][A
Epoch 29/32:  60%|█████▉    | 31/52 [01:01<00:41,  1.98s/it][A
Epoch 29/32:  60%|█████▉    | 31/52 [01:01<00:41,  1.98s/it][A
Epoch 29/32:  62%|██████▏   | 32/52 [01:03<00:39,  1.97s/it][A
Epoch 29/32:  62%|██████▏   | 32/52 [01:03<00:39,  1.97s/it][A
Epoch 29/32:  63%|██████▎   | 33/52 [01:05<00:37,  1.97s/it][A
Epoch 29/32:  63%|██████▎   | 33/52 [01:05<00:37,  1.97s/it][A
Epoch 29/32:  65%|██████▌   | 34/52 [01:07<00:35,  1.96s/it][A
Epoch 29/32:  65%|██████▌   | 34/52 [01:07<00:35,  1.96s/it][A
Epoch 29/32:  67%|██████▋   | 35/52 [01:09<00:33,  1.96s/it][A
Epoch 29/32:  67%|██████▋   | 35/52 [01:09<00:33,  1.96s/it][A
Epoch 29/32:  69%|██████▉   | 36/52 [01:11<00:31,  1.97s/it][A
Epoch 29/32:  69%|██████▉   | 36/52 [01:11<00:31,  1.97s/it][A
Epoch 29/32:  71%|███████   | 37/52 [01:13<00:29,  1.98s/it][A
Epoch 29/32:  71%|███████   | 37/52 [01:13<00:29,  1.98s/it][A
Epoch 29/32:  73%|███████▎  | 38/52 [01:15<00:27,  1.98s/it][A
Epoch 29/32:  73%|███████▎  | 38/52 [01:14<00:27,  1.98s/it][A
Epoch 29/32:  75%|███████▌  | 39/52 [01:17<00:25,  1.97s/it][A
Epoch 29/32:  75%|███████▌  | 39/52 [01:16<00:25,  1.97s/it][A
Epoch 29/32:  77%|███████▋  | 40/52 [01:18<00:23,  1.97s/it][A
Epoch 29/32:  77%|███████▋  | 40/52 [01:19<00:23,  1.98s/it][A
Epoch 29/32:  79%|███████▉  | 41/52 [01:20<00:21,  1.97s/it][A
Epoch 29/32:  79%|███████▉  | 41/52 [01:20<00:21,  1.97s/it][A
Epoch 29/32:  81%|████████  | 42/52 [01:22<00:19,  1.98s/it][A
Epoch 29/32:  81%|████████  | 42/52 [01:22<00:19,  1.98s/it][A
Epoch 29/32:  83%|████████▎ | 43/52 [01:24<00:17,  1.98s/it][A
Epoch 29/32:  83%|████████▎ | 43/52 [01:24<00:17,  1.98s/it][A
Epoch 29/32:  85%|████████▍ | 44/52 [01:26<00:15,  1.99s/it][A
Epoch 29/32:  85%|████████▍ | 44/52 [01:26<00:15,  2.00s/it][A
Epoch 29/32:  87%|████████▋ | 45/52 [01:28<00:13,  1.98s/it][A
Epoch 29/32:  87%|████████▋ | 45/52 [01:28<00:13,  1.98s/it][A
Epoch 29/32:  88%|████████▊ | 46/52 [01:30<00:11,  1.98s/it][A
Epoch 29/32:  88%|████████▊ | 46/52 [01:30<00:11,  1.98s/it][A
Epoch 29/32:  90%|█████████ | 47/52 [01:32<00:09,  1.98s/it][A
Epoch 29/32:  90%|█████████ | 47/52 [01:32<00:09,  1.98s/it][A
Epoch 29/32:  92%|█████████▏| 48/52 [01:34<00:07,  1.99s/it][A
Epoch 29/32:  92%|█████████▏| 48/52 [01:34<00:07,  1.98s/it][A
Epoch 29/32:  94%|█████████▍| 49/52 [01:37<00:06,  2.04s/it][A
Epoch 29/32:  94%|█████████▍| 49/52 [01:36<00:06,  2.03s/it][A
Epoch 29/32:  96%|█████████▌| 50/52 [01:39<00:04,  2.02s/it][A
Epoch 29/32:  96%|█████████▌| 50/52 [01:38<00:04,  2.02s/it][A
Epoch 29/32:  98%|█████████▊| 51/52 [01:41<00:02,  2.01s/it][A
Epoch 29/32:  98%|█████████▊| 51/52 [01:40<00:02,  2.01s/it][A
Epoch 29/32: 100%|██████████| 52/52 [01:42<00:00,  1.89s/it][AEpoch 29/32: 100%|██████████| 52/52 [01:42<00:00,  1.97s/it]

[β=2.0] Epoch 29 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.223e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.381e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.570e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.083e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.495e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.838e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=2.074e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.125e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.494e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.854e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=8.958e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.917e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=7.707e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.586e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.859e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.383e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.434e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.159e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.866e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.410e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.024e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.287e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.980e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.104e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.440e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.548e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.941e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=6.188e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.086e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.331e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.000e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.430e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.312e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.460e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=9.517e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.729e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.392e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=5.838e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=2.910e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.411e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.569e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=2.996e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.640e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=8.169e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=1.790e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.070e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.985e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.394e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.767e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.146e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.401e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.129e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.575e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.341e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=8.930e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.119e-01
  layer model_0__forward_module.module.input_conv act-std: mean=2.063e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.754e-01

  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.636e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.004e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.373e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.108e-01
Epoch 29/32: 100%|██████████| 52/52 [01:42<00:00,  1.88s/it]  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.253e-01[A
  layer model_0__forward_module.module.output_conv act-std: mean=1.847e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.733e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.307e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=3.179e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.377e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.656e-01Epoch 29/32: 100%|██████████| 52/52 [01:42<00:00,  1.97s/it]

  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.984e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.716e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.719e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=5.563e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.293e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.824e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.079e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.273e-02
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=5.520e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=3.040e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.589e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=1.069e-02
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=3.004e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=2.004e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=2.263e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=9.587e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.320e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.633e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.663e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=1.767e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.714e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.974e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.841e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.035e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.121e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.793e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.713e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=4.441e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=4.082e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.660e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=8.867e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.341e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.282e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.412e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.955e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.030e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.307e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=9.743e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.176e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.795e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.863e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.159e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.878e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.095e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.428e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.276e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.608e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.677e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.258e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.043e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.741e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=1.957e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.550e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.702e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.699e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.426e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.599e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.253e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.817e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.070e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.927e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.503e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.304e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.070e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.957e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=1.861e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=2.959e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.672e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.365e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.802e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.423e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.137e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.878e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.205e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.604e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.108e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.341e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.556e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.054e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.213e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.254e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.287e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.263e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.384e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.275e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=4.578e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.761e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.382e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.790e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=3.364e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=1.831e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.610e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.435e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.517e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.965e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.921e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.912e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.856e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.062e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.830e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.222e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.207e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.133e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.751e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=3.814e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.452e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.310e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.380e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=3.036e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.759e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.157e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.556e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=3.220e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=3.122e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.239e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.071e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.326e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.115e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.106e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.630e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.238e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.348e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.027e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=5.451e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.440e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.484e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.803e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=4.052e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.581e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.123e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.879e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.713e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.517e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.930e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=5.213e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.170e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.576e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.067e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.822e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.750e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.267e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=3.138e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=7.289e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=2.047e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.420e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.210e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.119e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.58it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.58it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.62it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.58it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.61it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.59it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.61it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.58it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:04,  1.61it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.59it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.60it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.59it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.60it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.59it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:04<00:03,  1.60it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.58it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.60it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.57it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.57it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.55it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.58it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:06<00:01,  1.56it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.59it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.57it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]
Training epochs:  91%|█████████ | 29/32 [50:05<05:30, 110.05s/it]
Epoch 30/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]
Epoch 29/32 - Train Loss: 16.952761 - Test Loss: 17.076358
Training epochs:  91%|█████████ | 29/32 [50:05<05:30, 110.08s/it]
Epoch 30/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 30/32:   2%|▏         | 1/52 [00:02<01:46,  2.10s/it][A
Epoch 30/32:   2%|▏         | 1/52 [00:02<01:43,  2.04s/it][A
Epoch 30/32:   4%|▍         | 2/52 [00:03<01:38,  1.97s/it][A
Epoch 30/32:   4%|▍         | 2/52 [00:04<01:40,  2.00s/it][A
Epoch 30/32:   6%|▌         | 3/52 [00:05<01:35,  1.96s/it][A
Epoch 30/32:   6%|▌         | 3/52 [00:05<01:36,  1.98s/it][A
Epoch 30/32:   8%|▊         | 4/52 [00:07<01:34,  1.97s/it][A
Epoch 30/32:   8%|▊         | 4/52 [00:07<01:34,  1.96s/it][A
Epoch 30/32:  10%|▉         | 5/52 [00:09<01:33,  1.98s/it][A
Epoch 30/32:  10%|▉         | 5/52 [00:09<01:32,  1.98s/it][A
Epoch 30/32:  12%|█▏        | 6/52 [00:11<01:31,  1.98s/it][A
Epoch 30/32:  12%|█▏        | 6/52 [00:11<01:31,  1.99s/it][A
Epoch 30/32:  13%|█▎        | 7/52 [00:13<01:28,  1.97s/it][A
Epoch 30/32:  13%|█▎        | 7/52 [00:13<01:28,  1.97s/it][A
Epoch 30/32:  15%|█▌        | 8/52 [00:15<01:26,  1.97s/it][A
Epoch 30/32:  15%|█▌        | 8/52 [00:15<01:26,  1.97s/it][A
Epoch 30/32:  17%|█▋        | 9/52 [00:17<01:24,  1.97s/it][A
Epoch 30/32:  17%|█▋        | 9/52 [00:17<01:24,  1.97s/it][A
Epoch 30/32:  19%|█▉        | 10/52 [00:19<01:22,  1.97s/it][A
Epoch 30/32:  19%|█▉        | 10/52 [00:19<01:22,  1.97s/it][A
Epoch 30/32:  21%|██        | 11/52 [00:21<01:20,  1.96s/it][A
Epoch 30/32:  21%|██        | 11/52 [00:21<01:20,  1.96s/it][A
Epoch 30/32:  23%|██▎       | 12/52 [00:23<01:18,  1.97s/it][A
Epoch 30/32:  23%|██▎       | 12/52 [00:23<01:18,  1.97s/it][A
Epoch 30/32:  25%|██▌       | 13/52 [00:25<01:16,  1.97s/it][A
Epoch 30/32:  25%|██▌       | 13/52 [00:25<01:17,  1.98s/it][A
Epoch 30/32:  27%|██▋       | 14/52 [00:27<01:14,  1.97s/it][A
Epoch 30/32:  27%|██▋       | 14/52 [00:27<01:15,  1.97s/it][A
Epoch 30/32:  29%|██▉       | 15/52 [00:29<01:13,  1.98s/it][A
Epoch 30/32:  29%|██▉       | 15/52 [00:29<01:13,  1.98s/it][A
Epoch 30/32:  31%|███       | 16/52 [00:31<01:11,  1.98s/it][A
Epoch 30/32:  31%|███       | 16/52 [00:31<01:11,  1.99s/it][A
Epoch 30/32:  33%|███▎      | 17/52 [00:33<01:09,  1.97s/it][A
Epoch 30/32:  33%|███▎      | 17/52 [00:33<01:09,  1.98s/it][A
Epoch 30/32:  35%|███▍      | 18/52 [00:35<01:06,  1.97s/it][A
Epoch 30/32:  35%|███▍      | 18/52 [00:35<01:06,  1.97s/it][A
Epoch 30/32:  37%|███▋      | 19/52 [00:37<01:04,  1.97s/it][A
Epoch 30/32:  37%|███▋      | 19/52 [00:37<01:04,  1.97s/it][A
Epoch 30/32:  38%|███▊      | 20/52 [00:39<01:03,  1.98s/it][A
Epoch 30/32:  38%|███▊      | 20/52 [00:39<01:03,  1.98s/it][A

Epoch 30/32:  40%|████      | 21/52 [00:41<01:01,  1.98s/it]Epoch 30/32:  40%|████      | 21/52 [00:41<01:01,  1.98s/it][A[A
Epoch 30/32:  42%|████▏     | 22/52 [00:43<00:59,  1.98s/it][A
Epoch 30/32:  42%|████▏     | 22/52 [00:43<00:59,  1.99s/it][A
Epoch 30/32:  44%|████▍     | 23/52 [00:45<00:57,  1.98s/it][A
Epoch 30/32:  44%|████▍     | 23/52 [00:45<00:57,  1.98s/it][A
Epoch 30/32:  46%|████▌     | 24/52 [00:47<00:55,  1.98s/it][A
Epoch 30/32:  46%|████▌     | 24/52 [00:47<00:55,  1.98s/it][A
Epoch 30/32:  48%|████▊     | 25/52 [00:49<00:53,  1.97s/it][A
Epoch 30/32:  48%|████▊     | 25/52 [00:49<00:53,  1.98s/it][A
Epoch 30/32:  50%|█████     | 26/52 [00:51<00:53,  2.07s/it][A
Epoch 30/32:  50%|█████     | 26/52 [00:51<00:53,  2.07s/it][A
Epoch 30/32:  52%|█████▏    | 27/52 [00:53<00:51,  2.05s/it][A
Epoch 30/32:  52%|█████▏    | 27/52 [00:53<00:51,  2.05s/it][A
Epoch 30/32:  54%|█████▍    | 28/52 [00:55<00:48,  2.04s/it][A
Epoch 30/32:  54%|█████▍    | 28/52 [00:55<00:48,  2.04s/it][A
Epoch 30/32:  56%|█████▌    | 29/52 [00:57<00:47,  2.06s/it][A
Epoch 30/32:  56%|█████▌    | 29/52 [00:57<00:47,  2.07s/it][A
Epoch 30/32:  58%|█████▊    | 30/52 [00:59<00:44,  2.03s/it][A
Epoch 30/32:  58%|█████▊    | 30/52 [00:59<00:44,  2.03s/it][A
Epoch 30/32:  60%|█████▉    | 31/52 [01:01<00:42,  2.00s/it][A
Epoch 30/32:  60%|█████▉    | 31/52 [01:01<00:42,  2.01s/it][A
Epoch 30/32:  62%|██████▏   | 32/52 [01:03<00:39,  1.99s/it][A
Epoch 30/32:  62%|██████▏   | 32/52 [01:03<00:39,  1.99s/it][A
Epoch 30/32:  63%|██████▎   | 33/52 [01:05<00:37,  1.99s/it][A
Epoch 30/32:  63%|██████▎   | 33/52 [01:05<00:37,  1.99s/it][A
Epoch 30/32:  65%|██████▌   | 34/52 [01:07<00:35,  1.97s/it][A
Epoch 30/32:  65%|██████▌   | 34/52 [01:07<00:35,  1.97s/it][A
Epoch 30/32:  67%|██████▋   | 35/52 [01:09<00:33,  1.97s/it][A
Epoch 30/32:  67%|██████▋   | 35/52 [01:09<00:33,  1.97s/it][A
Epoch 30/32:  69%|██████▉   | 36/52 [01:11<00:31,  1.96s/it][A
Epoch 30/32:  69%|██████▉   | 36/52 [01:11<00:31,  1.96s/it][A
Epoch 30/32:  71%|███████   | 37/52 [01:13<00:29,  1.96s/it][A
Epoch 30/32:  71%|███████   | 37/52 [01:13<00:29,  1.96s/it][A

Epoch 30/32:  73%|███████▎  | 38/52 [01:15<00:27,  1.96s/it][AEpoch 30/32:  73%|███████▎  | 38/52 [01:15<00:27,  1.96s/it][A
Epoch 30/32:  75%|███████▌  | 39/52 [01:17<00:25,  1.96s/it][A
Epoch 30/32:  75%|███████▌  | 39/52 [01:17<00:25,  1.96s/it][A
Epoch 30/32:  77%|███████▋  | 40/52 [01:19<00:23,  1.96s/it][A
Epoch 30/32:  77%|███████▋  | 40/52 [01:19<00:23,  1.96s/it][A
Epoch 30/32:  79%|███████▉  | 41/52 [01:21<00:21,  1.95s/it][A
Epoch 30/32:  79%|███████▉  | 41/52 [01:21<00:21,  1.95s/it][A
Epoch 30/32:  81%|████████  | 42/52 [01:23<00:19,  1.95s/it][A
Epoch 30/32:  81%|████████  | 42/52 [01:23<00:19,  1.95s/it][A
Epoch 30/32:  83%|████████▎ | 43/52 [01:25<00:17,  2.00s/it][A
Epoch 30/32:  83%|████████▎ | 43/52 [01:25<00:18,  2.00s/it][A
Epoch 30/32:  85%|████████▍ | 44/52 [01:27<00:15,  1.99s/it][A
Epoch 30/32:  85%|████████▍ | 44/52 [01:27<00:15,  1.99s/it][A
Epoch 30/32:  87%|████████▋ | 45/52 [01:29<00:13,  1.99s/it][A
Epoch 30/32:  87%|████████▋ | 45/52 [01:29<00:13,  2.00s/it][A
Epoch 30/32:  88%|████████▊ | 46/52 [01:31<00:11,  1.99s/it][A
Epoch 30/32:  88%|████████▊ | 46/52 [01:31<00:11,  1.99s/it][A
Epoch 30/32:  90%|█████████ | 47/52 [01:33<00:09,  1.98s/it][A
Epoch 30/32:  90%|█████████ | 47/52 [01:33<00:09,  1.99s/it][A
Epoch 30/32:  92%|█████████▏| 48/52 [01:35<00:07,  1.97s/it][A
Epoch 30/32:  92%|█████████▏| 48/52 [01:35<00:07,  1.97s/it][A
Epoch 30/32:  94%|█████████▍| 49/52 [01:37<00:05,  1.96s/it][A
Epoch 30/32:  94%|█████████▍| 49/52 [01:37<00:05,  1.96s/it][A
Epoch 30/32:  96%|█████████▌| 50/52 [01:39<00:03,  1.96s/it][A
Epoch 30/32:  96%|█████████▌| 50/52 [01:39<00:03,  1.96s/it][A
Epoch 30/32:  98%|█████████▊| 51/52 [01:41<00:01,  1.96s/it][A
Epoch 30/32:  98%|█████████▊| 51/52 [01:41<00:01,  1.96s/it][A
Epoch 30/32: 100%|██████████| 52/52 [01:42<00:00,  1.85s/it][AEpoch 30/32: 100%|██████████| 52/52 [01:42<00:00,  1.97s/it]

[β=2.0] Epoch 30 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.250e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.348e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.495e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.131e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.598e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.914e-01

  layer model_7__forward_module.module.channel_attention.3 act-std: mean=2.115e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.109e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.515e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.916e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=9.081e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.934e-01Epoch 30/32: 100%|██████████| 52/52 [01:42<00:00,  1.85s/it]
[A  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=7.807e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.588e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.847e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.391e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.462e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.109e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=4.937e-01
Epoch 30/32: 100%|██████████| 52/52 [01:42<00:00,  1.97s/it]  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.338e-01

  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.023e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.298e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=3.981e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.086e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.469e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.512e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=4.989e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=5.931e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.088e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.282e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=3.964e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.393e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.340e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.465e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=9.785e-01
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.791e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.388e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=5.754e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=2.847e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.395e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.590e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.018e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.680e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=8.015e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=1.746e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.049e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.876e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.381e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.786e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.257e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.660e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.100e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.560e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.197e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=8.810e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.088e-01
  layer model_0__forward_module.module.input_conv act-std: mean=2.106e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.843e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.743e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.048e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.355e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.053e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.293e-01
  layer model_0__forward_module.module.output_conv act-std: mean=1.824e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.412e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.018e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.310e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.125e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.316e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.354e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.914e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.874e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.819e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.309e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.125e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.397e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.123e-02
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=3.918e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.231e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.065e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=8.670e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=2.836e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.909e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.712e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=9.280e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.188e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.495e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.427e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.028e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.189e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.573e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.524e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.714e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.371e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.319e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.461e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.892e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.758e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.336e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.305e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.279e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.129e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.348e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=9.782e-01
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=1.991e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.352e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=9.905e-02
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.284e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.986e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.999e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.650e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.177e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.370e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.682e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.351e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.359e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.566e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.368e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.115e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=2.822e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=2.020e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.416e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.630e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.443e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.317e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.469e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.196e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.569e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.051e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.930e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.854e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.476e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.721e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.714e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=1.911e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.065e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.595e-02

  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.315e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.725e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.276e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.119e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=2.000e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.553e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.655e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.261e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.008e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.247e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.130e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.341e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.467e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.443e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.480e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.350e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.252e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=4.590e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.715e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.364e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.796e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=3.469e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=1.868e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.759e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.809e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.573e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=2.076e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.306e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.332e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.903e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.133e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.046e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.362e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.563e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.110e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.946e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=4.180e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.416e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.273e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.327e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=3.104e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.496e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.819e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.624e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.813e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.670e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=5.609e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.190e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.314e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.988e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.820e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.880e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.004e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.316e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=1.718e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=4.339e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.389e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.442e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.653e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=4.228e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.621e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=3.743e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=8.448e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=3.102e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=3.139e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=9.955e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=6.829e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.245e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.842e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.921e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.568e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.254e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.471e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=3.655e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=8.291e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=2.124e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.453e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.241e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.368e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.56it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.54it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:07,  1.57it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:07,  1.53it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.57it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.55it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.57it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.56it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.57it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.56it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.58it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.57it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.57it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.56it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.54it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.52it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.52it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.54it/s][A

Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.55it/s][AEvaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.53it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.56it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.54it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.57it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.55it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s]
Epoch 30/32 - Train Loss: 16.923184 - Test Loss: 17.051948

Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s]
Training epochs:  94%|█████████▍| 30/32 [51:56<03:40, 110.37s/it]
Epoch 31/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  94%|█████████▍| 30/32 [51:56<03:40, 110.37s/it]
Epoch 31/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 31/32:   2%|▏         | 1/52 [00:01<01:40,  1.97s/it][A
Epoch 31/32:   2%|▏         | 1/52 [00:01<01:40,  1.97s/it][A
Epoch 31/32:   4%|▍         | 2/52 [00:03<01:38,  1.96s/it][A
Epoch 31/32:   4%|▍         | 2/52 [00:03<01:38,  1.96s/it][A
Epoch 31/32:   6%|▌         | 3/52 [00:05<01:37,  1.98s/it][A
Epoch 31/32:   6%|▌         | 3/52 [00:05<01:37,  1.99s/it][A
Epoch 31/32:   8%|▊         | 4/52 [00:07<01:35,  1.99s/it][A
Epoch 31/32:   8%|▊         | 4/52 [00:07<01:35,  2.00s/it][A
Epoch 31/32:  10%|▉         | 5/52 [00:09<01:33,  1.99s/it][A
Epoch 31/32:  10%|▉         | 5/52 [00:09<01:33,  1.99s/it][A
Epoch 31/32:  12%|█▏        | 6/52 [00:11<01:31,  2.00s/it][A
Epoch 31/32:  12%|█▏        | 6/52 [00:11<01:31,  2.00s/it][A
Epoch 31/32:  13%|█▎        | 7/52 [00:13<01:29,  1.99s/it][A
Epoch 31/32:  13%|█▎        | 7/52 [00:13<01:29,  1.99s/it][A
Epoch 31/32:  15%|█▌        | 8/52 [00:15<01:27,  1.99s/it][A
Epoch 31/32:  15%|█▌        | 8/52 [00:15<01:27,  2.00s/it][A
Epoch 31/32:  17%|█▋        | 9/52 [00:17<01:26,  2.00s/it][A
Epoch 31/32:  17%|█▋        | 9/52 [00:17<01:26,  2.00s/it][A
Epoch 31/32:  19%|█▉        | 10/52 [00:19<01:23,  1.99s/it][A
Epoch 31/32:  19%|█▉        | 10/52 [00:19<01:23,  1.99s/it][A
Epoch 31/32:  21%|██        | 11/52 [00:21<01:21,  1.98s/it][A
Epoch 31/32:  21%|██        | 11/52 [00:21<01:21,  1.98s/it][A
Epoch 31/32:  23%|██▎       | 12/52 [00:23<01:19,  1.98s/it][A
Epoch 31/32:  23%|██▎       | 12/52 [00:23<01:19,  1.98s/it][A
Epoch 31/32:  25%|██▌       | 13/52 [00:25<01:17,  2.00s/it][A
Epoch 31/32:  25%|██▌       | 13/52 [00:25<01:17,  2.00s/it][A
Epoch 31/32:  27%|██▋       | 14/52 [00:27<01:15,  1.98s/it][A
Epoch 31/32:  27%|██▋       | 14/52 [00:27<01:15,  1.98s/it][A
Epoch 31/32:  29%|██▉       | 15/52 [00:29<01:13,  1.98s/it][A
Epoch 31/32:  29%|██▉       | 15/52 [00:29<01:13,  1.98s/it][A
Epoch 31/32:  31%|███       | 16/52 [00:31<01:11,  1.98s/it][A
Epoch 31/32:  31%|███       | 16/52 [00:31<01:11,  1.98s/it][A
Epoch 31/32:  33%|███▎      | 17/52 [00:33<01:09,  1.98s/it][A
Epoch 31/32:  33%|███▎      | 17/52 [00:33<01:09,  1.98s/it][A
Epoch 31/32:  35%|███▍      | 18/52 [00:35<01:07,  2.00s/it][A
Epoch 31/32:  35%|███▍      | 18/52 [00:35<01:08,  2.00s/it][A
Epoch 31/32:  37%|███▋      | 19/52 [00:37<01:05,  1.99s/it][A
Epoch 31/32:  37%|███▋      | 19/52 [00:37<01:05,  1.99s/it][A
Epoch 31/32:  38%|███▊      | 20/52 [00:39<01:05,  2.05s/it][A
Epoch 31/32:  38%|███▊      | 20/52 [00:39<01:05,  2.05s/it][A
Epoch 31/32:  40%|████      | 21/52 [00:42<01:04,  2.07s/it][A
Epoch 31/32:  40%|████      | 21/52 [00:42<01:04,  2.07s/it][A
Epoch 31/32:  42%|████▏     | 22/52 [00:44<01:01,  2.05s/it][A
Epoch 31/32:  42%|████▏     | 22/52 [00:44<01:01,  2.05s/it][A
Epoch 31/32:  44%|████▍     | 23/52 [00:46<00:59,  2.05s/it][A
Epoch 31/32:  44%|████▍     | 23/52 [00:46<00:59,  2.05s/it][A
Epoch 31/32:  46%|████▌     | 24/52 [00:48<00:57,  2.05s/it][A
Epoch 31/32:  46%|████▌     | 24/52 [00:48<00:57,  2.05s/it][A
Epoch 31/32:  48%|████▊     | 25/52 [00:50<00:54,  2.04s/it][A
Epoch 31/32:  48%|████▊     | 25/52 [00:50<00:54,  2.04s/it][A
Epoch 31/32:  50%|█████     | 26/52 [00:52<00:52,  2.02s/it][A
Epoch 31/32:  50%|█████     | 26/52 [00:52<00:52,  2.02s/it][A
Epoch 31/32:  52%|█████▏    | 27/52 [00:54<00:50,  2.01s/it]
[AEpoch 31/32:  52%|█████▏    | 27/52 [00:54<00:50,  2.01s/it][A

Epoch 31/32:  54%|█████▍    | 28/52 [00:56<00:49,  2.06s/it][AEpoch 31/32:  54%|█████▍    | 28/52 [00:56<00:49,  2.06s/it][A
Epoch 31/32:  56%|█████▌    | 29/52 [00:58<00:46,  2.03s/it][A
Epoch 31/32:  56%|█████▌    | 29/52 [00:58<00:46,  2.03s/it][A
Epoch 31/32:  58%|█████▊    | 30/52 [01:00<00:44,  2.01s/it][A
Epoch 31/32:  58%|█████▊    | 30/52 [01:00<00:44,  2.01s/it][A
Epoch 31/32:  60%|█████▉    | 31/52 [01:02<00:41,  2.00s/it][A
Epoch 31/32:  60%|█████▉    | 31/52 [01:02<00:41,  2.00s/it][A
Epoch 31/32:  62%|██████▏   | 32/52 [01:04<00:39,  2.00s/it][A
Epoch 31/32:  62%|██████▏   | 32/52 [01:04<00:39,  2.00s/it][A
Epoch 31/32:  63%|██████▎   | 33/52 [01:06<00:38,  2.00s/it][A
Epoch 31/32:  63%|██████▎   | 33/52 [01:06<00:38,  2.00s/it][A
Epoch 31/32:  65%|██████▌   | 34/52 [01:08<00:35,  2.00s/it][A
Epoch 31/32:  65%|██████▌   | 34/52 [01:08<00:35,  2.00s/it][A
Epoch 31/32:  67%|██████▋   | 35/52 [01:10<00:33,  1.99s/it][A
Epoch 31/32:  67%|██████▋   | 35/52 [01:10<00:33,  1.99s/it][A
Epoch 31/32:  69%|██████▉   | 36/52 [01:12<00:31,  1.98s/it][A
Epoch 31/32:  69%|██████▉   | 36/52 [01:12<00:31,  1.98s/it][A
Epoch 31/32:  71%|███████   | 37/52 [01:14<00:29,  1.98s/it][A
Epoch 31/32:  71%|███████   | 37/52 [01:14<00:29,  1.98s/it][A
Epoch 31/32:  73%|███████▎  | 38/52 [01:16<00:27,  1.98s/it][A
Epoch 31/32:  73%|███████▎  | 38/52 [01:16<00:27,  1.98s/it][A
Epoch 31/32:  75%|███████▌  | 39/52 [01:18<00:26,  2.01s/it][A
Epoch 31/32:  75%|███████▌  | 39/52 [01:18<00:26,  2.01s/it][A
Epoch 31/32:  77%|███████▋  | 40/52 [01:20<00:24,  2.01s/it][A
Epoch 31/32:  77%|███████▋  | 40/52 [01:20<00:24,  2.01s/it][A
Epoch 31/32:  79%|███████▉  | 41/52 [01:22<00:22,  2.03s/it][A
Epoch 31/32:  79%|███████▉  | 41/52 [01:22<00:22,  2.04s/it][A
Epoch 31/32:  81%|████████  | 42/52 [01:24<00:20,  2.02s/it][A
Epoch 31/32:  81%|████████  | 42/52 [01:24<00:20,  2.02s/it][A
Epoch 31/32:  83%|████████▎ | 43/52 [01:26<00:17,  2.00s/it][A
Epoch 31/32:  83%|████████▎ | 43/52 [01:26<00:17,  2.00s/it][A
Epoch 31/32:  85%|████████▍ | 44/52 [01:28<00:15,  1.99s/it][A
Epoch 31/32:  85%|████████▍ | 44/52 [01:28<00:15,  1.99s/it][A
Epoch 31/32:  87%|████████▋ | 45/52 [01:30<00:13,  1.98s/it][A
Epoch 31/32:  87%|████████▋ | 45/52 [01:30<00:13,  1.98s/it][A
Epoch 31/32:  88%|████████▊ | 46/52 [01:32<00:12,  2.01s/it][A
Epoch 31/32:  88%|████████▊ | 46/52 [01:32<00:12,  2.01s/it][A
Epoch 31/32:  90%|█████████ | 47/52 [01:34<00:10,  2.03s/it][A
Epoch 31/32:  90%|█████████ | 47/52 [01:34<00:10,  2.03s/it][A
Epoch 31/32:  92%|█████████▏| 48/52 [01:36<00:08,  2.01s/it][A
Epoch 31/32:  92%|█████████▏| 48/52 [01:36<00:08,  2.01s/it][A
Epoch 31/32:  94%|█████████▍| 49/52 [01:38<00:06,  2.04s/it][A
Epoch 31/32:  94%|█████████▍| 49/52 [01:38<00:06,  2.04s/it][A
Epoch 31/32:  96%|█████████▌| 50/52 [01:40<00:04,  2.01s/it][A
Epoch 31/32:  96%|█████████▌| 50/52 [01:40<00:04,  2.01s/it][A
Epoch 31/32:  98%|█████████▊| 51/52 [01:42<00:01,  1.99s/it][A
Epoch 31/32:  98%|█████████▊| 51/52 [01:42<00:01,  2.00s/it][A
Epoch 31/32: 100%|██████████| 52/52 [01:43<00:00,  1.88s/it][AEpoch 31/32: 100%|██████████| 52/52 [01:43<00:00,  2.00s/it]

[β=2.0] Epoch 31 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.269e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.321e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.464e-01
  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.196e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.815e-01
  layer model_7__forward_module.module.channel_attention.1 act-std: mean=4.998e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=2.163e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.129e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.538e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.856e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=8.854e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.848e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=7.575e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.528e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.820e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.335e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.491e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.072e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=5.054e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.332e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.011e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.347e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.003e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.082e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.496e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.450e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=5.063e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=5.821e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.099e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.288e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=3.995e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.390e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.362e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.561e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=1.024e+00
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.812e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.346e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=5.588e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=2.755e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.357e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.606e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.061e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.825e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=7.726e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=1.704e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=1.024e+00
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.728e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.376e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.807e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.324e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.794e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.082e-01

  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.546e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=6.064e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=8.678e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.058e-01Epoch 31/32: 100%|██████████| 52/52 [01:43<00:00,  1.88s/it]
[A  layer model_0__forward_module.module.input_conv act-std: mean=2.140e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.902e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.834e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=8.039e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.357e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=8.013e-01
Epoch 31/32: 100%|██████████| 52/52 [01:43<00:00,  2.00s/it]  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.330e-01

  layer model_0__forward_module.module.output_conv act-std: mean=1.799e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.038e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.216e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.820e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.343e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.450e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.838e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.245e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.222e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.903e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.731e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.570e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.233e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=9.304e-03
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=2.851e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=1.619e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=1.738e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=7.284e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=3.024e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=1.907e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.499e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=9.257e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.165e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.689e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.364e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.104e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.198e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.713e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.519e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.592e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.107e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.268e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.337e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.539e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.608e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.275e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.113e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.267e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.294e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.297e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.013e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.032e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.448e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.029e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.465e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.147e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=6.093e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.760e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.447e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.457e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.022e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.607e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=2.840e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=1.944e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=2.700e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.294e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=3.195e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=2.410e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.503e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.786e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.897e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.721e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.668e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.244e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.420e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.115e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.999e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=5.868e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.095e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.160e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.191e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.136e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.635e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=2.145e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.822e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=2.943e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=2.954e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=2.227e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.943e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=4.383e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=2.589e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=2.214e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.931e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.265e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.365e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.398e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.481e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.396e-02
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.822e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.316e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.395e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=4.987e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.744e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.370e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.797e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=3.486e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=1.292e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.431e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=2.831e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.104e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.423e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.074e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=1.934e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.811e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.884e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.944e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=9.275e-03
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.330e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.452e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=1.802e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=3.895e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.327e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.188e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.312e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=2.440e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.578e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=2.084e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=4.311e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=3.238e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=3.015e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.538e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.066e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.369e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.209e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.584e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.923e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.840e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.328e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.698e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=7.075e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.471e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.548e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.726e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=4.076e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.343e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.577e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=5.485e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.442e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.068e-01

  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.090e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.911e-02Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.168e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.645e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.997e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.596e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=8.918e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.232e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.710e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.001e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=2.189e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.470e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.441e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.158e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.55it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.54it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:07,  1.56it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:07,  1.56it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.56it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.55it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.55it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.54it/s][A

Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.56it/s][AEvaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.56it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.55it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.55it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.56it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.55it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.55it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.54it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.55it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.53it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.55it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.53it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.54it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.53it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.55it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.54it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s]

Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s]
Epoch 31/32 - Train Loss: 16.899586 - Test Loss: 17.045025
Training epochs:  97%|█████████▋| 31/32 [53:49<01:50, 110.97s/it]
Epoch 32/32:   0%|          | 0/52 [00:00<?, ?it/s][ATraining epochs:  97%|█████████▋| 31/32 [53:49<01:50, 110.97s/it]
Epoch 32/32:   0%|          | 0/52 [00:00<?, ?it/s][A
Epoch 32/32:   2%|▏         | 1/52 [00:01<01:41,  1.98s/it][A
Epoch 32/32:   2%|▏         | 1/52 [00:01<01:40,  1.96s/it][A
Epoch 32/32:   4%|▍         | 2/52 [00:03<01:38,  1.97s/it][A
Epoch 32/32:   4%|▍         | 2/52 [00:03<01:38,  1.96s/it][A
Epoch 32/32:   6%|▌         | 3/52 [00:05<01:36,  1.98s/it][A
Epoch 32/32:   6%|▌         | 3/52 [00:05<01:36,  1.97s/it][A
Epoch 32/32:   8%|▊         | 4/52 [00:07<01:35,  1.98s/it][A
Epoch 32/32:   8%|▊         | 4/52 [00:07<01:35,  1.98s/it][A
Epoch 32/32:  10%|▉         | 5/52 [00:09<01:33,  1.99s/it][A
Epoch 32/32:  10%|▉         | 5/52 [00:09<01:33,  1.99s/it][A
Epoch 32/32:  12%|█▏        | 6/52 [00:11<01:31,  2.00s/it][A
Epoch 32/32:  12%|█▏        | 6/52 [00:11<01:31,  2.00s/it][A
Epoch 32/32:  13%|█▎        | 7/52 [00:13<01:29,  1.99s/it][A
Epoch 32/32:  13%|█▎        | 7/52 [00:13<01:29,  1.99s/it][A
Epoch 32/32:  15%|█▌        | 8/52 [00:15<01:27,  1.99s/it][A
Epoch 32/32:  15%|█▌        | 8/52 [00:15<01:27,  1.99s/it][A
Epoch 32/32:  17%|█▋        | 9/52 [00:17<01:25,  1.98s/it][A
Epoch 32/32:  17%|█▋        | 9/52 [00:17<01:25,  1.98s/it][A
Epoch 32/32:  19%|█▉        | 10/52 [00:19<01:23,  1.99s/it][A
Epoch 32/32:  19%|█▉        | 10/52 [00:19<01:23,  1.99s/it][A
Epoch 32/32:  21%|██        | 11/52 [00:21<01:22,  2.01s/it][A
Epoch 32/32:  21%|██        | 11/52 [00:21<01:22,  2.01s/it][A
Epoch 32/32:  23%|██▎       | 12/52 [00:23<01:20,  2.01s/it][A
Epoch 32/32:  23%|██▎       | 12/52 [00:23<01:20,  2.01s/it][A
Epoch 32/32:  25%|██▌       | 13/52 [00:25<01:18,  2.01s/it][A
Epoch 32/32:  25%|██▌       | 13/52 [00:25<01:18,  2.01s/it][A
Epoch 32/32:  27%|██▋       | 14/52 [00:27<01:15,  2.00s/it][A
Epoch 32/32:  27%|██▋       | 14/52 [00:27<01:15,  2.00s/it][A
Epoch 32/32:  29%|██▉       | 15/52 [00:29<01:14,  2.01s/it][A
Epoch 32/32:  29%|██▉       | 15/52 [00:29<01:14,  2.01s/it][A
Epoch 32/32:  31%|███       | 16/52 [00:31<01:12,  2.02s/it][A
Epoch 32/32:  31%|███       | 16/52 [00:32<01:12,  2.02s/it][A
Epoch 32/32:  33%|███▎      | 17/52 [00:34<01:11,  2.05s/it][A
Epoch 32/32:  33%|███▎      | 17/52 [00:34<01:11,  2.06s/it][A
Epoch 32/32:  35%|███▍      | 18/52 [00:36<01:09,  2.04s/it][A
Epoch 32/32:  35%|███▍      | 18/52 [00:36<01:09,  2.04s/it][A
Epoch 32/32:  37%|███▋      | 19/52 [00:38<01:06,  2.01s/it][A
Epoch 32/32:  37%|███▋      | 19/52 [00:38<01:06,  2.01s/it][A
Epoch 32/32:  38%|███▊      | 20/52 [00:40<01:04,  2.00s/it][A
Epoch 32/32:  38%|███▊      | 20/52 [00:40<01:04,  2.00s/it][A
Epoch 32/32:  40%|████      | 21/52 [00:42<01:01,  1.99s/it][A
Epoch 32/32:  40%|████      | 21/52 [00:42<01:01,  1.99s/it][A
Epoch 32/32:  42%|████▏     | 22/52 [00:44<00:59,  2.00s/it][A
Epoch 32/32:  42%|████▏     | 22/52 [00:44<01:00,  2.00s/it][A
Epoch 32/32:  44%|████▍     | 23/52 [00:46<00:57,  2.00s/it][A
Epoch 32/32:  44%|████▍     | 23/52 [00:46<00:57,  2.00s/it][A
Epoch 32/32:  46%|████▌     | 24/52 [00:48<00:55,  1.99s/it][A
Epoch 32/32:  46%|████▌     | 24/52 [00:47<00:55,  1.99s/it][A
Epoch 32/32:  48%|████▊     | 25/52 [00:49<00:53,  1.98s/it][A
Epoch 32/32:  48%|████▊     | 25/52 [00:49<00:53,  1.98s/it][A

Epoch 32/32:  50%|█████     | 26/52 [00:51<00:51,  1.98s/it][AEpoch 32/32:  50%|█████     | 26/52 [00:51<00:51,  1.98s/it][A
Epoch 32/32:  52%|█████▏    | 27/52 [00:53<00:49,  1.98s/it][A
Epoch 32/32:  52%|█████▏    | 27/52 [00:53<00:49,  1.98s/it][A
Epoch 32/32:  54%|█████▍    | 28/52 [00:55<00:47,  1.98s/it][A
Epoch 32/32:  54%|█████▍    | 28/52 [00:55<00:47,  1.98s/it][A
Epoch 32/32:  56%|█████▌    | 29/52 [00:58<00:46,  2.03s/it][A
Epoch 32/32:  56%|█████▌    | 29/52 [00:58<00:46,  2.03s/it][A
Epoch 32/32:  58%|█████▊    | 30/52 [01:00<00:44,  2.01s/it][A
Epoch 32/32:  58%|█████▊    | 30/52 [00:59<00:44,  2.01s/it][A
Epoch 32/32:  60%|█████▉    | 31/52 [01:02<00:42,  2.02s/it][A
Epoch 32/32:  60%|█████▉    | 31/52 [01:02<00:42,  2.03s/it][A
Epoch 32/32:  62%|██████▏   | 32/52 [01:04<00:40,  2.01s/it][A
Epoch 32/32:  62%|██████▏   | 32/52 [01:04<00:40,  2.02s/it][A
Epoch 32/32:  63%|██████▎   | 33/52 [01:06<00:38,  2.00s/it][A
Epoch 32/32:  63%|██████▎   | 33/52 [01:06<00:38,  2.01s/it][A
Epoch 32/32:  65%|██████▌   | 34/52 [01:08<00:36,  2.01s/it][A
Epoch 32/32:  65%|██████▌   | 34/52 [01:08<00:36,  2.01s/it][A
Epoch 32/32:  67%|██████▋   | 35/52 [01:10<00:34,  2.01s/it][A
Epoch 32/32:  67%|██████▋   | 35/52 [01:10<00:34,  2.01s/it][A
Epoch 32/32:  69%|██████▉   | 36/52 [01:12<00:32,  2.00s/it][A
Epoch 32/32:  69%|██████▉   | 36/52 [01:12<00:32,  2.00s/it][A
Epoch 32/32:  71%|███████   | 37/52 [01:14<00:30,  2.01s/it][A
Epoch 32/32:  71%|███████   | 37/52 [01:14<00:30,  2.01s/it][A
Epoch 32/32:  73%|███████▎  | 38/52 [01:16<00:28,  2.00s/it][A
Epoch 32/32:  73%|███████▎  | 38/52 [01:16<00:28,  2.00s/it][A
Epoch 32/32:  75%|███████▌  | 39/52 [01:18<00:25,  2.00s/it][A
Epoch 32/32:  75%|███████▌  | 39/52 [01:18<00:26,  2.00s/it][A
Epoch 32/32:  77%|███████▋  | 40/52 [01:20<00:24,  2.00s/it][A
Epoch 32/32:  77%|███████▋  | 40/52 [01:20<00:24,  2.00s/it][A
Epoch 32/32:  79%|███████▉  | 41/52 [01:22<00:21,  1.99s/it][A
Epoch 32/32:  79%|███████▉  | 41/52 [01:22<00:21,  2.00s/it][A
Epoch 32/32:  81%|████████  | 42/52 [01:24<00:19,  1.99s/it][A
Epoch 32/32:  81%|████████  | 42/52 [01:24<00:19,  1.99s/it][A
Epoch 32/32:  83%|████████▎ | 43/52 [01:26<00:17,  2.00s/it][A
Epoch 32/32:  83%|████████▎ | 43/52 [01:26<00:18,  2.00s/it][A
Epoch 32/32:  85%|████████▍ | 44/52 [01:28<00:16,  2.00s/it][A
Epoch 32/32:  85%|████████▍ | 44/52 [01:28<00:16,  2.00s/it][A

Epoch 32/32:  87%|████████▋ | 45/52 [01:30<00:14,  2.01s/it][AEpoch 32/32:  87%|████████▋ | 45/52 [01:30<00:14,  2.01s/it][A
Epoch 32/32:  88%|████████▊ | 46/52 [01:32<00:12,  2.03s/it][A
Epoch 32/32:  88%|████████▊ | 46/52 [01:32<00:12,  2.03s/it][A
Epoch 32/32:  90%|█████████ | 47/52 [01:34<00:10,  2.02s/it][A
Epoch 32/32:  90%|█████████ | 47/52 [01:34<00:10,  2.02s/it][A
Epoch 32/32:  92%|█████████▏| 48/52 [01:36<00:08,  2.01s/it][A
Epoch 32/32:  92%|█████████▏| 48/52 [01:36<00:08,  2.01s/it][A
Epoch 32/32:  94%|█████████▍| 49/52 [01:38<00:06,  2.02s/it][A
Epoch 32/32:  94%|█████████▍| 49/52 [01:38<00:06,  2.02s/it][A
Epoch 32/32:  96%|█████████▌| 50/52 [01:40<00:04,  2.03s/it][A
Epoch 32/32:  96%|█████████▌| 50/52 [01:40<00:04,  2.03s/it][A
Epoch 32/32:  98%|█████████▊| 51/52 [01:42<00:02,  2.05s/it][A
Epoch 32/32:  98%|█████████▊| 51/52 [01:42<00:02,  2.05s/it][A
Epoch 32/32: 100%|██████████| 52/52 [01:43<00:00,  1.92s/it][AEpoch 32/32: 100%|██████████| 52/52 [01:43<00:00,  2.00s/it]

Epoch 32/32: 100%|██████████| 52/52 [01:43<00:00,  1.92s/it][AEpoch 32/32: 100%|██████████| 52/52 [01:43<00:00,  2.00s/it]

[β=2.0] Epoch 32 summary:
  input  std: mean=1.814e+00
  layer model_7__forward_module.module.input_conv act-std: mean=1.289e-01
  layer model_7__forward_module.module.res_block1.conv1 act-std: mean=5.311e-01
  layer model_7__forward_module.module.res_block1.conv2 act-std: mean=8.490e-01

  layer model_7__forward_module.module.res_block2.conv1 act-std: mean=3.174e-01
  layer model_7__forward_module.module.res_block2.conv2 act-std: mean=9.753e-01Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]
[A  layer model_7__forward_module.module.channel_attention.1 act-std: mean=5.022e-01
  layer model_7__forward_module.module.channel_attention.3 act-std: mean=2.209e-01
  layer model_7__forward_module.module.output_conv act-std: mean=1.111e-01
  layer model_6__forward_module.module.input_conv act-std: mean=1.567e-01
  layer model_6__forward_module.module.res_block1.conv1 act-std: mean=5.867e-01
  layer model_6__forward_module.module.res_block1.conv2 act-std: mean=8.898e-01
  layer model_6__forward_module.module.res_block2.conv1 act-std: mean=2.870e-01
  layer model_6__forward_module.module.res_block2.conv2 act-std: mean=7.638e-01
  layer model_6__forward_module.module.channel_attention.1 act-std: mean=4.569e-01
  layer model_6__forward_module.module.channel_attention.3 act-std: mean=1.814e-01
  layer model_6__forward_module.module.output_conv act-std: mean=1.350e-01
  layer model_5__forward_module.module.input_conv act-std: mean=1.514e-01
  layer model_5__forward_module.module.res_block1.conv1 act-std: mean=3.102e-01
  layer model_5__forward_module.module.res_block1.conv2 act-std: mean=5.130e-01
  layer model_5__forward_module.module.res_block2.conv1 act-std: mean=3.394e-01
  layer model_5__forward_module.module.res_block2.conv2 act-std: mean=1.027e+00
  layer model_5__forward_module.module.channel_attention.1 act-std: mean=5.425e-01
  layer model_5__forward_module.module.channel_attention.3 act-std: mean=4.025e-01
  layer model_5__forward_module.module.output_conv act-std: mean=1.077e-01
  layer model_4__forward_module.module.input_conv act-std: mean=1.517e-01
  layer model_4__forward_module.module.res_block1.conv1 act-std: mean=4.505e-01
  layer model_4__forward_module.module.res_block1.conv2 act-std: mean=5.245e-01
  layer model_4__forward_module.module.res_block2.conv1 act-std: mean=5.933e-01
  layer model_4__forward_module.module.res_block2.conv2 act-std: mean=1.090e+00
  layer model_4__forward_module.module.channel_attention.1 act-std: mean=4.304e-01
  layer model_4__forward_module.module.channel_attention.3 act-std: mean=4.012e-01
  layer model_4__forward_module.module.output_conv act-std: mean=1.397e-01
  layer model_3__forward_module.module.input_conv act-std: mean=1.394e-01
  layer model_3__forward_module.module.res_block1.conv1 act-std: mean=5.528e-01
  layer model_3__forward_module.module.res_block1.conv2 act-std: mean=1.028e+00
  layer model_3__forward_module.module.res_block2.conv1 act-std: mean=2.878e-01
  layer model_3__forward_module.module.res_block2.conv2 act-std: mean=1.314e+00
  layer model_3__forward_module.module.channel_attention.1 act-std: mean=5.449e-01
  layer model_3__forward_module.module.channel_attention.3 act-std: mean=2.687e-01
  layer model_3__forward_module.module.output_conv act-std: mean=1.333e-01
  layer model_2__forward_module.module.input_conv act-std: mean=1.626e-01
  layer model_2__forward_module.module.res_block1.conv1 act-std: mean=3.104e-01
  layer model_2__forward_module.module.res_block1.conv2 act-std: mean=7.948e-01
  layer model_2__forward_module.module.res_block2.conv1 act-std: mean=7.726e-01
  layer model_2__forward_module.module.res_block2.conv2 act-std: mean=1.669e+00
  layer model_2__forward_module.module.channel_attention.1 act-std: mean=9.930e-01
  layer model_2__forward_module.module.channel_attention.3 act-std: mean=5.566e-01
  layer model_2__forward_module.module.output_conv act-std: mean=1.357e-01
  layer model_1__forward_module.module.input_conv act-std: mean=1.837e-01
  layer model_1__forward_module.module.res_block1.conv1 act-std: mean=2.342e-01
  layer model_1__forward_module.module.res_block1.conv2 act-std: mean=3.785e-01
  layer model_1__forward_module.module.res_block2.conv1 act-std: mean=3.073e-01
  layer model_1__forward_module.module.res_block2.conv2 act-std: mean=1.532e+00
  layer model_1__forward_module.module.channel_attention.1 act-std: mean=5.928e-01
  layer model_1__forward_module.module.channel_attention.3 act-std: mean=8.546e-01
  layer model_1__forward_module.module.output_conv act-std: mean=2.023e-01
  layer model_0__forward_module.module.input_conv act-std: mean=2.183e-01
  layer model_0__forward_module.module.res_block1.conv1 act-std: mean=3.967e-01
  layer model_0__forward_module.module.res_block1.conv2 act-std: mean=8.882e-01
  layer model_0__forward_module.module.res_block2.conv1 act-std: mean=7.928e-01
  layer model_0__forward_module.module.res_block2.conv2 act-std: mean=2.348e+00
  layer model_0__forward_module.module.channel_attention.1 act-std: mean=7.976e-01
  layer model_0__forward_module.module.channel_attention.3 act-std: mean=8.355e-01
  layer model_0__forward_module.module.output_conv act-std: mean=1.776e-01
  param model_0__forward_module.module.output_scale grad-norm: mean=1.275e-01
  param model_0__forward_module.module.input_conv.weight grad-norm: mean=1.165e+00
  param model_0__forward_module.module.input_conv.bias grad-norm: mean=2.724e-01
  param model_0__forward_module.module.input_norm.weight grad-norm: mean=1.267e-01
  param model_0__forward_module.module.input_norm.bias grad-norm: mean=1.420e-01
  param model_0__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.255e-02
  param model_0__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.038e-02
  param model_0__forward_module.module.res_block1.conv2.weight grad-norm: mean=8.485e-02
  param model_0__forward_module.module.res_block1.conv2.bias grad-norm: mean=4.946e-02
  param model_0__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.064e-02
  param model_0__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.906e-02
  param model_0__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.314e-02
  param model_0__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.101e-02
  param model_0__forward_module.module.channel_attention.1.weight grad-norm: mean=3.990e-03
  param model_0__forward_module.module.channel_attention.1.bias grad-norm: mean=2.311e-03
  param model_0__forward_module.module.channel_attention.3.weight grad-norm: mean=2.021e-02
  param model_0__forward_module.module.channel_attention.3.bias grad-norm: mean=8.521e-03
  param model_0__forward_module.module.output_conv.weight grad-norm: mean=3.069e-01
  param model_0__forward_module.module.output_conv.bias grad-norm: mean=2.120e-01
  param model_1__forward_module.module.output_scale grad-norm: mean=1.415e-01
  param model_1__forward_module.module.input_conv.weight grad-norm: mean=9.129e-01
  param model_1__forward_module.module.input_conv.bias grad-norm: mean=2.125e-01
  param model_1__forward_module.module.input_norm.weight grad-norm: mean=1.499e-01
  param model_1__forward_module.module.input_norm.bias grad-norm: mean=1.301e-01
  param model_1__forward_module.module.res_block1.conv1.weight grad-norm: mean=2.071e-02
  param model_1__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.117e-02
  param model_1__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.870e-02
  param model_1__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.520e-02
  param model_1__forward_module.module.res_block2.conv1.weight grad-norm: mean=2.498e-02
  param model_1__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.818e-02
  param model_1__forward_module.module.res_block2.conv2.weight grad-norm: mean=3.369e-02
  param model_1__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.422e-02
  param model_1__forward_module.module.channel_attention.1.weight grad-norm: mean=2.604e-03
  param model_1__forward_module.module.channel_attention.1.bias grad-norm: mean=2.798e-03
  param model_1__forward_module.module.channel_attention.3.weight grad-norm: mean=1.381e-02
  param model_1__forward_module.module.channel_attention.3.bias grad-norm: mean=7.867e-03
  param model_1__forward_module.module.output_conv.weight grad-norm: mean=2.408e-01
  param model_1__forward_module.module.output_conv.bias grad-norm: mean=2.380e-01
  param model_2__forward_module.module.output_scale grad-norm: mean=1.774e-01
  param model_2__forward_module.module.input_conv.weight grad-norm: mean=1.134e+00
  param model_2__forward_module.module.input_conv.bias grad-norm: mean=2.401e-01
  param model_2__forward_module.module.input_norm.weight grad-norm: mean=1.522e-01
  param model_2__forward_module.module.input_norm.bias grad-norm: mean=1.230e-01
  param model_2__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.036e-02
  param model_2__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.628e-02
  param model_2__forward_module.module.res_block1.conv2.weight grad-norm: mean=7.145e-02
  param model_2__forward_module.module.res_block1.conv2.bias grad-norm: mean=3.425e-02
  param model_2__forward_module.module.res_block2.conv1.weight grad-norm: mean=4.225e-02
  param model_2__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.861e-02
  param model_2__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.498e-02
  param model_2__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.865e-02
  param model_2__forward_module.module.channel_attention.1.weight grad-norm: mean=3.274e-03
  param model_2__forward_module.module.channel_attention.1.bias grad-norm: mean=2.219e-03
  param model_2__forward_module.module.channel_attention.3.weight grad-norm: mean=3.073e-02
  param model_2__forward_module.module.channel_attention.3.bias grad-norm: mean=1.518e-02
  param model_2__forward_module.module.output_conv.weight grad-norm: mean=3.517e-01
  param model_2__forward_module.module.output_conv.bias grad-norm: mean=2.763e-01
  param model_3__forward_module.module.output_scale grad-norm: mean=1.572e-01
  param model_3__forward_module.module.input_conv.weight grad-norm: mean=1.785e+00
  param model_3__forward_module.module.input_conv.bias grad-norm: mean=3.880e-01
  param model_3__forward_module.module.input_norm.weight grad-norm: mean=2.528e-01
  param model_3__forward_module.module.input_norm.bias grad-norm: mean=1.689e-01
  param model_3__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.774e-02
  param model_3__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.022e-02
  param model_3__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.194e-01
  param model_3__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.209e-02
  param model_3__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.356e-02
  param model_3__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.501e-02
  param model_3__forward_module.module.res_block2.conv2.weight grad-norm: mean=4.424e-02
  param model_3__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.264e-02
  param model_3__forward_module.module.channel_attention.1.weight grad-norm: mean=2.248e-03
  param model_3__forward_module.module.channel_attention.1.bias grad-norm: mean=3.952e-03
  param model_3__forward_module.module.channel_attention.3.weight grad-norm: mean=1.918e-02
  param model_3__forward_module.module.channel_attention.3.bias grad-norm: mean=1.672e-02
  param model_3__forward_module.module.output_conv.weight grad-norm: mean=3.111e-01
  param model_3__forward_module.module.output_conv.bias grad-norm: mean=3.239e-01
  param model_4__forward_module.module.output_scale grad-norm: mean=1.264e-01
  param model_4__forward_module.module.input_conv.weight grad-norm: mean=1.363e+00
  param model_4__forward_module.module.input_conv.bias grad-norm: mean=2.789e-01
  param model_4__forward_module.module.input_norm.weight grad-norm: mean=1.966e-01
  param model_4__forward_module.module.input_norm.bias grad-norm: mean=1.373e-01
  param model_4__forward_module.module.res_block1.conv1.weight grad-norm: mean=4.403e-02
  param model_4__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.470e-02
  param model_4__forward_module.module.res_block1.conv2.weight grad-norm: mean=5.141e-02
  param model_4__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.846e-02
  param model_4__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.066e-02
  param model_4__forward_module.module.res_block2.conv1.bias grad-norm: mean=7.891e-03
  param model_4__forward_module.module.res_block2.conv2.weight grad-norm: mean=6.326e-02
  param model_4__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.656e-02
  param model_4__forward_module.module.channel_attention.1.weight grad-norm: mean=2.054e-03
  param model_4__forward_module.module.channel_attention.1.bias grad-norm: mean=4.283e-03
  param model_4__forward_module.module.channel_attention.3.weight grad-norm: mean=1.568e-02
  param model_4__forward_module.module.channel_attention.3.bias grad-norm: mean=1.237e-02
  param model_4__forward_module.module.output_conv.weight grad-norm: mean=2.663e-01
  param model_4__forward_module.module.output_conv.bias grad-norm: mean=2.822e-01
  param model_5__forward_module.module.output_scale grad-norm: mean=1.513e-01
  param model_5__forward_module.module.input_conv.weight grad-norm: mean=1.656e+00
  param model_5__forward_module.module.input_conv.bias grad-norm: mean=3.526e-01
  param model_5__forward_module.module.input_norm.weight grad-norm: mean=2.377e-01
  param model_5__forward_module.module.input_norm.bias grad-norm: mean=1.806e-01
  param model_5__forward_module.module.res_block1.conv1.weight grad-norm: mean=3.318e-02
  param model_5__forward_module.module.res_block1.conv1.bias grad-norm: mean=2.156e-02
  param model_5__forward_module.module.res_block1.conv2.weight grad-norm: mean=3.853e-02
  param model_5__forward_module.module.res_block1.conv2.bias grad-norm: mean=1.930e-02
  param model_5__forward_module.module.res_block2.conv1.weight grad-norm: mean=3.075e-02
  param model_5__forward_module.module.res_block2.conv1.bias grad-norm: mean=1.354e-02
  param model_5__forward_module.module.res_block2.conv2.weight grad-norm: mean=5.442e-02
  param model_5__forward_module.module.res_block2.conv2.bias grad-norm: mean=1.990e-02
  param model_5__forward_module.module.channel_attention.1.weight grad-norm: mean=2.029e-03
  param model_5__forward_module.module.channel_attention.1.bias grad-norm: mean=4.291e-03
  param model_5__forward_module.module.channel_attention.3.weight grad-norm: mean=1.455e-02
  param model_5__forward_module.module.channel_attention.3.bias grad-norm: mean=1.286e-02
  param model_5__forward_module.module.output_conv.weight grad-norm: mean=2.356e-01
  param model_5__forward_module.module.output_conv.bias grad-norm: mean=3.000e-01
  param model_6__forward_module.module.output_scale grad-norm: mean=1.146e-01
  param model_6__forward_module.module.input_conv.weight grad-norm: mean=1.760e+00
  param model_6__forward_module.module.input_conv.bias grad-norm: mean=3.437e-01
  param model_6__forward_module.module.input_norm.weight grad-norm: mean=2.601e-01
  param model_6__forward_module.module.input_norm.bias grad-norm: mean=2.157e-01
  param model_6__forward_module.module.res_block1.conv1.weight grad-norm: mean=6.010e-02
  param model_6__forward_module.module.res_block1.conv1.bias grad-norm: mean=3.721e-02
  param model_6__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.310e-01
  param model_6__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.884e-02
  param model_6__forward_module.module.res_block2.conv1.weight grad-norm: mean=6.930e-02
  param model_6__forward_module.module.res_block2.conv1.bias grad-norm: mean=2.530e-02
  param model_6__forward_module.module.res_block2.conv2.weight grad-norm: mean=7.937e-02
  param model_6__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.124e-02
  param model_6__forward_module.module.channel_attention.1.weight grad-norm: mean=2.296e-03
  param model_6__forward_module.module.channel_attention.1.bias grad-norm: mean=5.994e-03
  param model_6__forward_module.module.channel_attention.3.weight grad-norm: mean=1.430e-02
  param model_6__forward_module.module.channel_attention.3.bias grad-norm: mean=1.489e-02
  param model_6__forward_module.module.output_conv.weight grad-norm: mean=2.697e-01
  param model_6__forward_module.module.output_conv.bias grad-norm: mean=3.453e-01
  param model_7__forward_module.module.output_scale grad-norm: mean=1.401e-01
  param model_7__forward_module.module.input_conv.weight grad-norm: mean=2.906e+00
  param model_7__forward_module.module.input_conv.bias grad-norm: mean=6.270e-01
  param model_7__forward_module.module.input_norm.weight grad-norm: mean=2.716e-01
  param model_7__forward_module.module.input_norm.bias grad-norm: mean=2.482e-01
  param model_7__forward_module.module.res_block1.conv1.weight grad-norm: mean=7.737e-02
  param model_7__forward_module.module.res_block1.conv1.bias grad-norm: mean=4.523e-02
  param model_7__forward_module.module.res_block1.conv2.weight grad-norm: mean=1.186e-01
  param model_7__forward_module.module.res_block1.conv2.bias grad-norm: mean=2.682e-02
  param model_7__forward_module.module.res_block2.conv1.weight grad-norm: mean=7.807e-02
  param model_7__forward_module.module.res_block2.conv1.bias grad-norm: mean=3.160e-02
  param model_7__forward_module.module.res_block2.conv2.weight grad-norm: mean=9.340e-02
  param model_7__forward_module.module.res_block2.conv2.bias grad-norm: mean=2.515e-02
  param model_7__forward_module.module.channel_attention.1.weight grad-norm: mean=2.791e-03
  param model_7__forward_module.module.channel_attention.1.bias grad-norm: mean=6.126e-03
  param model_7__forward_module.module.channel_attention.3.weight grad-norm: mean=2.404e-02
  param model_7__forward_module.module.channel_attention.3.bias grad-norm: mean=1.609e-02
  param model_7__forward_module.module.output_conv.weight grad-norm: mean=3.586e-01
  param model_7__forward_module.module.output_conv.bias grad-norm: mean=3.957e-01

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.55it/s][A
Evaluating:   8%|▊         | 1/13 [00:00<00:07,  1.54it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:06,  1.58it/s][A
Evaluating:  15%|█▌        | 2/13 [00:01<00:07,  1.55it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.57it/s][A
Evaluating:  23%|██▎       | 3/13 [00:01<00:06,  1.56it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.56it/s][A
Evaluating:  31%|███       | 4/13 [00:02<00:05,  1.54it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.55it/s][A
Evaluating:  38%|███▊      | 5/13 [00:03<00:05,  1.54it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.56it/s][A
Evaluating:  46%|████▌     | 6/13 [00:03<00:04,  1.56it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.56it/s][A
Evaluating:  54%|█████▍    | 7/13 [00:04<00:03,  1.57it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.57it/s][A
Evaluating:  62%|██████▏   | 8/13 [00:05<00:03,  1.58it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.57it/s][A
Evaluating:  69%|██████▉   | 9/13 [00:05<00:02,  1.55it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.56it/s][A
Evaluating:  77%|███████▋  | 10/13 [00:06<00:01,  1.54it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.58it/s][A
Evaluating:  85%|████████▍ | 11/13 [00:07<00:01,  1.55it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.56it/s][A
Evaluating:  92%|█████████▏| 12/13 [00:07<00:00,  1.54it/s][A
Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s]
Training epochs: 100%|██████████| 32/32 [55:41<00:00, 111.39s/it]Training epochs: 100%|██████████| 32/32 [55:41<00:00, 104.43s/it]

Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s][AEvaluating: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s]
Epoch 32/32 - Train Loss: 16.865120 - Test Loss: 17.010272
Training epochs: 100%|██████████| 32/32 [55:41<00:00, 111.41s/it]Training epochs: 100%|██████████| 32/32 [55:41<00:00, 104.43s/it]
Loaded best models from epoch 32 with loss 17.010272

>>> Completed beta = 2.0
>>> Time for this beta: 0:55:44
>>> Total elapsed time: 0:55:46
==================================================
End time: 2025-07-22 17:37:27
Total time: 0h 55m 56s
